{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preparation\n",
    "\n",
    "This notebook is used to clean up, preprocess, and aggregate data necessary for STXGB models.\n",
    "\n",
    "Please refer to the `Method` section of the article and Supplementary Information document for more information.\n",
    "\n",
    "\n",
    "In order to run this notebook, you have to download the required datasets from this [address](https://drive.google.com/drive/u/1/folders/1laAZFCvsPLLaKDvg0isTMMr20kMe0x_r). Once downloaded, set the directory in which you have save the files as `input_directory` in the cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_directory = '/home/dante/SpatialData/spatial_project/data/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import geopandas as gpd\n",
    "import platform\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "import requests\n",
    "from urllib.request import urlopen\n",
    "import json\n",
    "import os\n",
    "from os import listdir\n",
    "\n",
    "#viz\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import matplotlib.ticker as ticker\n",
    "\n",
    "\n",
    "\n",
    "pd.set_option('display.max_rows', 30)\n",
    "pd.set_option('mode.chained_assignment', None) # need to be careful with this one!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def datetimer(cellstring):\n",
    "    if cellstring == \"Total\":\n",
    "        return cellstring\n",
    "    strlist = cellstring.split(\" \")\n",
    "    return f\"{strlist[1]}-{strlist[3]}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datetimer(\"Year 2020 Week 01\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Set date string formatting based on operating system\n",
    "\n",
    "if platform.system() == 'Windows':\n",
    "    conversion_format = '%#m/%#d/%y'\n",
    "else:\n",
    "    conversion_format = '%-m/%-d/%y'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing hospital data by province"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Taking the data straight from a csv from their pivot webpage. Address found in the good_resources.txt file.\n",
    "df_cov = pd.read_csv('/home/dante/SpatialData/spatial_project/data/covid/fact_epirapo_covid19case.csv',sep=\";\")\n",
    "mun_prov = pd.read_csv('/home/dante/SpatialData/spatial_project/scripts/municipality_province_map.csv',encoding = \"ISO-8859-1\",sep=\";\",header=1)\n",
    "prov_lang_map = pd.read_csv('/home/dante/SpatialData/spatial_project/scripts/mob_map_nuts3.txt').iloc[:,1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cov"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Making actual mappings (municipalities)\n",
    "mun_prov_map = dict(zip(mun_prov.iloc[:,1],mun_prov.iloc[:,3]))\n",
    "muns = list(mun_prov.iloc[:,1])\n",
    "\n",
    "# Making actual mappings (provinces)\n",
    "prov_lang_map = prov_lang_map[prov_lang_map.iloc[:,3] != 'Åland']\n",
    "prov_fin_eng = dict(zip(prov_lang_map.iloc[:,1],prov_lang_map.iloc[:,3]))\n",
    "provs = list(set(mun_prov.iloc[:,3]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dropping everything but the municipalities (contains data about hospital district as well)\n",
    "df_cov = df_cov[df_cov.iloc[:,1].isin(muns)]\n",
    "\n",
    "# Dropping \"kaikki ajat\" aka \"all times\":\n",
    "df_cov = df_cov[df_cov['Aika'] != 'Kaikki ajat']\n",
    "\n",
    "# Mapping municipalities to the correct province\n",
    "df_cov['province'] = df_cov.iloc[:,1].map(mun_prov_map)\n",
    "\n",
    "# Dropping Ahvenanmaa (Åland) - the autonomous island we don't \n",
    "# analyze in this thing\n",
    "df_cov = df_cov[df_cov.province != 'Ahvenanmaa']\n",
    "\n",
    "# Resetting index after all this\n",
    "df_cov.reset_index(drop=True,inplace=True)\n",
    "\n",
    "# Setting val as int\n",
    "df_cov['val'] = np.where(df_cov['val'] == '..',0,df_cov['val'])\n",
    "df_cov['val'] = df_cov.val.astype(int)\n",
    "\n",
    "# Grouping by both province and week\n",
    "df_cov = df_cov.groupby(['province','Aika']).agg({'val' : 'sum'}).reset_index(drop=False)\n",
    "\n",
    "# Pivoting, so that we get cases along dimensions (time x province)\n",
    "df_cov_pivot = df_cov.pivot('Aika','province','val').reset_index(drop=False).rename_axis(None, axis=1)\n",
    "\n",
    "# Putting it in the year-week format according to the rest of the data\n",
    "df_cov_pivot['Y-W'] = df_cov_pivot.Aika.apply(datetimer)\n",
    "df_cov_pivot.drop(columns=['Aika'],inplace=True)\n",
    "\n",
    "# Data seems uniform - just how we like it!\n",
    "print(df_cov.province.value_counts())\n",
    "\n",
    "# Changing province names into English:\n",
    "df_cov_pivot.rename(columns=prov_fin_eng,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_cov_pivot.head(10)\n",
    "# df_cov_pivot.to_csv('/home/dante/SpatialData/spatial_project/data/processed/newcases_by_week_province.csv',index=False,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cov_pivot.iloc[:,0:-1] = df_cov_pivot.iloc[:,0:-1].apply(lambda x: np.log(x+1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cov_pivot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Visualizing the measure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "f = plt.figure()\n",
    "f.set_figheight(9)\n",
    "f.set_figwidth(15)\n",
    "\n",
    "ax1 = f.add_subplot(111)\n",
    "\n",
    "\n",
    "\n",
    "sns.lineplot(x='Y-W',\n",
    "             y='value',\n",
    "             hue = 'variable',\n",
    "             data=pd.melt(df_cov_pivot, ['Y-W']),\n",
    "             ax=ax1)\n",
    "\n",
    "fontdict = {'fontsize': 20,\n",
    "#  'fontweight' : rcParams['axes.titleweight'],\n",
    "#  'verticalalignment': 'baseline',\n",
    "#  'horizontalalignment': loc}\n",
    "           }\n",
    "plt.xlim(0,115)\n",
    "ax1.xaxis.set_major_locator(ticker.MultipleLocator(5))\n",
    "ax1.xaxis.set_major_formatter(ticker.ScalarFormatter())\n",
    "ax1.set_title(r\"Ln new cases (by Week x Province)\",fontdict = fontdict)\n",
    "ax1.set_xlabel(r\"Week number (starting from 2020-01)\",fontsize=14)\n",
    "ax1.set_ylabel(r\"Ln new cases\",rotation=0,labelpad=30,fontsize=14)\n",
    "\n",
    "\n",
    "\n",
    "leg = plt.legend(title='Province', loc='upper left', labels=list(df_cov_pivot.columns[0:-1]))\n",
    "# leg._legend_box.align = \"top\"\n",
    "plt.setp(leg.get_title(),fontsize='large')\n",
    "plt.savefig('/home/dante/SpatialData/spatial_project/data/processed/new_cases_timeseries.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Google intra-region mobility data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Script that combines data from all years"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specifying the path to the folder with google raw data\n",
    "path = '/home/dante/SpatialData/spatial_project/data/google/'\n",
    "\n",
    "# Getting the name of all files (2020-2022)\n",
    "filelist = [file for file in os.listdir(path) if '.csv' in file]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specifying dtypes for faster processing of csv...\n",
    "dtypes = {\n",
    "    'country_region_code' : 'object',\n",
    "    'country_region' : 'object',\n",
    "    'sub_region_1' : 'object',\n",
    "    'sub_region_2' : 'object',\n",
    "    'metro_area' : 'object',\n",
    "    'iso_3166_2_code' : 'object',\n",
    "    'census_fips_code' : 'object',\n",
    "    'place_id' : 'object',\n",
    "    'date' : 'object',\n",
    "    'retail_and_recreation_percent_change_from_baseline' : 'float64',\n",
    "    'grocery_and_pharmacy_percent_change_from_baseline' : 'float64',\n",
    "    'parks_percent_change_from_baseline' : 'float64',\n",
    "    'transit_stations_percent_change_from_baseline' : 'float64',\n",
    "    'workplaces_percent_change_from_baseline' : 'float64',\n",
    "    'residential_percent_change_from_baseline' : 'float64',\n",
    "}\n",
    "\n",
    "# Assigning the correct column names\n",
    "headers = list(dtypes.keys())\n",
    "\n",
    "# Telling Pandas what column is a datecolumn (so that it can be parsed)\n",
    "parse_dates = ['date']\n",
    "\n",
    "# Creating a map between the ISO-codes of the different provinces and their ENG/FI/SWE \n",
    "# names, for legibility later on.\n",
    "df_codemap = pd.read_csv('/home/dante/SpatialData/spatial_project/scripts/mob_map_nuts3.txt',sep=',')\n",
    "iso_name_map = dict(zip(df_codemap.iloc[:,0],df_codemap.iloc[:,3]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_codemap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specifying empty dataframe for filling\n",
    "df_list = []\n",
    "\n",
    "for filename in filelist:\n",
    "    # Specifying file location\n",
    "    floc = path + filename\n",
    "    \n",
    "    # Reading in file\n",
    "    df_mr = pd.read_csv(floc,header=0,dtype=dtypes,names=headers,parse_dates=parse_dates)\n",
    "    \n",
    "    # Sorting values by date and sub_region_2,sub_region_1 as we want to be able to\n",
    "    # look at the data in an intuitive way.\n",
    "    df_mr.sort_values(by=['date','sub_region_2','sub_region_1'],inplace=True)\n",
    "    \n",
    "    df_mr['province'] = df_mr.iso_3166_2_code.map(iso_name_map)\n",
    "    \n",
    "    # Dropping unneccesary columns\n",
    "    df_mr.drop(columns=['country_region_code','country_region','census_fips_code','iso_3166_2_code','place_id','metro_area','sub_region_2','sub_region_1'],inplace=True)\n",
    "\n",
    "    # Specifying down our data to the province level\n",
    "    df_mr.dropna(subset=['province'],inplace=True)\n",
    "    df_mr.reset_index(drop=True,inplace=True)\n",
    "\n",
    "    # Casting date in correct format (apparently Pandas doesn't actually read the dtypes correctly)\n",
    "    df_mr['date'] = pd.to_datetime(df_mr.date)\n",
    "\n",
    "    # Adding day of year for gap checking if needed\n",
    "    df_mr['dayofyear'] = df_mr['date'].dt.dayofyear\n",
    "\n",
    "    # Defining the movement data columns of interest\n",
    "    # and filling in NaNs with 0 (i.e no change/information).\n",
    "    # I do not know how sustainable this assumption is...\n",
    "    mvcols = [col for col in df_mr.columns if 'baseline' in col]\n",
    "    values = {col : 0 for col in mvcols}\n",
    "    df_mr.fillna(value=values, inplace=True)\n",
    "    \n",
    "    \n",
    "    for prov in iso_name_map.values():\n",
    "    \n",
    "        # Specify the correct province \n",
    "        df = df_mr[df_mr.province == prov]\n",
    "\n",
    "        # The below code bins the data according to week commencing MONDAY\n",
    "        # This code looks complicated, but achieves something quite understanable.\n",
    "        df['date'] = pd.to_datetime(df['date']) - pd.to_timedelta(7, unit='d')\n",
    "        df = df.groupby([pd.Grouper(key='date', freq='W-MON')])[mvcols].mean().reset_index().sort_values('date')\n",
    "\n",
    "        # Creating the column with which we can then merge to the cases data..\n",
    "        df['Y-W'] = df.date.dt.year.astype(str)+'-'+df.date.dt.isocalendar().week.astype(str)\n",
    "\n",
    "        # Adding name of province\n",
    "        df['province'] = prov\n",
    "        df.reset_index(drop=True) \n",
    "\n",
    "        # Finally, appending to the list of dfs...\n",
    "        df_list.append(df)\n",
    "    \n",
    "# Finally, concatenating the whole list\n",
    "combined_mv_df = pd.concat(df_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx,prov in enumerate(combined_mv_df.province.value_counts().index):\n",
    "    datas = combined_mv_df[combined_mv_df.province == prov].shape\n",
    "    print(f\"Province {idx+1}, shape : {datas}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving the file as a CSV.\n",
    "# combined_mv_df.to_csv('/home/dante/SpatialData/spatial_project/data/processed/google_mobility_by_week_province.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_mv_df.sort_values(by=['province','date'],inplace=True)\n",
    "# combined_mv_df[combined_mv_df.province == 'Uusimaa'].head(120)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FB inter-region connectivity data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating the Social Proximity to Cases Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sci = pd.read_csv('/home/dante/SpatialData/spatial_project/data/processed/fb_sci_by_province.csv')\n",
    "df_cov = pd.read_csv('/home/dante/SpatialData/spatial_project/data/processed/newcases_by_week_province.csv')\n",
    "prov_nam_conv = pd.read_csv('/home/dante/SpatialData/spatial_project/scripts/mob_map_nuts3.txt')\n",
    "# mun_prov = pd.read_csv('/home/dante/SpatialData/spatial_project/scripts/municipality_province_map.csv',encoding = \"ISO-8859-1\",sep=\";\",header=1)\n",
    "mun_pop = pd.read_csv('/home/dante/SpatialData/spatial_project/data/municipal/municipal_population.txt',header=0,encoding = \"UTF-8\",sep=\"\\t\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Collecting provincial information\n",
    "prov_pop = mun_pop.iloc[310:328,:]\n",
    "\n",
    "# Casting the population figures as INT\n",
    "prov_pop.iloc[:,1] = prov_pop.iloc[:,1].str.replace(\",\",\"\").astype(int)\n",
    "prov_pop.reset_index(drop=True)\n",
    "\n",
    "# Sorting values for easier legibility\n",
    "prov_pop.sort_values(by=prov_pop.columns[0],inplace=True)\n",
    "\n",
    "prov_pop['province'] = prov_pop['Population'].map(dict(zip(prov_nam_conv.iloc[:,-2],prov_nam_conv.iloc[:,3])))\n",
    "\n",
    "# Making province : population dict for division later...\n",
    "prov_pop_dict = dict(zip(prov_pop.iloc[:,2],prov_pop.iloc[:,1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Below, you'll see the dict with province x population\n",
    "prov_pop_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Changing names from THL convention to the convention we have been using.\n",
    "df_cov.rename(columns=dict(zip(prov_nam_conv.iloc[:,-1],prov_nam_conv.iloc[:,-4])),inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Having a peek at our data...\n",
    "df_cov.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Creating a copy of the COVID df so that we can always restart\n",
    "# the modifications from here.\n",
    "df_cov_c = df_cov.copy()\n",
    "\n",
    "# The below for-loop scales the covid cases for each day x province \n",
    "# to the level of new cases per 10k people. In this way, we are stand-\n",
    "# ardizing the measurement across provinces.\n",
    "for idx,prov in enumerate(df_cov.columns[0:-1]):\n",
    "    #FOR EACH PROVINCE\n",
    "\n",
    "    # For each province, fetch the population of the province,\n",
    "    # and then divide the whole time series for that province\n",
    "    # by the population.\n",
    "    case_by_pop = df_cov_c[prov] / prov_pop_dict[prov]\n",
    "    \n",
    "    # Multiply the per capita number by 10000, to get cases\n",
    "    # per 10k people.\n",
    "    case_by_10k = case_by_pop * 10000\n",
    "    \n",
    "    # Assign back to the dataframe.\n",
    "    df_cov_c[prov] = case_by_10k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cov_c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sci.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mapping GADM NUTS3 code to our used standard for the province name\n",
    "mob_map = dict(zip(prov_nam_conv.iloc[:,4],prov_nam_conv.iloc[:,3]))\n",
    "\n",
    "# Creating a map between SCI weights of a given province and all OTHERS.\n",
    "sci_dict = {}\n",
    "for prov in df_sci.user_loc_province.value_counts().index:\n",
    "    # FOR EACH PROVINCE\n",
    "    \n",
    "    # Get list of all other provinces, i.e. not the one that is being\n",
    "    # considered in the loop (because this was already deleted earlier\n",
    "    # when I made the fb_sci_by_province.csv file).\n",
    "    fr_list = list(df_sci[(df_sci.user_loc_province == prov)].fr_loc_province)\n",
    "    \n",
    "    # Get the list of all other province weights.\n",
    "    sci_weights_list = list(df_sci[(df_sci.user_loc_province == prov)].sci_weights)\n",
    "    \n",
    "    # Assign the list of all other province weights to a dictionary, with the name\n",
    "    # of the 'parent' province.\n",
    "    sci_dict[prov] = {fr_loc:sci_weight for fr_loc,sci_weight in zip(fr_list,sci_weights_list)}\n",
    "\n",
    "\n",
    "# The SCI-dict looks as follows:\n",
    "# sci_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FINALLY, we can create the SCI-weighted data!\n",
    "\n",
    "# Creating a final dataframe to write to.\n",
    "df_spc = df_cov.copy()\n",
    "\n",
    "for prov in sci_dict.keys():\n",
    "    #FOR PROVINCE IN THE SCI-DICT\n",
    "    \n",
    "    # Creating a local copy for computation\n",
    "    df = df_cov_c.copy()\n",
    "    \n",
    "    # Get the weights for the province\n",
    "    weights = sci_dict[prov]\n",
    "    \n",
    "    # Then, we weigh the local copy of the dataframe\n",
    "    # by the weights in the sci_dict\n",
    "    for neigh_prov in weights.keys():  \n",
    "        #FOR EACH PROVINCE APART FROM prov\n",
    "        \n",
    "        # Weighting the observations by SCI weights\n",
    "        df[neigh_prov] = df[neigh_prov]*weights[neigh_prov]\n",
    "    \n",
    "    # Collecting the names of the neighbors\n",
    "    neighs = list(weights.keys())\n",
    "    \n",
    "    # Summing the weighted observations across all provinces \n",
    "    # not equal to the province we are processing the data for.\n",
    "    df_spc[prov] = df[neighs].sum(axis=1)\n",
    "    \n",
    "    # Setting the data to the correct format (i.e ln(x+1)) so\n",
    "    # that we won't have minus-valued predicted cases.\n",
    "    df_spc[prov] = np.log(df_spc[prov]+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = plt.figure()\n",
    "f.set_figheight(9)\n",
    "f.set_figwidth(15)\n",
    "\n",
    "ax1 = f.add_subplot(111)\n",
    "\n",
    "\n",
    "\n",
    "sns.lineplot(x='Y-W',\n",
    "             y='value',\n",
    "             hue = 'variable',\n",
    "             data=pd.melt(df_spc, ['Y-W']),\n",
    "             ax=ax1)\n",
    "\n",
    "fontdict = {'fontsize': 20,\n",
    "#  'fontweight' : rcParams['axes.titleweight'],\n",
    "#  'verticalalignment': 'baseline',\n",
    "#  'horizontalalignment': loc}\n",
    "           }\n",
    "plt.xlim(0,115)\n",
    "ax1.xaxis.set_major_locator(ticker.MultipleLocator(5))\n",
    "ax1.xaxis.set_major_formatter(ticker.ScalarFormatter())\n",
    "ax1.set_title(r\"Social Proximity to Cases (SPC))\",fontdict = fontdict)\n",
    "ax1.set_xlabel(r\"Week number (starting from 2020-01)\",fontsize=14)\n",
    "ax1.set_ylabel(r\"$SPC_{i,t}$\",rotation=0,labelpad=30,fontsize=14)\n",
    "\n",
    "\n",
    "\n",
    "leg = plt.legend(title='Province', loc='upper left', labels=list(df_spc.columns[0:-1]))\n",
    "# leg._legend_box.align = \"top\"\n",
    "plt.setp(leg.get_title(),fontsize='large')\n",
    "plt.savefig('/home/dante/SpatialData/spatial_project/data/processed/SPC_timeseries.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- I have a feeling that these are always driven by Uusimaa.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This now holds the SCI weighted cases!    \n",
    "df_spc.head(15)\n",
    "\n",
    "# df_spc.to_csv('/home/dante/SpatialData/spatial_project/data/processed/spc_by_week_province.csv',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Weather data\n",
    "- The weather data was gathered from the Finnish Meteorological Institute (FMI)\n",
    "https://en.ilmatieteenlaitos.fi/download-observations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filepath = '/home/dante/SpatialData/spatial_project/data/weather/'\n",
    "filelist = [file for file in os.listdir(filepath) if '.csv' in file]\n",
    "provinces = pd.read_csv('/home/dante/SpatialData/spatial_project/scripts/mob_map_nuts3.txt',sep=',').iloc[:,3].values.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initializing an empty list to fill with weather data\n",
    "df_list = []\n",
    "\n",
    "for idx,prov in enumerate(provinces):\n",
    "    # FOR EACH PROVINCE NAME\n",
    "    \n",
    "    # Create filename\n",
    "    path2data = filepath + str(prov) + '.csv'\n",
    "    \n",
    "    # Fetch data, parse it\n",
    "    df = pd.read_csv(path2data,\n",
    "                     parse_dates = {\"date\" : [\"Year\",\"m\",\"d\"]},\n",
    "                     dtype = {'Time zone' : 'object',\n",
    "                              'Precipitation amount (mm)' : 'float64',\n",
    "                              'Maximum temperature (degC)' : 'float64',\n",
    "                              'Minimum temperature (degC)' : 'float64'},\n",
    "#                      nrows=1,\n",
    "                     header=0)\n",
    "    \n",
    "    print(df.shape)\n",
    "    # Assigning columns of interest\n",
    "    cols = list(df.columns)[2:]\n",
    "        \n",
    "    # The below code bins the data according to week commencing MONDAY\n",
    "    # This code looks complicated, but achieves something quite understanable.\n",
    "    df['date'] = pd.to_datetime(df['date']) - pd.to_timedelta(7, unit='d')\n",
    "    df = df.groupby([pd.Grouper(key='date', freq='W-MON')])[cols].mean().reset_index().sort_values('date')\n",
    "\n",
    "    # Creating the column with which we can then merge to the cases data..\n",
    "    df['Y-W'] = df.date.dt.year.astype(str)+'-'+df.date.dt.isocalendar().week.astype(str)\n",
    "    \n",
    "    if idx == 0:\n",
    "        y_w = df['Y-W']\n",
    "    \n",
    "    # Adding name of province\n",
    "    df['province'] = prov\n",
    "    df.reset_index(drop=True) \n",
    "    \n",
    "    # Setting first week of January to start on 2020-01-01\n",
    "    # as per data from THL (covid source)\n",
    "    df.iloc[0,-2] = '2020-1'\n",
    "\n",
    "    # Finally, appending to the list of dfs...\n",
    "    df_list.append(df)\n",
    "    \n",
    "\n",
    "\n",
    "# concatenating it all into one big df...\n",
    "weather_data = pd.concat(df_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# weather_data.pivot('date','province','Maximum temperature (degC)').head(150).reset_index().isna().sum()\n",
    "# weather_data.pivot('date','province','Minimum temperature (degC)').head(150).reset_index().isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting the max temperature reading per weep for all provinces\n",
    "max_temp_data = weather_data.pivot('date','province','Maximum temperature (degC)')\n",
    "max_temp_data['Y-W'] = y_w.values\n",
    "max_temp_data.reset_index(drop=False)\n",
    "\n",
    "# Getting the min temperature reading per weep for all provinces\n",
    "min_temp_data = weather_data.pivot('date','province','Minimum temperature (degC)')\n",
    "min_temp_data['Y-W'] = y_w.values\n",
    "min_temp_data.reset_index(drop=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# min_temp_data.to_csv('/home/dante/SpatialData/spatial_project/data/processed/min_temp_by_week_province.csv',index=False)\n",
    "# max_temp_data.to_csv('/home/dante/SpatialData/spatial_project/data/processed/max_temp_by_week_province.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating final dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Temperature data\n",
    "df_temp_min = pd.read_csv('/home/dante/SpatialData/spatial_project/data/processed/min_temp_by_week_province.csv')\n",
    "df_temp_max = pd.read_csv('/home/dante/SpatialData/spatial_project/data/processed/max_temp_by_week_province.csv')\n",
    "\n",
    "# Socioeconomic data\n",
    "\n",
    "# Google mobility data\n",
    "df_mobility = pd.read_csv('/home/dante/SpatialData/spatial_project/data/processed/google_mobility_by_week_province.csv')\n",
    "\n",
    "# Social Proximity to Cases data\n",
    "df_spc = pd.read_csv('/home/dante/SpatialData/spatial_project/data/processed/spc_by_week_province.csv')\n",
    "\n",
    "# New cases by week by province data\n",
    "df_covid = pd.read_csv('/home/dante/SpatialData/spatial_project/data/processed/newcases_by_week_province.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The data will all be merged into this one dataframe:\n",
    "df_master = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Taking the Y-W denotations to create a map later on.\n",
    "yw_list = df_covid['Y-W'].value_counts().index.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['2020-01',\n",
       " '2020-02',\n",
       " '2020-03',\n",
       " '2020-04',\n",
       " '2020-05',\n",
       " '2020-06',\n",
       " '2020-07',\n",
       " '2020-08',\n",
       " '2020-09',\n",
       " '2020-10',\n",
       " '2020-11',\n",
       " '2020-12',\n",
       " '2020-13',\n",
       " '2020-14',\n",
       " '2020-15',\n",
       " '2020-16',\n",
       " '2020-17',\n",
       " '2020-18',\n",
       " '2020-19',\n",
       " '2020-20',\n",
       " '2020-21',\n",
       " '2020-22',\n",
       " '2020-23',\n",
       " '2020-24',\n",
       " '2020-25',\n",
       " '2020-26',\n",
       " '2020-27',\n",
       " '2020-28',\n",
       " '2020-29',\n",
       " '2020-30',\n",
       " '2020-31',\n",
       " '2020-32',\n",
       " '2020-33',\n",
       " '2020-34',\n",
       " '2020-35',\n",
       " '2020-36',\n",
       " '2020-37',\n",
       " '2020-38',\n",
       " '2020-39',\n",
       " '2020-40',\n",
       " '2020-41',\n",
       " '2020-42',\n",
       " '2020-43',\n",
       " '2020-44',\n",
       " '2020-45',\n",
       " '2020-46',\n",
       " '2020-47',\n",
       " '2020-48',\n",
       " '2020-49',\n",
       " '2020-50',\n",
       " '2020-51',\n",
       " '2020-52',\n",
       " '2020-53',\n",
       " '2021-01',\n",
       " '2021-02',\n",
       " '2021-03',\n",
       " '2021-04',\n",
       " '2021-05',\n",
       " '2021-06',\n",
       " '2021-07',\n",
       " '2021-08',\n",
       " '2021-09',\n",
       " '2021-10',\n",
       " '2021-11',\n",
       " '2021-12',\n",
       " '2021-13',\n",
       " '2021-14',\n",
       " '2021-15',\n",
       " '2021-16',\n",
       " '2021-17',\n",
       " '2021-18',\n",
       " '2021-19',\n",
       " '2021-20',\n",
       " '2021-21',\n",
       " '2021-22',\n",
       " '2021-23',\n",
       " '2021-24',\n",
       " '2021-25',\n",
       " '2021-26',\n",
       " '2021-27',\n",
       " '2021-28',\n",
       " '2021-29',\n",
       " '2021-30',\n",
       " '2021-31',\n",
       " '2021-32',\n",
       " '2021-33',\n",
       " '2021-34',\n",
       " '2021-35',\n",
       " '2021-36',\n",
       " '2021-37',\n",
       " '2021-38',\n",
       " '2021-39',\n",
       " '2021-40',\n",
       " '2021-41',\n",
       " '2021-42',\n",
       " '2021-43',\n",
       " '2021-44',\n",
       " '2021-45',\n",
       " '2021-46',\n",
       " '2021-47',\n",
       " '2021-48',\n",
       " '2021-49',\n",
       " '2021-50',\n",
       " '2021-51',\n",
       " '2021-52',\n",
       " '2022-01',\n",
       " '2022-02',\n",
       " '2022-03',\n",
       " '2022-04',\n",
       " '2022-05',\n",
       " '2022-06',\n",
       " '2022-07',\n",
       " '2022-08',\n",
       " '2022-09',\n",
       " '2022-10']"
      ]
     },
     "execution_count": 279,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "yw_list.sort()\n",
    "yw_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A note about units\n",
    "- The SPC data format is in ln(1+casesper10kpopulation)\n",
    "- The COVID data format is in new cases by week by province.\n",
    "\n",
    "Therefore, we need to change the units of the COVID data to reflect the same data format as with the SPC data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a copy of the COVID df so that we can always restart\n",
    "# the modifications from here.\n",
    "df_cov_c = df_covid.copy()\n",
    "\n",
    "# Getting the population data from earlier in the script..\n",
    "prov_pop_dict = {\n",
    " 'Central Finland': 272617,\n",
    " 'Central Ostrobothnia': 67988,\n",
    " 'Kainuu': 71664,\n",
    " 'Tavastia Proper': 170577,\n",
    " 'Kymenlaakso': 162812,\n",
    " 'Lapland': 176665,\n",
    " 'North Karelia': 163537,\n",
    " 'Northern Ostrobothnia': 413830,\n",
    " 'Northern Savonia': 248265,\n",
    " 'Ostrobothnia': 175816,\n",
    " 'Pirkanmaa': 522852,\n",
    " 'Päijänne Tavastia': 205771,\n",
    " 'Satakunta': 215416,\n",
    " 'South Karelia': 126921,\n",
    " 'Southern Ostrobothnia': 192150,\n",
    " 'Southern Savonia': 132702,\n",
    " 'Southwest Finland': 481403,\n",
    " 'Uusimaa': 1702678\n",
    "}\n",
    "\n",
    "# The below for-loop scales the covid cases for each day x province \n",
    "# to the level of new cases per 10k people. In this way, we are stand-\n",
    "# ardizing the measurement across provinces.\n",
    "for idx,prov in enumerate(df_covid.columns[0:-1]):\n",
    "    #FOR EACH PROVINCE\n",
    "\n",
    "    # For each province, fetch the population of the province,\n",
    "    # and then divide the whole time series for that province\n",
    "    # by the population.\n",
    "    case_by_pop = df_cov_c[prov] / prov_pop_dict[prov]\n",
    "    \n",
    "    # Multiply the per capita number by 10000, to get cases\n",
    "    # per 10k people.\n",
    "    case_by_10k = case_by_pop * 10000\n",
    "    \n",
    "    # Assign back to the dataframe.\n",
    "    df_cov_c[prov] = np.log(case_by_10k+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "metadata": {},
   "outputs": [],
   "source": [
    "yw_dict = dict(zip(list(range(len(yw_list))),yw_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 2, 3, 4, 5]"
      ]
     },
     "execution_count": 282,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "periods = 5\n",
    "periods = list(range(1,periods+1))\n",
    "periods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tminus(dataframe,periods):\n",
    "    \"\"\"\n",
    "    Creates int(periods) dataframes out of one,\n",
    "    with each dataframe having it's time-period\n",
    "    shifted FORWARD by one week.\n",
    "    \"\"\"\n",
    "    periods = list(range(1,periods+1))\n",
    "\n",
    "    \n",
    "    shifted_list = []\n",
    "    for t_minus in periods:\n",
    "        df_shifted = dataframe.shift(1*t_minus)\n",
    "        shifted_list.append(df_shifted)\n",
    "    return shifted_list\n",
    "\n",
    "def tplus(dataframe,periods):\n",
    "    \"\"\"\n",
    "    Creates int(periods) dataframes out of one,\n",
    "    with each dataframe having it's time-period\n",
    "    shifted BACKWARD by one week.\n",
    "    \"\"\"\n",
    "\n",
    "    \n",
    "    shifted_list = []\n",
    "    for t_minus in periods:\n",
    "        df_shifted = dataframe.shift(-1*t_minus)\n",
    "        shifted_list.append(df_shifted)\n",
    "    return shifted_list\n",
    "    \n",
    "def longitudinizer(dataframe_list,varname,ywdict):\n",
    "    \"\"\"\n",
    "    Puts the data in the right format\n",
    "    for use by the xgboost algorithm.\n",
    "    \"\"\"\n",
    "    \n",
    "    out_list = []\n",
    "    for idx,dataframe in enumerate(dataframe_list):\n",
    "#         print(dataframe)\n",
    "        df_unpivoted = dataframe.iloc[:,0:-1].T.reset_index(drop=False).rename(columns={'index' : 'province'})\n",
    "#         print(df_unpivoted)\n",
    "        df_out = df_unpivoted.melt(id_vars=['province'], var_name='Y-W', value_name=varname)\n",
    "#         print(df_out)\n",
    "        if len(dataframe_list) > 1:\n",
    "            df_out['Y-W'] = df_out['Y-W']+(idx+1)\n",
    "            df_out['Y-W'] = df_out['Y-W'].map(ywdict)\n",
    "        else:\n",
    "            df_out['Y-W'] = df_out['Y-W'].map(ywdict)\n",
    "#         print(df_out)\n",
    "        out_list.append(df_out)\n",
    "    return out_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## COVID DATA -> Longitudinal format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dependent variable\n",
    "df_covids = longitudinizer([df_cov_c],'newcases',yw_dict)[0]\n",
    "df_master = df_covids.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Time lags of dependent variable\n",
    "covid_frames = longitudinizer(tminus(df_cov_c,4),'newcases',yw_dict)\n",
    "\n",
    "# Forward lags of dependent variable (for choosing dep.var)\n",
    "forecasting_periods = [1,2,4]\n",
    "covid_futures = longitudinizer(tplus(df_cov_c,forecasting_periods),'newcases',yw_dict)\n",
    "\n",
    "# Inputting into master:\n",
    "for idx,frame in enumerate(covid_frames):\n",
    "    frame.sort_values(by=['Y-W','province'],inplace=True)\n",
    "    colname = f\"newcases_tminus{idx+1}\"\n",
    "    df_master[colname] = frame.iloc[:,2]\n",
    "    \n",
    "for idx,frame in enumerate(covid_futures):\n",
    "    frame.sort_values(by=['Y-W','province'],inplace=True)\n",
    "    colname = f\"newcases_tplus{forecasting_periods[idx]}\"\n",
    "    df_master[colname] = frame.iloc[:,2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SOCIAL PROXIMITY TO CASES -> Longitudinal format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "metadata": {},
   "outputs": [],
   "source": [
    "spc_frames = longitudinizer(tminus(df_spc,4),'spc',yw_dict)\n",
    "\n",
    "\n",
    "# Putting into master dataframe\n",
    "for idx, frame in enumerate(spc_frames):\n",
    "    frame.sort_values(by=['Y-W','province'],inplace=True)\n",
    "    colname = f\"spc_tminus{idx+1}\"\n",
    "    df_master[colname] = frame.iloc[:,2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## WEATHER DATA -> Longitudinal format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 307,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[     Central Finland  Central Ostrobothnia     Kainuu  Kymenlaakso    Lapland  \\\n",
       " 0                NaN                   NaN        NaN          NaN        NaN   \n",
       " 1          -3.516667             -1.483333  -3.483333    -3.600000  -9.500000   \n",
       " 2          -1.657143             -1.114286  -1.471429    -1.642857  -6.414286   \n",
       " 3          -0.585714             -1.457143  -2.171429     0.928571  -8.957143   \n",
       " 4          -5.600000             -5.285714  -9.457143    -3.028571 -14.628571   \n",
       " ..               ...                   ...        ...          ...        ...   \n",
       " 110       -13.185714            -12.157143 -13.542857    -8.785714 -14.028571   \n",
       " 111        -6.014286             -8.742857  -9.814286    -4.671429 -12.228571   \n",
       " 112        -4.885714             -8.185714  -8.257143    -2.885714 -11.814286   \n",
       " 113       -11.057143             -9.685714 -12.800000    -7.400000 -12.057143   \n",
       " 114        -6.585714             -7.014286  -7.857143    -7.371429  -7.228571   \n",
       " \n",
       "      North Karelia  Northern Ostrobothnia  Northern Savonia  Ostrobothnia  \\\n",
       " 0              NaN                    NaN               NaN           NaN   \n",
       " 1        -5.483333              -9.683333         -3.033333      0.083333   \n",
       " 2        -2.757143              -7.057143         -0.657143      0.585714   \n",
       " 3        -0.800000              -8.442857         -0.200000     -1.071429   \n",
       " 4        -7.271429             -16.985714         -5.885714     -1.328571   \n",
       " ..             ...                    ...               ...           ...   \n",
       " 110     -11.785714             -14.328571        -11.371429    -10.528571   \n",
       " 111      -7.357143             -12.771429         -5.528571     -6.328571   \n",
       " 112      -3.714286             -13.314286         -4.128571     -6.785714   \n",
       " 113     -10.842857             -11.771429        -10.600000     -7.028571   \n",
       " 114      -9.485714              -7.728571         -6.828571     -3.957143   \n",
       " \n",
       "      Pirkanmaa  Päijänne Tavastia  Satakunta  South Karelia  \\\n",
       " 0          NaN                NaN        NaN            NaN   \n",
       " 1    -1.366667          -3.016667  -0.300000      -2.950000   \n",
       " 2    -1.285714          -1.700000  -0.242857      -1.114286   \n",
       " 3     1.371429           1.200000   2.342857       0.657143   \n",
       " 4    -1.857143          -2.657143  -0.428571      -3.500000   \n",
       " ..         ...                ...        ...            ...   \n",
       " 110  -9.071429          -7.114286 -10.742857     -10.000000   \n",
       " 111  -5.071429          -5.471429  -2.685714      -3.671429   \n",
       " 112  -3.100000          -2.814286  -3.042857      -2.542857   \n",
       " 113  -8.314286          -8.485714  -4.785714      -6.914286   \n",
       " 114  -5.571429          -7.442857  -3.000000      -6.342857   \n",
       " \n",
       "      Southern Ostrobothnia  Southern Savonia  Southwest Finland  \\\n",
       " 0                      NaN               NaN                NaN   \n",
       " 1                -1.300000         -4.433333           0.700000   \n",
       " 2                -1.642857         -1.542857          -0.585714   \n",
       " 3                -0.914286          0.471429           2.585714   \n",
       " 4                -4.600000         -5.457143           0.014286   \n",
       " ..                     ...               ...                ...   \n",
       " 110             -11.485714        -12.357143          -9.371429   \n",
       " 111             -10.057143         -5.314286          -3.285714   \n",
       " 112              -8.128571         -3.385714          -2.671429   \n",
       " 113              -9.000000        -10.171429          -4.585714   \n",
       " 114              -7.571429         -8.500000          -3.114286   \n",
       " \n",
       "      Tavastia Proper   Uusimaa     Y-W  \n",
       " 0                NaN       NaN     NaN  \n",
       " 1          -1.966667  0.416667  2020-1  \n",
       " 2          -1.300000  1.071429  2020-2  \n",
       " 3           1.328571  2.928571  2020-3  \n",
       " 4          -1.585714  0.285714  2020-4  \n",
       " ..               ...       ...     ...  \n",
       " 110        -9.757143 -6.657143  2022-5  \n",
       " 111        -5.114286 -2.042857  2022-6  \n",
       " 112        -3.485714 -0.828571  2022-7  \n",
       " 113        -9.200000 -4.500000  2022-8  \n",
       " 114        -6.328571 -3.014286  2022-9  \n",
       " \n",
       " [115 rows x 19 columns],\n",
       "      Central Finland  Central Ostrobothnia     Kainuu  Kymenlaakso    Lapland  \\\n",
       " 0                NaN                   NaN        NaN          NaN        NaN   \n",
       " 1                NaN                   NaN        NaN          NaN        NaN   \n",
       " 2          -3.516667             -1.483333  -3.483333    -3.600000  -9.500000   \n",
       " 3          -1.657143             -1.114286  -1.471429    -1.642857  -6.414286   \n",
       " 4          -0.585714             -1.457143  -2.171429     0.928571  -8.957143   \n",
       " ..               ...                   ...        ...          ...        ...   \n",
       " 110        -8.485714             -9.442857 -10.685714    -7.328571 -14.100000   \n",
       " 111       -13.185714            -12.157143 -13.542857    -8.785714 -14.028571   \n",
       " 112        -6.014286             -8.742857  -9.814286    -4.671429 -12.228571   \n",
       " 113        -4.885714             -8.185714  -8.257143    -2.885714 -11.814286   \n",
       " 114       -11.057143             -9.685714 -12.800000    -7.400000 -12.057143   \n",
       " \n",
       "      North Karelia  Northern Ostrobothnia  Northern Savonia  Ostrobothnia  \\\n",
       " 0              NaN                    NaN               NaN           NaN   \n",
       " 1              NaN                    NaN               NaN           NaN   \n",
       " 2        -5.483333              -9.683333         -3.033333      0.083333   \n",
       " 3        -2.757143              -7.057143         -0.657143      0.585714   \n",
       " 4        -0.800000              -8.442857         -0.200000     -1.071429   \n",
       " ..             ...                    ...               ...           ...   \n",
       " 110      -8.528571             -15.214286         -7.228571     -6.942857   \n",
       " 111     -11.785714             -14.328571        -11.371429    -10.528571   \n",
       " 112      -7.357143             -12.771429         -5.528571     -6.328571   \n",
       " 113      -3.714286             -13.314286         -4.128571     -6.785714   \n",
       " 114     -10.842857             -11.771429        -10.600000     -7.028571   \n",
       " \n",
       "      Pirkanmaa  Päijänne Tavastia  Satakunta  South Karelia  \\\n",
       " 0          NaN                NaN        NaN            NaN   \n",
       " 1          NaN                NaN        NaN            NaN   \n",
       " 2    -1.366667          -3.016667  -0.300000      -2.950000   \n",
       " 3    -1.285714          -1.700000  -0.242857      -1.114286   \n",
       " 4     1.371429           1.200000   2.342857       0.657143   \n",
       " ..         ...                ...        ...            ...   \n",
       " 110  -6.942857          -6.700000  -5.842857      -6.342857   \n",
       " 111  -9.071429          -7.114286 -10.742857     -10.000000   \n",
       " 112  -5.071429          -5.471429  -2.685714      -3.671429   \n",
       " 113  -3.100000          -2.814286  -3.042857      -2.542857   \n",
       " 114  -8.314286          -8.485714  -4.785714      -6.914286   \n",
       " \n",
       "      Southern Ostrobothnia  Southern Savonia  Southwest Finland  \\\n",
       " 0                      NaN               NaN                NaN   \n",
       " 1                      NaN               NaN                NaN   \n",
       " 2                -1.300000         -4.433333           0.700000   \n",
       " 3                -1.642857         -1.542857          -0.585714   \n",
       " 4                -0.914286          0.471429           2.585714   \n",
       " ..                     ...               ...                ...   \n",
       " 110             -10.571429         -8.100000          -4.657143   \n",
       " 111             -11.485714        -12.357143          -9.371429   \n",
       " 112             -10.057143         -5.314286          -3.285714   \n",
       " 113              -8.128571         -3.385714          -2.671429   \n",
       " 114              -9.000000        -10.171429          -4.585714   \n",
       " \n",
       "      Tavastia Proper   Uusimaa     Y-W  \n",
       " 0                NaN       NaN     NaN  \n",
       " 1                NaN       NaN     NaN  \n",
       " 2          -1.966667  0.416667  2020-1  \n",
       " 3          -1.300000  1.071429  2020-2  \n",
       " 4           1.328571  2.928571  2020-3  \n",
       " ..               ...       ...     ...  \n",
       " 110        -7.142857 -3.500000  2022-4  \n",
       " 111        -9.757143 -6.657143  2022-5  \n",
       " 112        -5.114286 -2.042857  2022-6  \n",
       " 113        -3.485714 -0.828571  2022-7  \n",
       " 114        -9.200000 -4.500000  2022-8  \n",
       " \n",
       " [115 rows x 19 columns],\n",
       "      Central Finland  Central Ostrobothnia     Kainuu  Kymenlaakso    Lapland  \\\n",
       " 0                NaN                   NaN        NaN          NaN        NaN   \n",
       " 1                NaN                   NaN        NaN          NaN        NaN   \n",
       " 2                NaN                   NaN        NaN          NaN        NaN   \n",
       " 3          -3.516667             -1.483333  -3.483333    -3.600000  -9.500000   \n",
       " 4          -1.657143             -1.114286  -1.471429    -1.642857  -6.414286   \n",
       " ..               ...                   ...        ...          ...        ...   \n",
       " 110        -7.528571             -5.485714  -7.314286    -5.842857  -9.585714   \n",
       " 111        -8.485714             -9.442857 -10.685714    -7.328571 -14.100000   \n",
       " 112       -13.185714            -12.157143 -13.542857    -8.785714 -14.028571   \n",
       " 113        -6.014286             -8.742857  -9.814286    -4.671429 -12.228571   \n",
       " 114        -4.885714             -8.185714  -8.257143    -2.885714 -11.814286   \n",
       " \n",
       "      North Karelia  Northern Ostrobothnia  Northern Savonia  Ostrobothnia  \\\n",
       " 0              NaN                    NaN               NaN           NaN   \n",
       " 1              NaN                    NaN               NaN           NaN   \n",
       " 2              NaN                    NaN               NaN           NaN   \n",
       " 3        -5.483333              -9.683333         -3.033333      0.083333   \n",
       " 4        -2.757143              -7.057143         -0.657143      0.585714   \n",
       " ..             ...                    ...               ...           ...   \n",
       " 110      -6.528571              -9.914286         -7.585714     -3.700000   \n",
       " 111      -8.528571             -15.214286         -7.228571     -6.942857   \n",
       " 112     -11.785714             -14.328571        -11.371429    -10.528571   \n",
       " 113      -7.357143             -12.771429         -5.528571     -6.328571   \n",
       " 114      -3.714286             -13.314286         -4.128571     -6.785714   \n",
       " \n",
       "      Pirkanmaa  Päijänne Tavastia  Satakunta  South Karelia  \\\n",
       " 0          NaN                NaN        NaN            NaN   \n",
       " 1          NaN                NaN        NaN            NaN   \n",
       " 2          NaN                NaN        NaN            NaN   \n",
       " 3    -1.366667          -3.016667  -0.300000      -2.950000   \n",
       " 4    -1.285714          -1.700000  -0.242857      -1.114286   \n",
       " ..         ...                ...        ...            ...   \n",
       " 110  -5.671429          -5.942857  -4.900000      -4.414286   \n",
       " 111  -6.942857          -6.700000  -5.842857      -6.342857   \n",
       " 112  -9.071429          -7.114286 -10.742857     -10.000000   \n",
       " 113  -5.071429          -5.471429  -2.685714      -3.671429   \n",
       " 114  -3.100000          -2.814286  -3.042857      -2.542857   \n",
       " \n",
       "      Southern Ostrobothnia  Southern Savonia  Southwest Finland  \\\n",
       " 0                      NaN               NaN                NaN   \n",
       " 1                      NaN               NaN                NaN   \n",
       " 2                      NaN               NaN                NaN   \n",
       " 3                -1.300000         -4.433333           0.700000   \n",
       " 4                -1.642857         -1.542857          -0.585714   \n",
       " ..                     ...               ...                ...   \n",
       " 110              -6.742857         -6.357143          -5.171429   \n",
       " 111             -10.571429         -8.100000          -4.657143   \n",
       " 112             -11.485714        -12.357143          -9.371429   \n",
       " 113             -10.057143         -5.314286          -3.285714   \n",
       " 114              -8.128571         -3.385714          -2.671429   \n",
       " \n",
       "      Tavastia Proper   Uusimaa     Y-W  \n",
       " 0                NaN       NaN     NaN  \n",
       " 1                NaN       NaN     NaN  \n",
       " 2                NaN       NaN     NaN  \n",
       " 3          -1.966667  0.416667  2020-1  \n",
       " 4          -1.300000  1.071429  2020-2  \n",
       " ..               ...       ...     ...  \n",
       " 110        -6.042857 -3.685714  2022-3  \n",
       " 111        -7.142857 -3.500000  2022-4  \n",
       " 112        -9.757143 -6.657143  2022-5  \n",
       " 113        -5.114286 -2.042857  2022-6  \n",
       " 114        -3.485714 -0.828571  2022-7  \n",
       " \n",
       " [115 rows x 19 columns],\n",
       "      Central Finland  Central Ostrobothnia     Kainuu  Kymenlaakso    Lapland  \\\n",
       " 0                NaN                   NaN        NaN          NaN        NaN   \n",
       " 1                NaN                   NaN        NaN          NaN        NaN   \n",
       " 2                NaN                   NaN        NaN          NaN        NaN   \n",
       " 3                NaN                   NaN        NaN          NaN        NaN   \n",
       " 4          -3.516667             -1.483333  -3.483333    -3.600000  -9.500000   \n",
       " ..               ...                   ...        ...          ...        ...   \n",
       " 110        -8.200000             -4.914286  -5.842857    -7.614286  -9.457143   \n",
       " 111        -7.528571             -5.485714  -7.314286    -5.842857  -9.585714   \n",
       " 112        -8.485714             -9.442857 -10.685714    -7.328571 -14.100000   \n",
       " 113       -13.185714            -12.157143 -13.542857    -8.785714 -14.028571   \n",
       " 114        -6.014286             -8.742857  -9.814286    -4.671429 -12.228571   \n",
       " \n",
       "      North Karelia  Northern Ostrobothnia  Northern Savonia  Ostrobothnia  \\\n",
       " 0              NaN                    NaN               NaN           NaN   \n",
       " 1              NaN                    NaN               NaN           NaN   \n",
       " 2              NaN                    NaN               NaN           NaN   \n",
       " 3              NaN                    NaN               NaN           NaN   \n",
       " 4        -5.483333              -9.683333         -3.033333      0.083333   \n",
       " ..             ...                    ...               ...           ...   \n",
       " 110     -11.357143             -10.385714         -8.542857     -3.142857   \n",
       " 111      -6.528571              -9.914286         -7.585714     -3.700000   \n",
       " 112      -8.528571             -15.214286         -7.228571     -6.942857   \n",
       " 113     -11.785714             -14.328571        -11.371429    -10.528571   \n",
       " 114      -7.357143             -12.771429         -5.528571     -6.328571   \n",
       " \n",
       "      Pirkanmaa  Päijänne Tavastia  Satakunta  South Karelia  \\\n",
       " 0          NaN                NaN        NaN            NaN   \n",
       " 1          NaN                NaN        NaN            NaN   \n",
       " 2          NaN                NaN        NaN            NaN   \n",
       " 3          NaN                NaN        NaN            NaN   \n",
       " 4    -1.366667          -3.016667  -0.300000      -2.950000   \n",
       " ..         ...                ...        ...            ...   \n",
       " 110  -6.128571          -6.842857  -3.600000      -8.185714   \n",
       " 111  -5.671429          -5.942857  -4.900000      -4.414286   \n",
       " 112  -6.942857          -6.700000  -5.842857      -6.342857   \n",
       " 113  -9.071429          -7.114286 -10.742857     -10.000000   \n",
       " 114  -5.071429          -5.471429  -2.685714      -3.671429   \n",
       " \n",
       "      Southern Ostrobothnia  Southern Savonia  Southwest Finland  \\\n",
       " 0                      NaN               NaN                NaN   \n",
       " 1                      NaN               NaN                NaN   \n",
       " 2                      NaN               NaN                NaN   \n",
       " 3                      NaN               NaN                NaN   \n",
       " 4                -1.300000         -4.433333           0.700000   \n",
       " ..                     ...               ...                ...   \n",
       " 110              -7.442857         -8.614286          -3.042857   \n",
       " 111              -6.742857         -6.357143          -5.171429   \n",
       " 112             -10.571429         -8.100000          -4.657143   \n",
       " 113             -11.485714        -12.357143          -9.371429   \n",
       " 114             -10.057143         -5.314286          -3.285714   \n",
       " \n",
       "      Tavastia Proper   Uusimaa     Y-W  \n",
       " 0                NaN       NaN     NaN  \n",
       " 1                NaN       NaN     NaN  \n",
       " 2                NaN       NaN     NaN  \n",
       " 3                NaN       NaN     NaN  \n",
       " 4          -1.966667  0.416667  2020-1  \n",
       " ..               ...       ...     ...  \n",
       " 110        -6.400000 -3.900000  2022-2  \n",
       " 111        -6.042857 -3.685714  2022-3  \n",
       " 112        -7.142857 -3.500000  2022-4  \n",
       " 113        -9.757143 -6.657143  2022-5  \n",
       " 114        -5.114286 -2.042857  2022-6  \n",
       " \n",
       " [115 rows x 19 columns]]"
      ]
     },
     "execution_count": 307,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tminus(df_temp_min,4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_temp_frames = longitudinizer(tminus(df_temp_min,4),'temp_min',yw_dict)\n",
    "\n",
    "# Putting into master dataframe\n",
    "for idx, frame in enumerate(min_temp_frames):\n",
    "    frame.sort_values(by=['Y-W','province'],inplace=True)\n",
    "    colname = f\"temp_min_tminus{idx+1}\"\n",
    "    df_master[colname] = frame.iloc[:,2]\n",
    "\n",
    "max_temp_frames = longitudinizer(tminus(df_temp_max,4),'temp_max',yw_dict)\n",
    "\n",
    "# Putting into master dataframe\n",
    "for idx, frame in enumerate(max_temp_frames):\n",
    "    frame.sort_values(by=['Y-W','province'],inplace=True)\n",
    "    colname = f\"temp_max_tminus{idx+1}\"\n",
    "    df_master[colname] = frame.iloc[:,2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>province</th>\n",
       "      <th>Y-W</th>\n",
       "      <th>newcases</th>\n",
       "      <th>newcases_tminus1</th>\n",
       "      <th>newcases_tminus2</th>\n",
       "      <th>newcases_tminus3</th>\n",
       "      <th>newcases_tminus4</th>\n",
       "      <th>newcases_tplus1</th>\n",
       "      <th>newcases_tplus2</th>\n",
       "      <th>newcases_tplus4</th>\n",
       "      <th>...</th>\n",
       "      <th>spc_tminus3</th>\n",
       "      <th>spc_tminus4</th>\n",
       "      <th>temp_min_tminus1</th>\n",
       "      <th>temp_min_tminus2</th>\n",
       "      <th>temp_min_tminus3</th>\n",
       "      <th>temp_min_tminus4</th>\n",
       "      <th>temp_max_tminus1</th>\n",
       "      <th>temp_max_tminus2</th>\n",
       "      <th>temp_max_tminus3</th>\n",
       "      <th>temp_max_tminus4</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>South Karelia</td>\n",
       "      <td>2020-01</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>South Karelia</td>\n",
       "      <td>2020-02</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-3.516667</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.450000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>South Karelia</td>\n",
       "      <td>2020-03</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-1.657143</td>\n",
       "      <td>-3.516667</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.185714</td>\n",
       "      <td>2.450000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54</th>\n",
       "      <td>South Karelia</td>\n",
       "      <td>2020-04</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.585714</td>\n",
       "      <td>-1.657143</td>\n",
       "      <td>-3.516667</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3.657143</td>\n",
       "      <td>2.185714</td>\n",
       "      <td>2.450000</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72</th>\n",
       "      <td>South Karelia</td>\n",
       "      <td>2020-05</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-5.600000</td>\n",
       "      <td>-0.585714</td>\n",
       "      <td>-1.657143</td>\n",
       "      <td>-3.516667</td>\n",
       "      <td>0.328571</td>\n",
       "      <td>3.657143</td>\n",
       "      <td>2.185714</td>\n",
       "      <td>2.450000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90</th>\n",
       "      <td>South Karelia</td>\n",
       "      <td>2020-06</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-3.900000</td>\n",
       "      <td>-5.600000</td>\n",
       "      <td>-0.585714</td>\n",
       "      <td>-1.657143</td>\n",
       "      <td>-0.985714</td>\n",
       "      <td>0.328571</td>\n",
       "      <td>3.657143</td>\n",
       "      <td>2.185714</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>108</th>\n",
       "      <td>South Karelia</td>\n",
       "      <td>2020-07</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-7.100000</td>\n",
       "      <td>-3.900000</td>\n",
       "      <td>-5.600000</td>\n",
       "      <td>-0.585714</td>\n",
       "      <td>-0.785714</td>\n",
       "      <td>-0.985714</td>\n",
       "      <td>0.328571</td>\n",
       "      <td>3.657143</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>126</th>\n",
       "      <td>South Karelia</td>\n",
       "      <td>2020-08</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.500000</td>\n",
       "      <td>-7.100000</td>\n",
       "      <td>-3.900000</td>\n",
       "      <td>-5.600000</td>\n",
       "      <td>2.257143</td>\n",
       "      <td>-0.785714</td>\n",
       "      <td>-0.985714</td>\n",
       "      <td>0.328571</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>144</th>\n",
       "      <td>South Karelia</td>\n",
       "      <td>2020-09</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.514286</td>\n",
       "      <td>-1.500000</td>\n",
       "      <td>-7.100000</td>\n",
       "      <td>-3.900000</td>\n",
       "      <td>2.771429</td>\n",
       "      <td>2.257143</td>\n",
       "      <td>-0.785714</td>\n",
       "      <td>-0.985714</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>162</th>\n",
       "      <td>South Karelia</td>\n",
       "      <td>2020-10</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-11.571429</td>\n",
       "      <td>-1.514286</td>\n",
       "      <td>-1.500000</td>\n",
       "      <td>-7.100000</td>\n",
       "      <td>-0.828571</td>\n",
       "      <td>2.771429</td>\n",
       "      <td>2.257143</td>\n",
       "      <td>-0.785714</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10 rows × 22 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          province      Y-W  newcases  newcases_tminus1  newcases_tminus2  \\\n",
       "0    South Karelia  2020-01       0.0               NaN               NaN   \n",
       "18   South Karelia  2020-02       0.0               0.0               NaN   \n",
       "36   South Karelia  2020-03       0.0               0.0               0.0   \n",
       "54   South Karelia  2020-04       0.0               0.0               0.0   \n",
       "72   South Karelia  2020-05       0.0               0.0               0.0   \n",
       "90   South Karelia  2020-06       0.0               0.0               0.0   \n",
       "108  South Karelia  2020-07       0.0               0.0               0.0   \n",
       "126  South Karelia  2020-08       0.0               0.0               0.0   \n",
       "144  South Karelia  2020-09       0.0               0.0               0.0   \n",
       "162  South Karelia  2020-10       0.0               0.0               0.0   \n",
       "\n",
       "     newcases_tminus3  newcases_tminus4  newcases_tplus1  newcases_tplus2  \\\n",
       "0                 NaN               NaN              0.0              0.0   \n",
       "18                NaN               NaN              0.0              0.0   \n",
       "36                NaN               NaN              0.0              0.0   \n",
       "54                0.0               NaN              0.0              0.0   \n",
       "72                0.0               0.0              0.0              0.0   \n",
       "90                0.0               0.0              0.0              0.0   \n",
       "108               0.0               0.0              0.0              0.0   \n",
       "126               0.0               0.0              0.0              0.0   \n",
       "144               0.0               0.0              0.0              0.0   \n",
       "162               0.0               0.0              0.0              0.0   \n",
       "\n",
       "     newcases_tplus4  ...  spc_tminus3  spc_tminus4  temp_min_tminus1  \\\n",
       "0                0.0  ...          NaN          NaN               NaN   \n",
       "18               0.0  ...          NaN          NaN         -3.516667   \n",
       "36               0.0  ...          NaN          NaN         -1.657143   \n",
       "54               0.0  ...          0.0          NaN         -0.585714   \n",
       "72               0.0  ...          0.0          0.0         -5.600000   \n",
       "90               0.0  ...          0.0          0.0         -3.900000   \n",
       "108              0.0  ...          0.0          0.0         -7.100000   \n",
       "126              0.0  ...          0.0          0.0         -1.500000   \n",
       "144              0.0  ...          0.0          0.0         -1.514286   \n",
       "162              0.0  ...          0.0          0.0        -11.571429   \n",
       "\n",
       "     temp_min_tminus2  temp_min_tminus3  temp_min_tminus4  temp_max_tminus1  \\\n",
       "0                 NaN               NaN               NaN               NaN   \n",
       "18                NaN               NaN               NaN          2.450000   \n",
       "36          -3.516667               NaN               NaN          2.185714   \n",
       "54          -1.657143         -3.516667               NaN          3.657143   \n",
       "72          -0.585714         -1.657143         -3.516667          0.328571   \n",
       "90          -5.600000         -0.585714         -1.657143         -0.985714   \n",
       "108         -3.900000         -5.600000         -0.585714         -0.785714   \n",
       "126         -7.100000         -3.900000         -5.600000          2.257143   \n",
       "144         -1.500000         -7.100000         -3.900000          2.771429   \n",
       "162         -1.514286         -1.500000         -7.100000         -0.828571   \n",
       "\n",
       "     temp_max_tminus2  temp_max_tminus3  temp_max_tminus4  \n",
       "0                 NaN               NaN               NaN  \n",
       "18                NaN               NaN               NaN  \n",
       "36           2.450000               NaN               NaN  \n",
       "54           2.185714          2.450000               NaN  \n",
       "72           3.657143          2.185714          2.450000  \n",
       "90           0.328571          3.657143          2.185714  \n",
       "108         -0.985714          0.328571          3.657143  \n",
       "126         -0.785714         -0.985714          0.328571  \n",
       "144          2.257143         -0.785714         -0.985714  \n",
       "162          2.771429          2.257143         -0.785714  \n",
       "\n",
       "[10 rows x 22 columns]"
      ]
     },
     "execution_count": 289,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_master[df_master.province == 'South Karelia'].head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GOOGLE MOVEMENT DATA -> Longitudinal format."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Notice that we have a set of different kinds of mobility categories:\n",
    " - retail_and_recreation_percent_change_from_baseline;\n",
    " - grocery_and_pharmacy_percent_change_from_baseline;\n",
    " - parks_percent_change_from_baseline;\n",
    " - transit_stations_percent_change_from_baseline;\n",
    " - workplaces_percent_change_from_baseline; and\n",
    " - residential_percent_change_from_baseline.\n",
    " \n",
    " - We will create lags from all of these."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_mobility.sort_values(by=['Y-W','province'],inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Index contains duplicate entries, cannot reshape",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Input \u001b[0;32mIn [311]\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mdf_mobility\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpivot\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mY-W\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mprovince\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mretail_and_recreation_percent_change_from_baseline\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/pandas/core/frame.py:7793\u001b[0m, in \u001b[0;36mDataFrame.pivot\u001b[0;34m(self, index, columns, values)\u001b[0m\n\u001b[1;32m   7788\u001b[0m \u001b[38;5;129m@Substitution\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   7789\u001b[0m \u001b[38;5;129m@Appender\u001b[39m(_shared_docs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpivot\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m   7790\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpivot\u001b[39m(\u001b[38;5;28mself\u001b[39m, index\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, columns\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, values\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m DataFrame:\n\u001b[1;32m   7791\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mreshape\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpivot\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m pivot\n\u001b[0;32m-> 7793\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mpivot\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindex\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mindex\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalues\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalues\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/pandas/core/reshape/pivot.py:517\u001b[0m, in \u001b[0;36mpivot\u001b[0;34m(data, index, columns, values)\u001b[0m\n\u001b[1;32m    515\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    516\u001b[0m         indexed \u001b[38;5;241m=\u001b[39m data\u001b[38;5;241m.\u001b[39m_constructor_sliced(data[values]\u001b[38;5;241m.\u001b[39m_values, index\u001b[38;5;241m=\u001b[39mmultiindex)\n\u001b[0;32m--> 517\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mindexed\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43munstack\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcolumns_listlike\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/pandas/core/series.py:4081\u001b[0m, in \u001b[0;36mSeries.unstack\u001b[0;34m(self, level, fill_value)\u001b[0m\n\u001b[1;32m   4042\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   4043\u001b[0m \u001b[38;5;124;03mUnstack, also known as pivot, Series with MultiIndex to produce DataFrame.\u001b[39;00m\n\u001b[1;32m   4044\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   4077\u001b[0m \u001b[38;5;124;03mb    2    4\u001b[39;00m\n\u001b[1;32m   4078\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   4079\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mreshape\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mreshape\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m unstack\n\u001b[0;32m-> 4081\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43munstack\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfill_value\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/pandas/core/reshape/reshape.py:460\u001b[0m, in \u001b[0;36munstack\u001b[0;34m(obj, level, fill_value)\u001b[0m\n\u001b[1;32m    458\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_1d_only_ea_dtype(obj\u001b[38;5;241m.\u001b[39mdtype):\n\u001b[1;32m    459\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _unstack_extension_series(obj, level, fill_value)\n\u001b[0;32m--> 460\u001b[0m unstacker \u001b[38;5;241m=\u001b[39m \u001b[43m_Unstacker\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    461\u001b[0m \u001b[43m    \u001b[49m\u001b[43mobj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mindex\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlevel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlevel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconstructor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mobj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_constructor_expanddim\u001b[49m\n\u001b[1;32m    462\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    463\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m unstacker\u001b[38;5;241m.\u001b[39mget_result(\n\u001b[1;32m    464\u001b[0m     obj\u001b[38;5;241m.\u001b[39m_values, value_columns\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, fill_value\u001b[38;5;241m=\u001b[39mfill_value\n\u001b[1;32m    465\u001b[0m )\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/pandas/core/reshape/reshape.py:133\u001b[0m, in \u001b[0;36m_Unstacker.__init__\u001b[0;34m(self, index, level, constructor)\u001b[0m\n\u001b[1;32m    130\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m num_rows \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m num_columns \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m num_cells \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    131\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnstacked DataFrame is too big, causing int32 overflow\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 133\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_selectors\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/pandas/core/reshape/reshape.py:185\u001b[0m, in \u001b[0;36m_Unstacker._make_selectors\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    182\u001b[0m mask\u001b[38;5;241m.\u001b[39mput(selector, \u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m    184\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m mask\u001b[38;5;241m.\u001b[39msum() \u001b[38;5;241m<\u001b[39m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindex):\n\u001b[0;32m--> 185\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIndex contains duplicate entries, cannot reshape\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    187\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgroup_index \u001b[38;5;241m=\u001b[39m comp_index\n\u001b[1;32m    188\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmask \u001b[38;5;241m=\u001b[39m mask\n",
      "\u001b[0;31mValueError\u001b[0m: Index contains duplicate entries, cannot reshape"
     ]
    }
   ],
   "source": [
    "df_mobility.pivot('Y-W','province','retail_and_recreation_percent_change_from_baseline')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                  province  \\\n",
      "1401                   NaN   \n",
      "1354       Central Finland   \n",
      "1260  Central Ostrobothnia   \n",
      "1448                Kainuu   \n",
      "1495           Kymenlaakso   \n",
      "\n",
      "      retail_and_recreation_percent_change_from_baseline      Y-W  \n",
      "1401                                                NaN       NaN  \n",
      "1354                                           2.142857   2020-10  \n",
      "1260                                          -0.428571   2020-10  \n",
      "1448                                          14.285714   2020-10  \n",
      "1495                                           2.142857   2020-10  \n",
      "                  province  grocery_and_pharmacy_percent_change_from_baseline  \\\n",
      "1401                   NaN                                                NaN   \n",
      "1354       Central Finland                                           6.142857   \n",
      "1260  Central Ostrobothnia                                          10.000000   \n",
      "1448                Kainuu                                          11.428571   \n",
      "1495           Kymenlaakso                                           5.000000   \n",
      "\n",
      "          Y-W  \n",
      "1401      NaN  \n",
      "1354  2020-10  \n",
      "1260  2020-10  \n",
      "1448  2020-10  \n",
      "1495  2020-10  \n",
      "                  province  parks_percent_change_from_baseline      Y-W\n",
      "1401                   NaN                                 NaN      NaN\n",
      "1354       Central Finland                           20.142857  2020-10\n",
      "1260  Central Ostrobothnia                            0.000000  2020-10\n",
      "1448                Kainuu                            0.000000  2020-10\n",
      "1495           Kymenlaakso                            3.571429  2020-10\n",
      "                  province  transit_stations_percent_change_from_baseline  \\\n",
      "1401                   NaN                                            NaN   \n",
      "1354       Central Finland                                      -1.428571   \n",
      "1260  Central Ostrobothnia                                       0.285714   \n",
      "1448                Kainuu                                      -2.142857   \n",
      "1495           Kymenlaakso                                      -0.285714   \n",
      "\n",
      "          Y-W  \n",
      "1401      NaN  \n",
      "1354  2020-10  \n",
      "1260  2020-10  \n",
      "1448  2020-10  \n",
      "1495  2020-10  \n",
      "                  province  workplaces_percent_change_from_baseline      Y-W\n",
      "1401                   NaN                                      NaN      NaN\n",
      "1354       Central Finland                                -1.285714  2020-10\n",
      "1260  Central Ostrobothnia                                -2.571429  2020-10\n",
      "1448                Kainuu                               -20.857143  2020-10\n",
      "1495           Kymenlaakso                                 1.428571  2020-10\n",
      "                  province  residential_percent_change_from_baseline      Y-W\n",
      "1401                   NaN                                       NaN      NaN\n",
      "1354       Central Finland                                  0.428571  2020-10\n",
      "1260  Central Ostrobothnia                                  1.000000  2020-10\n",
      "1448                Kainuu                                  2.428571  2020-10\n",
      "1495           Kymenlaakso                                  0.285714  2020-10\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "for cat in mob cat\n",
    "    create longitudinal dataset with timelags\n",
    "    add to master\n",
    "        This includes matching the dates...\n",
    "\"\"\"\n",
    "mobility_categories = [cat for cat in df_mobility.columns if \"baseline\" in cat]\n",
    "\n",
    "mob_dfs = []\n",
    "for cat in mobility_categories:\n",
    "    name = cat.split(\"_\")[0]\n",
    "    df = df_mobility[['province',cat,'Y-W']]\n",
    "#     print(df.head)\n",
    "    shifted_frames = tminus(df,4)\n",
    "    print(shifted_frames[0].head())\n",
    "#     mobility_frames = longitudinizer(tminus(df,4),name,yw_dict)\n",
    "#     mob_dfs.append(mobility_frames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>province</th>\n",
       "      <th>Y-W</th>\n",
       "      <th>residential</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>province</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>residential_percent_change_from_baseline</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>province</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Central Finland</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>residential_percent_change_from_baseline</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.428571</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>province</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Central Ostrobothnia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3919</th>\n",
       "      <td>residential_percent_change_from_baseline</td>\n",
       "      <td>NaN</td>\n",
       "      <td>6.714286</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3920</th>\n",
       "      <td>province</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Southwest Finland</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3921</th>\n",
       "      <td>residential_percent_change_from_baseline</td>\n",
       "      <td>NaN</td>\n",
       "      <td>8.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3922</th>\n",
       "      <td>province</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Tavastia Proper</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3923</th>\n",
       "      <td>residential_percent_change_from_baseline</td>\n",
       "      <td>NaN</td>\n",
       "      <td>7.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3924 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                      province  Y-W           residential\n",
       "0                                     province  NaN                   NaN\n",
       "1     residential_percent_change_from_baseline  NaN                   NaN\n",
       "2                                     province  NaN       Central Finland\n",
       "3     residential_percent_change_from_baseline  NaN              0.428571\n",
       "4                                     province  NaN  Central Ostrobothnia\n",
       "...                                        ...  ...                   ...\n",
       "3919  residential_percent_change_from_baseline  NaN              6.714286\n",
       "3920                                  province  NaN     Southwest Finland\n",
       "3921  residential_percent_change_from_baseline  NaN                   8.0\n",
       "3922                                  province  NaN       Tavastia Proper\n",
       "3923  residential_percent_change_from_baseline  NaN                   7.0\n",
       "\n",
       "[3924 rows x 3 columns]"
      ]
     },
     "execution_count": 306,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "longitudinizer(shifted_frames[0:2],'residential',yw_dict)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>province</th>\n",
       "      <th>Y-W</th>\n",
       "      <th>retail</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>province</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Y-W</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>province</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Central Finland</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Y-W</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2020-10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>province</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Central Ostrobothnia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3919</th>\n",
       "      <td>Y-W</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2022-8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3920</th>\n",
       "      <td>province</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Southwest Finland</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3921</th>\n",
       "      <td>Y-W</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2022-8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3922</th>\n",
       "      <td>province</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Tavastia Proper</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3923</th>\n",
       "      <td>Y-W</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2022-8</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3924 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      province  Y-W                retail\n",
       "0     province  NaN                   NaN\n",
       "1          Y-W  NaN                   NaN\n",
       "2     province  NaN       Central Finland\n",
       "3          Y-W  NaN               2020-10\n",
       "4     province  NaN  Central Ostrobothnia\n",
       "...        ...  ...                   ...\n",
       "3919       Y-W  NaN                2022-8\n",
       "3920  province  NaN     Southwest Finland\n",
       "3921       Y-W  NaN                2022-8\n",
       "3922  province  NaN       Tavastia Proper\n",
       "3923       Y-W  NaN                2022-8\n",
       "\n",
       "[3924 rows x 3 columns]"
      ]
     },
     "execution_count": 294,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mob_dfs[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['retail_and_recreation_percent_change_from_baseline',\n",
       " 'grocery_and_pharmacy_percent_change_from_baseline',\n",
       " 'parks_percent_change_from_baseline',\n",
       " 'transit_stations_percent_change_from_baseline',\n",
       " 'workplaces_percent_change_from_baseline',\n",
       " 'residential_percent_change_from_baseline']"
      ]
     },
     "execution_count": 260,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mobility_categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = gpd.read_file( input_directory + 'Contiguous_US.geojson')\n",
    "\n",
    "# Or alternatively:\n",
    "# url='https://drive.google.com/file/d/1MVyLzzHl3hzno4o1rLZtI0peqIi23zsr/view?usp=sharing'\n",
    "# url_counties='https://drive.google.com/uc?id=' + url.split('/')[-2]\n",
    "# data = gpd.read_file(url_counties)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['STATEFP'] = data.apply(lambda L: L.GEOID[:2], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "global number_counties \n",
    "number_counties = data.shape[0] #3103"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data.sort_values(by='GEOID', inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load COVID data and apply smoothing \n",
    "\n",
    "To alleviate inconsistencies in reporting COVID-19 cases, we apply a 7-day moving average to the case data published by JHU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_JH_covid_data(target, smooth):\n",
    "    \n",
    "    '''\n",
    "    Parameters:\n",
    "    --------------\n",
    "        target: str\n",
    "            the target variable: either 'case' or 'death'\n",
    "            \n",
    "        smooth: bool\n",
    "            wether to smooth the data frame or not.\n",
    "            The smoothing is done by using a 7-day rolling average   \n",
    "    '''\n",
    "    \n",
    "    assert isinstance(smooth, bool), \"Smooth must be a boolean variable!\"\n",
    "    \n",
    "    base_url = 'https://raw.githubusercontent.com/CSSEGISandData/COVID-19/master/csse_covid_19_data/'\n",
    "    \n",
    "    \n",
    "    if target == 'case':\n",
    "        jh_data_url = base_url + 'csse_covid_19_time_series/time_series_covid19_confirmed_US.csv'\n",
    "        cols_to_drp = ['UID', 'iso2', 'iso3', 'code3','Country_Region', 'Lat', 'Long_']\n",
    "\n",
    "    elif target=='death':\n",
    "        jh_data_url = base_url + 'csse_covid_19_time_series/time_series_covid19_deaths_US.csv'\n",
    "        cols_to_drp = ['UID', 'iso2', 'iso3', 'code3','Country_Region', 'Lat', 'Long_','Population']\n",
    "    else:\n",
    "        print(\"invalid argument for target. Acceptable values are: 'case' or 'death'\")\n",
    "        return None\n",
    "\n",
    "    jh_covid_df = pd.read_csv(jh_data_url)\n",
    "\n",
    "    # preprocessing JH COVID data\n",
    "    jh_covid_df.dropna(axis=0, how='any', inplace=True)\n",
    "\n",
    "    jh_covid_df['FIPS'] = jh_covid_df['FIPS'].astype('int64')\n",
    "\n",
    "    jh_covid_df.drop(columns=cols_to_drp, inplace=True)\n",
    "\n",
    "    #Important: check to see the column index is adherent to the imported df\n",
    "\n",
    "    first_date = datetime.strptime(jh_covid_df.columns[4], '%m/%d/%y').date()\n",
    "\n",
    "    last_date = datetime.strptime(jh_covid_df.columns[-1], '%m/%d/%y').date()\n",
    "\n",
    "\n",
    "    current_date = last_date\n",
    "\n",
    "    previous_date = last_date - timedelta (days=1)\n",
    "\n",
    "\n",
    "    while current_date > first_date:\n",
    "\n",
    "        #For unix, replace # with - in the time format\n",
    "\n",
    "        current_col = current_date.strftime(conversion_format) #replace # with - in Mac or Linux\n",
    "\n",
    "        previous_col = previous_date.strftime(conversion_format)\n",
    "\n",
    "        jh_covid_df[previous_col] = np.where(jh_covid_df[previous_col] > jh_covid_df[current_col], \n",
    "                                             jh_covid_df[current_col], jh_covid_df[previous_col])\n",
    "\n",
    "        current_date = current_date - timedelta(days=1)\n",
    "\n",
    "        previous_date = previous_date - timedelta(days=1)\n",
    "        \n",
    "    \n",
    "    if smooth:\n",
    "        jh_covid_df.iloc[:,4:] = jh_covid_df.iloc[:,4:].rolling(7,min_periods=1,axis=1).mean()\n",
    "\n",
    "\n",
    "    return jh_covid_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "covid_df = get_JH_covid_data('case', True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Facebook Movement Range Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fb_mobility = pd.read_csv(input_directory + 'movement-range-2021-03-02.txt', sep=\"\\t\", dtype={'polygon_id':str})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fb_mobility_us = fb_mobility[fb_mobility['country']=='USA']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of unique counties for which we have at least one day of data\n",
    "len(fb_mobility_us['polygon_id'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# counting Counties in the contiguous US for which there is no data in FB mobility\n",
    "contiguous_fips = set(data['GEOID']) # number of unique fips: 3103\n",
    "mobility_fips = set(fb_mobility_us['polygon_id']) # number of unique fips: 2694\n",
    "i = 0\n",
    "missing_fips = []\n",
    "for fips in contiguous_fips:\n",
    "    if (fips in mobility_fips):\n",
    "        i+=1\n",
    "    else:\n",
    "        missing_fips.append(fips)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# number of Counties in the contiguous US for which there is no data in FB mobility\n",
    "len(missing_fips)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new dataframe as transpose of the above, with days as columns and counties as rows\n",
    "\n",
    "relative_df = pd.DataFrame(columns=fb_mobility_us['ds'].unique(), index=data['GEOID'].unique(), dtype=float)\n",
    "ratio_df = pd.DataFrame(columns=fb_mobility_us['ds'].unique(), index=data['GEOID'].unique(), dtype=float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fb_mobility_us"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx_contiguous = fb_mobility_us.index[fb_mobility_us['polygon_id'].isin(contiguous_fips)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fb_mobility_contiguous = fb_mobility_us.loc[idx_contiguous]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "start = time.time()\n",
    "\n",
    "for index, row in fb_mobility_contiguous.iterrows():\n",
    "    relative_df.loc[row['polygon_id']][row['ds']] = row['all_day_bing_tiles_visited_relative_change']\n",
    "    ratio_df.loc[row['polygon_id']][row['ds']] = row['all_day_ratio_single_tile_users']\n",
    "    \n",
    "\n",
    "end = time.time()\n",
    "print(end - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "relative_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(relative_df.shape , ratio_df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Impute FB mobility dataframes\n",
    "The two dataframes above have a lot of Nan values which should be imputed by state average"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_ratio_df = data[['GEOID', 'STATEFP']].merge(ratio_df, left_on='GEOID', right_index=True, how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in temp_ratio_df.iloc[:,2:].columns:\n",
    "    temp_ratio_df[col] = temp_ratio_df.groupby('STATEFP')[col].transform(lambda x: x.fillna(x.mean()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_ratio_df.isna().sum().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_relative_df = data[['GEOID', 'STATEFP']].merge(relative_df, left_on='GEOID', right_index=True, how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in temp_relative_df.iloc[:,2:].columns:\n",
    "    temp_relative_df[col] = temp_relative_df.groupby('STATEFP')[col].transform(lambda x: x.fillna(x.mean()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_relative_df.isna().sum().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "relative_df_smooth = temp_relative_df.copy()\n",
    "ratio_df_smooth = temp_ratio_df.copy()\n",
    "\n",
    "relative_df_smooth.iloc[:,2:] = relative_df_smooth.iloc[:,2:].rolling(7,min_periods=1, axis=1).mean()\n",
    "ratio_df_smooth.iloc[:,2:] = ratio_df_smooth.iloc[:,2:].rolling(7,min_periods=1, axis=1).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "relative_df_smooth.iloc[:,2:].columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "relative_df_smooth.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add Social Proximity to Cases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SCI_df = pd.read_csv(input_directory + 'SCI_matrix.csv', dtype={'Unnamed: 0':str})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SCI_df.set_index('Unnamed: 0', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create normalized SCI. It is calculated by dividing all the columns of the sci_matrix by the sum of the rpw\n",
    "# This would give us the second term in social proximity formula above\n",
    "\n",
    "sci_matrix_normal = SCI_df.div(SCI_df.sum(axis=1), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set diagonal to zero\n",
    "sci_matrix_normal.values[[np.arange(sci_matrix_normal.shape[0])]*2] = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The matrix above is created for the entire US, but we are using contiguous US data here, therefore some rows and columns should be removed**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_drop=[]\n",
    "\n",
    "for index in sci_matrix_normal.index:\n",
    "    if not index in contiguous_fips:\n",
    "        to_drop.append(index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sci_matrix_normal.drop(to_drop, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sci_matrix_normal.drop(to_drop, axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sci_matrix_normal.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add SafeGraph mobility features\n",
    "\n",
    "Updated based on forecast hub dates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "safegraph_mobility = pd.read_csv(input_directory + 'safegraph_mobility.csv', dtype={'county_fips':str})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "safegraph_contiguous = safegraph_mobility[safegraph_mobility['county_fips'].isin(contiguous_fips)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(safegraph_contiguous['county_fips'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_cols = safegraph_contiguous.drop(['start_date', 'end_date', 'base_start', 'base_end'], axis=1)\n",
    "safegraph_metrics = temp_cols.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(safegraph_metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Temperature Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_temp = pd.read_csv(input_directory + 'max_temp_df_2021.csv', dtype={'GEOID':str})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_temp = pd.read_csv(input_directory + 'min_temp_df_2021.csv', dtype={'GEOID':str})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# return FCI-normal table for the input date\n",
    "# set path_to_fci to where FCI matrices are stored\n",
    "def get_normal_fci(date):\n",
    "    path_to_fci = './output/' + str(date.year) + '/'+ date.strftime('%m') + \n",
    "                '/FCI_normal/' + date.strftime('%Y-%m-%d') + '-FCI-normal.csv'\n",
    "    fci_norm = pd.read_csv(path_to_fci, dtype={'Unnamed: 0':str})\n",
    "    fci_norm.set_index('Unnamed: 0', inplace=True)\n",
    "    \n",
    "    to_drop=[]\n",
    "\n",
    "    for index in fci_norm.index:\n",
    "        if not index in contiguous_fips:\n",
    "            to_drop.append(index)\n",
    "            \n",
    "    fci_norm.drop(to_drop, inplace=True)\n",
    "    fci_norm.drop(to_drop, axis=1, inplace=True)\n",
    "    return fci_norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculates weekly average FPC using the end date and the start date of the week\n",
    "# the input to this fuction should be of type datetime\n",
    "def weekly_mean_FPC(end_date, start_date, logged=False):\n",
    "    \n",
    "    dates = [end_date]\n",
    "    while end_date> start_date:\n",
    "        end_date -= timedelta(days=1)\n",
    "        dates.append(end_date)\n",
    "    \n",
    "    \n",
    "    temp = data[['GEOID','FIPS', 'POPULATION']]\n",
    "    \n",
    "    \n",
    "    for date in dates:\n",
    "        # convert date to String\n",
    "        date_str = date.strftime('%Y-%m-%d')\n",
    "        \n",
    "        temp = temp.merge(covid_df_contiguous[['FIPS', date_str]], on='FIPS', how='left')\n",
    "        \n",
    "        if logged:\n",
    "            temp['inc_rate_' + date_str] = np.log(temp[date_str] / temp['POPULATION'] * 10000 + 1)\n",
    "            \n",
    "        else:\n",
    "            temp['inc_rate_' + date_str] = temp[date_str] / temp['POPULATION'] * 10000\n",
    "        \n",
    "        normal_fci = get_normal_fci(date)\n",
    "        normal_fci = normal_fci.merge(temp, left_index= True, right_on='GEOID')\n",
    "        \n",
    "        normal_fci['fpc_'+ date_str] = np.dot(normal_fci.iloc[:,:number_counties], normal_fci['inc_rate_' + date_str])\n",
    "        \n",
    "    \n",
    "    normal_fci['mean_fpc'] = normal_fci.iloc[:,-len(dates):].mean(axis=1)\n",
    "        \n",
    "    return normal_fci[['GEOID','mean_fpc']]    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculates weekly average SPC\n",
    "# the input to this fuction should be of type datetime\n",
    "def weekly_mean_SPC(end_date, start_date, logged=False):\n",
    "    \n",
    "    dates = [end_date]\n",
    "    while end_date> start_date:\n",
    "        end_date -= timedelta(days=1)\n",
    "        dates.append(end_date)\n",
    "    \n",
    "    \n",
    "    temp = data[['GEOID','FIPS', 'POPULATION']]\n",
    "    \n",
    "    for date in dates:\n",
    "        # convert date to String\n",
    "        date_str = date.strftime('%Y-%m-%d')\n",
    "        \n",
    "        temp = temp.merge(covid_df_contiguous[['FIPS', date_str]], on='FIPS', how='left')\n",
    "        \n",
    "        if logged:\n",
    "            temp['inc_rate_' + date_str] = np.log(temp[date_str] / temp['POPULATION'] * 10000 + 1)\n",
    "            \n",
    "        else:\n",
    "            temp['inc_rate_' + date_str] = temp[date_str] / temp['POPULATION'] * 10000\n",
    "        \n",
    "        \n",
    "        normal_sci = sci_matrix_normal.merge(temp, left_index= True, right_on='GEOID')\n",
    "        \n",
    "        normal_sci['spc_'+ date_str] = np.dot(normal_sci.iloc[:,:number_counties], normal_sci['inc_rate_' + date_str])\n",
    "        \n",
    "    \n",
    "    normal_sci['mean_spc'] = normal_sci.iloc[:,-len(dates):].mean(axis=1)\n",
    "        \n",
    "    return normal_sci[['GEOID','mean_spc']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the input to this fuction should be of type datetime.\n",
    "# returns a subset of FB movement range dfs based on the given week\n",
    "def weekly_fb_mobility(end_date, start_date, df):\n",
    "    \n",
    "    dates = [end_date]\n",
    "    while end_date> start_date:\n",
    "        end_date -= timedelta(days=1)\n",
    "        dates.append(end_date)\n",
    "    \n",
    "    dates_str=[]\n",
    "    for date in dates:\n",
    "        # convert date to String\n",
    "        dates_str.append(date.strftime('%Y-%m-%d'))\n",
    "    \n",
    "    return df[['GEOID', *dates_str]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate slope features\n",
    "\n",
    "from scipy.stats import linregress\n",
    "\n",
    "def linear_reg(week_df):\n",
    "    \n",
    "    x = np.arange(1,(week_df.shape[1]),1)\n",
    "    x = (x - np.mean(x))/ np.std(x)\n",
    "    \n",
    "    slopes=[]\n",
    "    \n",
    "    for index, row in week_df.iloc[:,1:].iterrows():\n",
    "        y = row\n",
    "        y = (y - np.mean(y))/ np.std(y)\n",
    "        slopes.append(linregress(x, y)[0])\n",
    "        \n",
    "    week_df.loc[:,'slope'] = slopes\n",
    "    return week_df[['GEOID','slope']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final function to combine all features generated above\n",
    "\n",
    "This function generates a dataframe and for a given date, will add the following features to the dataframe\n",
    "\n",
    "- incidence rate data\n",
    "- FB mobility data (ratio, relative)\n",
    "- SPC (facebook SCI and incidence rates)\n",
    "- SafeGraph mobility \n",
    "- FPC (FCI and incidence rate)\n",
    "\n",
    "For each period, there is a 5 week difference between the actual date (t) and the start of the second lag. For example if `T: Oct 1 (Sep 24 to Oct 1)`, then `T-1: Sep 10 to Sep 24`, and `T-2: August 27 to Sep 10`.\n",
    "\n",
    "Since the earliest day for which we have FB mobility data is March 1, the rearliest  (end) date for T will be April 5th."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "covid_df_contiguous = data[['FIPS','STATEFP','COUNTYFP','GEOID']].merge(covid_df, on='FIPS', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main function\n",
    "# the input to the funtion is the end date for which we want to add data\n",
    "\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "\n",
    "def add_lagged_data(date):\n",
    "    global data\n",
    "    \n",
    "    T_end = date\n",
    "    T_start = T_end - timedelta(days=6)\n",
    "    \n",
    "    T_1_end = T_start - timedelta(days=1)\n",
    "    T_1_start = T_1_end - timedelta(days=6)\n",
    "    \n",
    "    T_2_end = T_1_start - timedelta(days=1)\n",
    "    T_2_start = T_2_end - timedelta(days=6)\n",
    "    \n",
    "    T_3_end = T_2_start - timedelta(days=1)\n",
    "    T_3_start = T_3_end - timedelta(days=6)\n",
    "    \n",
    "    T_4_end = T_3_start - timedelta(days=1)\n",
    "    T_4_start = T_4_end - timedelta(days=6)\n",
    "    \n",
    "    # These dates are used for cumulative cases (Saturday to Saturday)\n",
    "    T_start_case = T_end - timedelta(days=7)\n",
    "    T_1_start_case = T_1_end - timedelta(days=7)\n",
    "    T_2_start_case = T_2_end - timedelta(days=7)\n",
    "    T_3_start_case = T_3_end - timedelta(days=7)\n",
    "    T_4_start_case = T_4_end - timedelta(days=7)\n",
    "    \n",
    "\n",
    "    dates = [T_end.strftime('%Y-%m-%d'), T_start.strftime('%Y-%m-%d'),\n",
    "             T_1_end.strftime('%Y-%m-%d'), T_1_start.strftime('%Y-%m-%d'), \n",
    "             T_2_end.strftime('%Y-%m-%d'), T_2_start.strftime('%Y-%m-%d'),\n",
    "             T_3_end.strftime('%Y-%m-%d'), T_3_start.strftime('%Y-%m-%d'), \n",
    "             T_4_end.strftime('%Y-%m-%d'), T_4_start.strftime('%Y-%m-%d')]\n",
    "    \n",
    "    \n",
    "    dates_non_str = [T_end, T_start,\n",
    "             T_1_end, T_1_start, \n",
    "             T_2_end, T_2_start,\n",
    "             T_3_end, T_3_start, \n",
    "             T_4_end, T_4_start]\n",
    "    \n",
    "    dates_case = [T_end.strftime('%Y-%m-%d'), T_start_case.strftime('%Y-%m-%d'),\n",
    "             T_1_end.strftime('%Y-%m-%d'), T_1_start_case.strftime('%Y-%m-%d'), \n",
    "             T_2_end.strftime('%Y-%m-%d'), T_2_start_case.strftime('%Y-%m-%d'),\n",
    "             T_3_end.strftime('%Y-%m-%d'), T_3_start_case.strftime('%Y-%m-%d'), \n",
    "             T_4_end.strftime('%Y-%m-%d'), T_4_start_case.strftime('%Y-%m-%d')]\n",
    "    \n",
    "    \n",
    "    dates_non_str_case = [T_end, T_start_case,\n",
    "             T_1_end, T_1_start_case, \n",
    "             T_2_end, T_2_start_case,\n",
    "             T_3_end, T_3_start_case, \n",
    "             T_4_end, T_4_start_case]\n",
    "\n",
    "    \n",
    "    temp = data.copy()\n",
    "    \n",
    "    temp['date_end_period'] = T_end.strftime('%Y-%m-%d')\n",
    "    temp['date_start_period'] = T_start.strftime('%Y-%m-%d')\n",
    "    temp['date_end_lag'] = T_1_end.strftime('%Y-%m-%d')\n",
    "    temp['date_start_lag'] = T_4_start.strftime('%Y-%m-%d')\n",
    "    \n",
    "    time_periods = ['T_end', 'T_start', 'T_1_end', 'T_1_start','T_2_end','T_2_start',\n",
    "                    'T_3_end','T_3_start','T_4_end','T_4_start']\n",
    "    i = 0\n",
    "    for period in time_periods:\n",
    "        \n",
    "        \n",
    "        temp = temp.merge(covid_df_contiguous[['GEOID',dates_case[i]]], on='GEOID', how='left')\n",
    "        temp.rename(columns={dates_case[i]:'case_'+ period}, inplace=True)\n",
    "\n",
    "        \n",
    "        temp['inc_rate_' + period] = temp['case_'+ period] / temp['POPULATION'] * 10000\n",
    "        \n",
    "        temp = temp.merge(relative_df_smooth[['GEOID',dates[i]]], on='GEOID', how='left')\n",
    "        temp.rename(columns={dates[i]:'relative_'+ period}, inplace=True) \n",
    "\n",
    "\n",
    "        temp = temp.merge(ratio_df_smooth[['GEOID',dates[i]]], on='GEOID', how='left')\n",
    "        temp.rename(columns={dates[i]:'ratio_'+ period}, inplace=True)\n",
    "        \n",
    "        # The same date is used as the input to weekly_mean_SPC function to calculate\n",
    "        # SPC for that given date (instead of an average over a period)\n",
    "        temp = temp.merge(weekly_mean_SPC(dates_non_str_case[i],dates_non_str_case[i], logged=False), on='GEOID', how='left')\n",
    "        temp.rename(columns={'mean_spc':'SPC_'+ period}, inplace=True)\n",
    "\n",
    "        # simiar to SPC, add FPC values\n",
    "        temp = temp.merge(weekly_mean_FPC(dates_non_str_case[i],dates_non_str_case[i], logged=False), on='GEOID', how='left')\n",
    "        temp.rename(columns={'mean_fpc':'FPC_'+ period}, inplace=True)\n",
    "        \n",
    "        \n",
    "        # add logged SPC (defined as log(delta incidence rate)*sci/sum(sci))\n",
    "        temp = temp.merge(weekly_mean_SPC(dates_non_str_case[i],dates_non_str_case[i], logged=True), on='GEOID', how='left')\n",
    "        temp.rename(columns={'mean_spc':'SPC_logged_'+ period}, inplace=True)\n",
    "        \n",
    "        \n",
    "        # add logged FPC (defined as log(delta incidence rate)*fci/sum(fci))\n",
    "        temp = temp.merge(weekly_mean_FPC(dates_non_str_case[i],dates_non_str_case[i], logged=True), on='GEOID', how='left')\n",
    "        temp.rename(columns={'mean_fpc':'FPC_logged_'+ period}, inplace=True)\n",
    "\n",
    "        # add raw John Hopkins case data\n",
    "        temp = temp.merge(jh_covid_df[['FIPS',dates_non_str_case[i].strftime('%#m/%#d/%y')]], on='FIPS', how='left')\n",
    "        temp.rename(columns={dates_non_str_case[i].strftime('%#m/%#d/%y'):'case_JH_'+ period}, inplace=True)\n",
    "        \n",
    "        # add smoothed John Hopkins case data\n",
    "        temp = temp.merge(covid_df_contiguous[['FIPS',dates_case[i]]], on='FIPS', how='left')\n",
    "        temp.rename(columns={dates_case[i]:'case_JH_smoothed_'+ period}, inplace=True)\n",
    "       \n",
    "        \n",
    "        \n",
    "        i+=1\n",
    "    \n",
    "    times = ['T', 'T_1', 'T_2', 'T_3', 'T_4']\n",
    "    \n",
    "    j = 0\n",
    "    for period in times:\n",
    "        temp['DELTA_INC_RATE_' + period] = temp['inc_rate_' + time_periods[j]] - temp['inc_rate_' + time_periods[j+1]]\n",
    "        \n",
    "        temp['DELTA_REL_MOB_' + period] = temp['relative_' + time_periods[j]] - temp['relative_' + time_periods[j+1]]\n",
    "        temp['DELTA_RATIO_MOB_' + period] = temp['ratio_' + time_periods[j]] - temp['ratio_' + time_periods[j+1]]\n",
    "        \n",
    "        temp['DELTA_SPC_' + period] = temp['SPC_' + time_periods[j]] - temp['SPC_' + time_periods[j+1]]\n",
    "        temp['DELTA_FPC_' + period] = temp['FPC_' + time_periods[j]] - temp['FPC_' + time_periods[j+1]]\n",
    "        temp['DELTA_SPC_LOGGED_' + period] = temp['SPC_logged_' + time_periods[j]] - temp['SPC_logged_' + time_periods[j+1]]\n",
    "        temp['DELTA_FPC_LOGGED_' + period] = temp['FPC_logged_' + time_periods[j]] - temp['FPC_logged_' + time_periods[j+1]]\n",
    "        \n",
    "        temp['DELTA_CASE_JH_' + period] = temp['case_JH_'+ time_periods[j]] - temp['case_JH_'+ time_periods[j+1]]\n",
    "        temp['DELTA_CASE_JH_SMOOTH_' + period] = temp['case_JH_smoothed_'+ time_periods[j]] - \n",
    "                                                 temp['case_JH_smoothed_'+ time_periods[j+1]]\n",
    "        \n",
    "        # mean incidence rate is calculated between Sunday and Saturday\n",
    "        temp['MEAN_INC_RATE_' + period] = covid_df_contiguous[pd.date_range(start=dates[j+1], end=dates[j])\n",
    "                                            .strftime('%Y-%m-%d')].mean(axis=1) / temp['POPULATION'] * 10000\n",
    "        temp['MEAN_REL_MOB_' + period] = relative_df_smooth[pd.date_range(start=dates[j+1], end=dates[j])\n",
    "                                            .strftime('%Y-%m-%d')].mean(axis=1)\n",
    "        temp['MEAN_RATIO_MOB_' + period] = ratio_df_smooth[pd.date_range(start=dates[j+1], end=dates[j])\n",
    "                                            .strftime('%Y-%m-%d')].mean(axis=1)\n",
    "\n",
    "        \n",
    "        # add Safegraph mobility features\n",
    "        safegraph_data = safegraph_contiguous[safegraph_contiguous['end_date']==dates[j]][safegraph_metrics]\n",
    "        temp = temp.merge(safegraph_data, left_on='GEOID', right_on='county_fips')\n",
    "        \n",
    "        rename_dict = dict()\n",
    "        for col in safegraph_metrics[1:]:\n",
    "            rename_dict[col] = col + '_' + period\n",
    "            \n",
    "        temp.rename(columns=rename_dict, inplace=True)\n",
    "        \n",
    "        \n",
    "        # add MEAN_FPC\n",
    "        temp = temp.merge(weekly_mean_FPC(dates_non_str_case[j], dates_non_str_case[j+1]), on='GEOID', how='left')\n",
    "        temp.rename(columns={'mean_fpc':'MEAN_FPC_'+ period}, inplace=True)\n",
    "        \n",
    "        # add logged MEAN_FPC \n",
    "        temp = temp.merge(weekly_mean_FPC(dates_non_str_case[j], dates_non_str_case[j+1], logged=True), on='GEOID', how='left')\n",
    "        temp.rename(columns={'mean_fpc':'MEAN_FPC_LOGGED_'+ period}, inplace=True)\n",
    "        \n",
    "        # add MEAN_SPC\n",
    "        temp = temp.merge(weekly_mean_SPC(dates_non_str_case[j], dates_non_str_case[j+1]), on='GEOID', how='left')\n",
    "        temp.rename(columns={'mean_spc':'MEAN_SPC_'+ period}, inplace=True)\n",
    "        \n",
    "        # add logged MEAN_SPC\n",
    "        temp = temp.merge(weekly_mean_SPC(dates_non_str_case[j], dates_non_str_case[j+1], logged=True), on='GEOID', how='left')\n",
    "        temp.rename(columns={'mean_spc':'MEAN_SPC_LOGGED_'+ period}, inplace=True)\n",
    "        \n",
    "        # add FB mobility slopes\n",
    "        ratio_slope = linear_reg(weekly_fb_mobility(dates_non_str[j], dates_non_str[j+1], ratio_df_smooth))\n",
    "        temp = temp.merge(ratio_slope, on='GEOID', how='left')\n",
    "        temp.rename(columns={'slope':'SLOPE_RATIO_MOB_'+ period}, inplace=True)\n",
    "        \n",
    "        relative_slope = linear_reg(weekly_fb_mobility(dates_non_str[j], dates_non_str[j+1], relative_df_smooth))\n",
    "        temp = temp.merge(relative_slope, on='GEOID', how='left')\n",
    "        temp.rename(columns={'slope':'SLOPE_REL_MOB_'+ period}, inplace=True)\n",
    "\n",
    "        \n",
    "        \n",
    "        # add temperature features\n",
    "        # to update for the new dates, min and max temperature are used with one day offset\n",
    "        adj_temp_date = (dates_non_str[j] + timedelta(days=1)).strftime('%Y-%m-%d')\n",
    "        \n",
    "        temp = temp.merge(max_temp[['GEOID', adj_temp_date]], on='GEOID', how='left')\n",
    "        temp.rename(columns={adj_temp_date:'MAX_TEMP_'+ period}, inplace=True)\n",
    "        \n",
    "        temp = temp.merge(min_temp[['GEOID', adj_temp_date]], on='GEOID', how='left')\n",
    "        temp.rename(columns={adj_temp_date:'MIN_TEMP_'+ period}, inplace=True)\n",
    "\n",
    "        j += 2\n",
    "\n",
    "    output_df = temp.copy()\n",
    "    \n",
    "    return output_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "end_date = datetime(2021, 2, 27)\n",
    "week_counter = 0\n",
    "df_list = []\n",
    "\n",
    "while end_date > datetime(2020, 3, 29):\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    df_list.append(add_lagged_data(end_date))\n",
    "    \n",
    "    end_time = time.time()\n",
    "    print('Feature generation for the week ending in {} finished in {} seconds'.format(end_date.strftime('%Y-%m-%d'),\n",
    "                                                                                       round(end_time-start_time,1)))\n",
    "    \n",
    "    end_date -= timedelta(weeks=1)\n",
    "    week_counter+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df = pd.concat(df_list, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check number of weeks for which we have features\n",
    "final_df.shape[0]/3103"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imputing NAs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pd.set_option('max_rows', 200)\n",
    "final_df.isna().sum().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('max_rows', 400)\n",
    "final_df.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_to_save = final_df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "na_columns = data_to_save.columns[data_to_save.isnull().any()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# impute missing values by state average\n",
    "for col in data_to_save[na_columns].columns:\n",
    "    data_to_save[col] = data_to_save.groupby('STATEFP')[col].transform(lambda x: x.fillna(x.mean()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_to_save.isna().sum().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "times = ['T', 'T_1', 'T_2', 'T_3', 'T_4']\n",
    "\n",
    "for period in times:\n",
    "    data_to_save['LOG_DELTA_INC_RATE_' + period] = np.log(data_to_save['DELTA_INC_RATE_' + period] + 1)\n",
    "    data_to_save['LOG_MEAN_INC_RATE_' + period] = np.log(data_to_save['MEAN_INC_RATE_' + period] + 1)\n",
    "    data_to_save['LOG_DELTA_SPC_' + period] = np.log(data_to_save['DELTA_SPC_' + period] + 1)\n",
    "    data_to_save['LOG_DELTA_FPC_' + period] = np.log(data_to_save['DELTA_FPC_' + period] + 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_cols = [\n",
    "'GEOID',\n",
    "'NAME',\n",
    "'State_Name',\n",
    "'STATEFP', \n",
    "'COUNTYFP', \n",
    "'date_end_period',\n",
    "'date_start_period',\n",
    "'date_end_lag',\n",
    "'date_start_lag',\n",
    "'LOG_DELTA_INC_RATE_T',\n",
    "'PCT_MALE',\n",
    "'PCT_BLACK',\n",
    "'PCT_HISPAN', \n",
    "'PCT_AMIND',\n",
    "'PCT_RURAL',\n",
    "'PCT_COL_DE' ,\n",
    "'PCT_TRUMP_',\n",
    "'MED_HOS_IN',\n",
    "'POPULATION',\n",
    "'DELTA_CASE_JH_T',\n",
    "'DELTA_CASE_JH_SMOOTH_T'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "additional_cols=[\n",
    "'LOG_DELTA_INC_RATE_T_',\n",
    "'DELTA_REL_MOB_T_',\n",
    "'DELTA_RATIO_MOB_T_',\n",
    "'DELTA_SPC_T_',\n",
    "'DELTA_SPC_LOGGED_T_',\n",
    "'DELTA_FPC_T_',\n",
    "'DELTA_FPC_LOGGED_T_',\n",
    "'LOG_MEAN_INC_RATE_T_',\n",
    "'MEAN_REL_MOB_T_',\n",
    "'MEAN_RATIO_MOB_T_',\n",
    "'MEAN_FPC_T_',\n",
    "'MEAN_SPC_T_',\n",
    "'SLOPE_RATIO_MOB_T_',\n",
    "'SLOPE_REL_MOB_T_',\n",
    "'MAX_TEMP_T_',\n",
    "'MIN_TEMP_T_',\n",
    "'pct_completely_home_device_count_current_T_',\n",
    "'pct_full_time_work_behavior_devices_current_T_',\n",
    "'pct_part_time_work_behavior_devices_current_T_',\n",
    "'pct_delivery_behavior_devices_current_T_',\n",
    "'distance_traveled_from_home_current_T_',\n",
    "'median_home_dwell_time_current_T_',\n",
    "'pct_completely_home_device_count_baselined_T_',\n",
    "'pct_full_time_work_behavior_devices_baselined_T_',\n",
    "'pct_part_time_work_behavior_devices_baselined_T_',\n",
    "'pct_delivery_behavior_devices_baselined_T_',\n",
    "'distance_traveled_from_home_baselined_T_',\n",
    "'median_home_dwell_time_baselined_T_',\n",
    "'pct_completely_home_device_count_slope_T_',\n",
    "'pct_full_time_work_behavior_devices_slope_T_',\n",
    "'pct_part_time_work_behavior_devices_slope_T_',\n",
    "'pct_delivery_behavior_devices_slope_T_',\n",
    "'distance_traveled_from_home_slope_T_',\n",
    "'median_home_dwell_time_slope_T_',\n",
    "'DELTA_CASE_JH_T_',\n",
    "'MEAN_SPC_LOGGED_T_',\n",
    "'MEAN_FPC_LOGGED_T_'\n",
    "]\n",
    "\n",
    "for i in range(1,5):\n",
    "    for col in additional_cols:\n",
    "        final_cols.append(col+str(i))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = data_to_save[final_cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output.to_csv('./output/all_features_updated_incidence.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating dataframes for 2, 3, and 4-week predictions\n",
    "\n",
    "in this dataframe, the target variables is the the number of cumulative cases in 2, 3, and 4 weeks ahead, denoted by `LOG_DELTA_INC_RATE_T_14`, `LOG_DELTA_INC_RATE_T_21`, and `LOG_DELTA_INC_RATE_T_28` respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main function\n",
    "# the input to the funtion is the end date for which we want to add data\n",
    "\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "\n",
    "def add_lagged_y(date):\n",
    "    global output, jh_covid_df\n",
    "    \n",
    "    T_end = date\n",
    "    T_start = T_end - timedelta(days=7)\n",
    "    \n",
    "    T_start_period = (T_end - timedelta(days=6)).strftime('%Y-%m-%d')\n",
    "    \n",
    "    T_14 =  T_end + timedelta(days=7)\n",
    "    T_21 =  T_end + timedelta(days=14)\n",
    "    T_28 =  T_end + timedelta(days=21)\n",
    "    \n",
    "\n",
    "    dates_non_str = [T_end, T_start, T_14, T_21, T_28]\n",
    "    \n",
    "    dates = [item.strftime('%Y-%m-%d') for item in dates_non_str]\n",
    "    \n",
    "    dates_jh = [item.strftime('%#m/%#d/%y') for item in dates_non_str]\n",
    "    \n",
    "    \n",
    "    periods = ['T_end', 'T_start', 'T_14', 'T_21', 'T_28']\n",
    "    \n",
    "    temp = output.loc[(output.date_end_period==dates[0]) & (output.date_start_period==T_start_period)].copy()\n",
    "    \n",
    "    temp['FIPS'] = temp['GEOID'].astype(int)\n",
    "    \n",
    "    #print('check 1 {}'.format(temp.shape))\n",
    "    temp['target_date_2wk'] = T_14.strftime('%Y-%m-%d')\n",
    "    temp['target_date_3wk'] = T_21.strftime('%Y-%m-%d')\n",
    "    temp['target_date_4wk'] = T_28.strftime('%Y-%m-%d')\n",
    "        \n",
    "    temp = temp.merge(covid_df_contiguous[['GEOID',*dates]], on='GEOID', how='left')\n",
    "    \n",
    "    temp = temp.merge(jh_covid_df[['FIPS',*dates_jh]], on='FIPS', how='left')\n",
    "    #print('check 2 {}'.format(temp.shape))\n",
    "\n",
    "    for period, date in zip(periods, dates):\n",
    "        temp['inc_rate_' + period] = temp[date] / temp['POPULATION'] * 10000\n",
    "\n",
    "\n",
    "    for period, date in zip(periods[-3:], dates[-3:]):\n",
    "        temp['DELTA_CASE_SMOOTHED_' + period] = temp[date] - temp[dates[1]]\n",
    "        temp['DELTA_INC_RATE_' + period] = temp['inc_rate_' + period] - temp['inc_rate_T_start']\n",
    "        temp['LOG_DELTA_INC_RATE_' + period] = np.log(temp['DELTA_INC_RATE_' + period] + 1)\n",
    "    \n",
    "    for period, date in zip(periods[-3:], dates_jh[-3:]):\n",
    "        temp['DELTA_CASE_JH_' + period] = temp[date] - temp[dates_jh[1]]\n",
    "    \n",
    "    temp['DELTA_CASE_JH_T'] = temp[dates_jh[0]] - temp[dates_jh[1]]\n",
    "        \n",
    "    \n",
    "    \n",
    "    cols = ['target_date_2wk','LOG_DELTA_INC_RATE_T_14', \n",
    "            'target_date_3wk','LOG_DELTA_INC_RATE_T_21',\n",
    "            'target_date_4wk','LOG_DELTA_INC_RATE_T_28' ]\n",
    "    #print('check 3 {}'.format(temp.shape))\n",
    "    \n",
    "    return temp[[*output.columns,'DELTA_CASE_JH_T',\n",
    "            'target_date_2wk','LOG_DELTA_INC_RATE_T_14', 'DELTA_CASE_SMOOTHED_T_14', 'DELTA_CASE_JH_T_14',\n",
    "            'target_date_3wk','LOG_DELTA_INC_RATE_T_21', 'DELTA_CASE_SMOOTHED_T_21', 'DELTA_CASE_JH_T_21',\n",
    "            'target_date_4wk','LOG_DELTA_INC_RATE_T_28', 'DELTA_CASE_SMOOTHED_T_28', 'DELTA_CASE_JH_T_28']]\n",
    "    \n",
    "    \n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "end_date = datetime(2021, 2, 27)\n",
    "df_lagged_list = []\n",
    "\n",
    "while end_date > datetime(2020, 3, 29):\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    df_lagged_list.append(add_lagged_y(end_date))\n",
    "    \n",
    "    end_time = time.time()\n",
    "    print('Feature generation for the week ending in {} finished in {} seconds'.format(end_date.strftime('%Y-%m-%d'),\n",
    "                                                                                       round(end_time-start_time,1)))\n",
    "    end_date -= timedelta(weeks=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_lagged = pd.concat(df_lagged_list, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_lagged.isna().sum().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_lagged.shape, df_lagged.shape[0]/3103"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_lagged.to_csv('./output/all_features.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sp",
   "language": "python",
   "name": "sp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
