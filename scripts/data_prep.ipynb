{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preparation\n",
    "\n",
    "This notebook is used to clean up, preprocess, and aggregate data necessary for STXGB models.\n",
    "\n",
    "Please refer to the `Method` section of the article and Supplementary Information document for more information.\n",
    "\n",
    "\n",
    "In order to run this notebook, you have to download the required datasets from this [address](https://drive.google.com/drive/u/1/folders/1laAZFCvsPLLaKDvg0isTMMr20kMe0x_r). Once downloaded, set the directory in which you have save the files as `input_directory` in the cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_directory = '/home/dante/SpatialData/spatial_project/data/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid character in identifier (2783386656.py, line 22)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Input \u001b[0;32mIn [37]\u001b[0;36m\u001b[0m\n\u001b[0;31m    pd.set_option(‘mode.chained_assignment’, None) # need to be careful with this one!\u001b[0m\n\u001b[0m                      ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid character in identifier\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import geopandas as gpd\n",
    "import platform\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "import requests\n",
    "from urllib.request import urlopen\n",
    "import json\n",
    "import os\n",
    "from os import listdir\n",
    "\n",
    "#viz\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import matplotlib.ticker as ticker\n",
    "\n",
    "\n",
    "\n",
    "pd.set_option('display.max_rows', 5)\n",
    "pd.set_option(‘mode.chained_assignment’, None) # need to be careful with this one!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def datetimer(cellstring):\n",
    "    if cellstring == \"Total\":\n",
    "        return cellstring\n",
    "    strlist = cellstring.split(\" \")\n",
    "    return f\"{strlist[1]}-{strlist[3]}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datetimer(\"Year 2020 Week 01\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Set date string formatting based on operating system\n",
    "\n",
    "if platform.system() == 'Windows':\n",
    "    conversion_format = '%#m/%#d/%y'\n",
    "else:\n",
    "    conversion_format = '%-m/%-d/%y'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing hospital data by province"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Taking the data straight from a csv from their pivot webpage. Address found in the good_resources.txt file.\n",
    "df_cov = pd.read_csv('/home/dante/SpatialData/spatial_project/data/covid/fact_epirapo_covid19case.csv',sep=\";\")\n",
    "mun_prov = pd.read_csv('/home/dante/SpatialData/spatial_project/scripts/municipality_province_map.csv',encoding = \"ISO-8859-1\",sep=\";\",header=1)\n",
    "prov_lang_map = pd.read_csv('/home/dante/SpatialData/spatial_project/scripts/mob_map_nuts3.txt').iloc[:,1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Making actual mappings (municipalities)\n",
    "mun_prov_map = dict(zip(mun_prov.iloc[:,1],mun_prov.iloc[:,3]))\n",
    "muns = list(mun_prov.iloc[:,1])\n",
    "\n",
    "# Making actual mappings (provinces)\n",
    "prov_lang_map = prov_lang_map[prov_lang_map.iloc[:,3] != 'Åland']\n",
    "prov_fin_eng = dict(zip(prov_lang_map.iloc[:,1],prov_lang_map.iloc[:,3]))\n",
    "provs = list(set(mun_prov.iloc[:,3]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'DataFrame' object has no attribute 'Alue'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Input \u001b[0;32mIn [39]\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Dropping everything but the municipalities (contains data about hospital district as well)\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m df_cov \u001b[38;5;241m=\u001b[39m df_cov[\u001b[43mdf_cov\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mAlue\u001b[49m\u001b[38;5;241m.\u001b[39misin(muns)]\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# Dropping \"kaikki ajat\" aka \"all times\":\u001b[39;00m\n\u001b[1;32m      5\u001b[0m df_cov \u001b[38;5;241m=\u001b[39m df_cov[df_cov[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mAika\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mKaikki ajat\u001b[39m\u001b[38;5;124m'\u001b[39m]\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/pandas/core/generic.py:5487\u001b[0m, in \u001b[0;36mNDFrame.__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   5480\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m   5481\u001b[0m     name \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_internal_names_set\n\u001b[1;32m   5482\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m name \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_metadata\n\u001b[1;32m   5483\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m name \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_accessors\n\u001b[1;32m   5484\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_info_axis\u001b[38;5;241m.\u001b[39m_can_hold_identifiers_and_holds_name(name)\n\u001b[1;32m   5485\u001b[0m ):\n\u001b[1;32m   5486\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m[name]\n\u001b[0;32m-> 5487\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mobject\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__getattribute__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'DataFrame' object has no attribute 'Alue'"
     ]
    }
   ],
   "source": [
    "# Dropping everything but the municipalities (contains data about hospital district as well)\n",
    "df_cov = df_cov[df_cov.Alue.isin(muns)]\n",
    "\n",
    "# Dropping \"kaikki ajat\" aka \"all times\":\n",
    "df_cov = df_cov[df_cov['Aika'] != 'Kaikki ajat']\n",
    "\n",
    "# Mapping municipalities to the correct province\n",
    "df_cov['province'] = df_cov.iloc[:,1].map(mun_prov_map)\n",
    "\n",
    "# Dropping Ahvenanmaa (Åland) - the autonomous island we don't \n",
    "# analyze in this thing\n",
    "df_cov = df_cov[df_cov.province != 'Ahvenanmaa']\n",
    "\n",
    "# Resetting index after all this\n",
    "df_cov.reset_index(drop=True,inplace=True)\n",
    "\n",
    "# Setting val as int\n",
    "df_cov['val'] = np.where(df_cov['val'] == '..',0,df_cov['val'])\n",
    "df_cov['val'] = df_cov.val.astype(int)\n",
    "\n",
    "# Grouping by both province and week\n",
    "df_cov = df_cov.groupby(['province','Aika']).agg({'val' : 'sum'}).reset_index(drop=False)\n",
    "\n",
    "# Pivoting, so that we get cases along dimensions (time x province)\n",
    "df_cov_pivot = df_cov.pivot('Aika','province','val').reset_index(drop=False).rename_axis(None, axis=1)\n",
    "\n",
    "# Putting it in the year-week format according to the rest of the data\n",
    "df_cov_pivot['Y-W'] = df_cov_pivot.Aika.apply(datetimer)\n",
    "df_cov_pivot.drop(columns=['Aika'],inplace=True)\n",
    "\n",
    "# Data seems uniform - just how we like it!\n",
    "print(df_cov.province.value_counts())\n",
    "\n",
    "# Changing province names into English:\n",
    "df_cov_pivot.rename(columns=prov_fin_eng,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_cov_pivot.head(10)\n",
    "# df_cov_pivot.to_csv('/home/dante/SpatialData/spatial_project/data/processed/newcases_by_week_province.csv',index=False,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cov_pivot.iloc[:,0:-1] = df_cov_pivot.iloc[:,0:-1].apply(lambda x: np.log(x+1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cov_pivot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Visualizing the measure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "f = plt.figure()\n",
    "f.set_figheight(9)\n",
    "f.set_figwidth(15)\n",
    "\n",
    "ax1 = f.add_subplot(111)\n",
    "\n",
    "\n",
    "\n",
    "sns.lineplot(x='Y-W',\n",
    "             y='value',\n",
    "             hue = 'variable',\n",
    "             data=pd.melt(df_cov_pivot, ['Y-W']),\n",
    "             ax=ax1)\n",
    "\n",
    "fontdict = {'fontsize': 20,\n",
    "#  'fontweight' : rcParams['axes.titleweight'],\n",
    "#  'verticalalignment': 'baseline',\n",
    "#  'horizontalalignment': loc}\n",
    "           }\n",
    "plt.xlim(0,115)\n",
    "ax1.xaxis.set_major_locator(ticker.MultipleLocator(5))\n",
    "ax1.xaxis.set_major_formatter(ticker.ScalarFormatter())\n",
    "ax1.set_title(r\"Ln new cases (by Week x Province)\",fontdict = fontdict)\n",
    "ax1.set_xlabel(r\"Week number (starting from 2020-01)\",fontsize=14)\n",
    "ax1.set_ylabel(r\"Ln new cases\",rotation=0,labelpad=30,fontsize=14)\n",
    "\n",
    "\n",
    "\n",
    "leg = plt.legend(title='Province', loc='upper left', labels=list(df_cov_pivot.columns[0:-1]))\n",
    "# leg._legend_box.align = \"top\"\n",
    "plt.setp(leg.get_title(),fontsize='large')\n",
    "plt.savefig('/home/dante/SpatialData/spatial_project/data/processed/new_cases_timeseries.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Google intra-region mobility data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Script that combines data from all years"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specifying the path to the folder with google raw data\n",
    "path = '/home/dante/SpatialData/spatial_project/data/google/'\n",
    "\n",
    "# Getting the name of all files (2020-2022)\n",
    "filelist = [file for file in os.listdir(path) if '.csv' in file]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specifying dtypes for faster processing of csv...\n",
    "dtypes = {\n",
    "    'country_region_code' : 'object',\n",
    "    'country_region' : 'object',\n",
    "    'sub_region_1' : 'object',\n",
    "    'sub_region_2' : 'object',\n",
    "    'metro_area' : 'object',\n",
    "    'iso_3166_2_code' : 'object',\n",
    "    'census_fips_code' : 'object',\n",
    "    'place_id' : 'object',\n",
    "    'date' : 'object',\n",
    "    'retail_and_recreation_percent_change_from_baseline' : 'float64',\n",
    "    'grocery_and_pharmacy_percent_change_from_baseline' : 'float64',\n",
    "    'parks_percent_change_from_baseline' : 'float64',\n",
    "    'transit_stations_percent_change_from_baseline' : 'float64',\n",
    "    'workplaces_percent_change_from_baseline' : 'float64',\n",
    "    'residential_percent_change_from_baseline' : 'float64',\n",
    "}\n",
    "\n",
    "# Assigning the correct column names\n",
    "headers = list(dtypes.keys())\n",
    "\n",
    "# Telling Pandas what column is a datecolumn (so that it can be parsed)\n",
    "parse_dates = ['date']\n",
    "\n",
    "# Creating a map between the ISO-codes of the different provinces and their ENG/FI/SWE \n",
    "# names, for legibility later on.\n",
    "df_codemap = pd.read_csv('/home/dante/SpatialData/spatial_project/scripts/mob_map_nuts3.txt',sep=',')\n",
    "iso_name_map = dict(zip(df_codemap.iloc[:,0],df_codemap.iloc[:,3]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_codemap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specifying empty dataframe for filling\n",
    "df_list = []\n",
    "\n",
    "for filename in filelist:\n",
    "    # Specifying file location\n",
    "    floc = path + filename\n",
    "    \n",
    "    # Reading in file\n",
    "    df_mr = pd.read_csv(floc,header=0,dtype=dtypes,names=headers,parse_dates=parse_dates)\n",
    "    \n",
    "    # Sorting values by date and sub_region_2,sub_region_1 as we want to be able to\n",
    "    # look at the data in an intuitive way.\n",
    "    df_mr.sort_values(by=['date','sub_region_2','sub_region_1'],inplace=True)\n",
    "    \n",
    "    df_mr['province'] = df_mr.iso_3166_2_code.map(iso_name_map)\n",
    "    \n",
    "    # Dropping unneccesary columns\n",
    "    df_mr.drop(columns=['country_region_code','country_region','census_fips_code','iso_3166_2_code','place_id','metro_area','sub_region_2','sub_region_1'],inplace=True)\n",
    "\n",
    "    # Specifying down our data to the province level\n",
    "    df_mr.dropna(subset=['province'],inplace=True)\n",
    "    df_mr.reset_index(drop=True,inplace=True)\n",
    "\n",
    "    # Casting date in correct format (apparently Pandas doesn't actually read the dtypes correctly)\n",
    "    df_mr['date'] = pd.to_datetime(df_mr.date)\n",
    "\n",
    "    # Adding day of year for gap checking if needed\n",
    "    df_mr['dayofyear'] = df_mr['date'].dt.dayofyear\n",
    "\n",
    "    # Defining the movement data columns of interest\n",
    "    # and filling in NaNs with 0 (i.e no change/information).\n",
    "    # I do not know how sustainable this assumption is...\n",
    "    mvcols = [col for col in df_mr.columns if 'baseline' in col]\n",
    "    values = {col : 0 for col in mvcols}\n",
    "    df_mr.fillna(value=values, inplace=True)\n",
    "    \n",
    "    \n",
    "    for prov in iso_name_map.values():\n",
    "    \n",
    "        # Specify the correct province \n",
    "        df = df_mr[df_mr.province == prov]\n",
    "\n",
    "        # The below code bins the data according to week commencing MONDAY\n",
    "        # This code looks complicated, but achieves something quite understanable.\n",
    "        df['date'] = pd.to_datetime(df['date']) - pd.to_timedelta(7, unit='d')\n",
    "        df = df.groupby([pd.Grouper(key='date', freq='W-MON')])[mvcols].mean().reset_index().sort_values('date')\n",
    "\n",
    "        # Creating the column with which we can then merge to the cases data..\n",
    "        df['Y-W'] = df.date.dt.year.astype(str)+'-'+df.date.dt.isocalendar().week.astype(str)\n",
    "\n",
    "        # Adding name of province\n",
    "        df['province'] = prov\n",
    "        df.reset_index(drop=True) \n",
    "\n",
    "        # Finally, appending to the list of dfs...\n",
    "        df_list.append(df)\n",
    "    \n",
    "# Finally, concatenating the whole list\n",
    "combined_mv_df = pd.concat(df_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx,prov in enumerate(combined_mv_df.province.value_counts().index):\n",
    "    datas = combined_mv_df[combined_mv_df.province == prov].shape\n",
    "    print(f\"Province {idx}, shape : {datas}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving the file as a CSV.\n",
    "# combined_mv_df.to_csv('/home/dante/SpatialData/spatial_project/data/processed/google_mobility_by_week_province.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_mv_df.sort_values(by=['province','date'],inplace=True)\n",
    "# combined_mv_df[combined_mv_df.province == 'Uusimaa'].head(120)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FB inter-region connectivity data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating the Social Proximity to Cases Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sci = pd.read_csv('/home/dante/SpatialData/spatial_project/data/processed/fb_sci_by_province.csv')\n",
    "df_cov = pd.read_csv('/home/dante/SpatialData/spatial_project/data/processed/newcases_by_week_province.csv')\n",
    "prov_nam_conv = pd.read_csv('/home/dante/SpatialData/spatial_project/scripts/mob_map_nuts3.txt')\n",
    "# mun_prov = pd.read_csv('/home/dante/SpatialData/spatial_project/scripts/municipality_province_map.csv',encoding = \"ISO-8859-1\",sep=\";\",header=1)\n",
    "mun_pop = pd.read_csv('/home/dante/SpatialData/spatial_project/data/municipal/municipal_population.txt',header=0,encoding = \"UTF-8\",sep=\"\\t\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Collecting provincial information\n",
    "prov_pop = mun_pop.iloc[310:328,:]\n",
    "\n",
    "# Casting the population figures as INT\n",
    "prov_pop.iloc[:,1] = prov_pop.iloc[:,1].str.replace(\",\",\"\").astype(int)\n",
    "prov_pop.reset_index(drop=True)\n",
    "\n",
    "# Sorting values for easier legibility\n",
    "prov_pop.sort_values(by=prov_pop.columns[0],inplace=True)\n",
    "\n",
    "prov_pop['province'] = prov_pop['Population'].map(dict(zip(prov_nam_conv.iloc[:,-2],prov_nam_conv.iloc[:,3])))\n",
    "\n",
    "# Making province : population dict for division later...\n",
    "prov_pop_dict = dict(zip(prov_pop.iloc[:,2],prov_pop.iloc[:,1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Below, you'll see the dict with province x population\n",
    "prov_pop_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Changing names from THL convention to the convention we have been using.\n",
    "df_cov.rename(columns=dict(zip(prov_nam_conv.iloc[:,-1],prov_nam_conv.iloc[:,-4])),inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Having a peek at our data...\n",
    "df_cov.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Creating a copy of the COVID df so that we can always restart\n",
    "# the modifications from here.\n",
    "df_cov_c = df_cov.copy()\n",
    "\n",
    "# The below for-loop scales the covid cases for each day x province \n",
    "# to the level of new cases per 10k people. In this way, we are stand-\n",
    "# ardizing the measurement across provinces.\n",
    "for idx,prov in enumerate(df_cov.columns[0:-1]):\n",
    "    #FOR EACH PROVINCE\n",
    "\n",
    "    # For each province, fetch the population of the province,\n",
    "    # and then divide the whole time series for that province\n",
    "    # by the population.\n",
    "    case_by_pop = df_cov_c[prov] / prov_pop_dict[prov]\n",
    "    \n",
    "    # Multiply the per capita number by 10000, to get cases\n",
    "    # per 10k people.\n",
    "    case_by_10k = case_by_pop * 10000\n",
    "    \n",
    "    # Assign back to the dataframe.\n",
    "    df_cov_c[prov] = case_by_10k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cov_c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sci.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mapping GADM NUTS3 code to our used standard for the province name\n",
    "mob_map = dict(zip(prov_nam_conv.iloc[:,4],prov_nam_conv.iloc[:,3]))\n",
    "\n",
    "# Creating a map between SCI weights of a given province and all OTHERS.\n",
    "sci_dict = {}\n",
    "for prov in df_sci.user_loc_province.value_counts().index:\n",
    "    # FOR EACH PROVINCE\n",
    "    \n",
    "    # Get list of all other provinces, i.e. not the one that is being\n",
    "    # considered in the loop (because this was already deleted earlier\n",
    "    # when I made the fb_sci_by_province.csv file).\n",
    "    fr_list = list(df_sci[(df_sci.user_loc_province == prov)].fr_loc_province)\n",
    "    \n",
    "    # Get the list of all other province weights.\n",
    "    sci_weights_list = list(df_sci[(df_sci.user_loc_province == prov)].sci_weights)\n",
    "    \n",
    "    # Assign the list of all other province weights to a dictionary, with the name\n",
    "    # of the 'parent' province.\n",
    "    sci_dict[prov] = {fr_loc:sci_weight for fr_loc,sci_weight in zip(fr_list,sci_weights_list)}\n",
    "\n",
    "\n",
    "# The SCI-dict looks as follows:\n",
    "# sci_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FINALLY, we can create the SCI-weighted data!\n",
    "\n",
    "# Creating a final dataframe to write to.\n",
    "df_spc = df_cov.copy()\n",
    "\n",
    "for prov in sci_dict.keys():\n",
    "    #FOR PROVINCE IN THE SCI-DICT\n",
    "    \n",
    "    # Creating a local copy for computation\n",
    "    df = df_cov_c.copy()\n",
    "    \n",
    "    # Get the weights for the province\n",
    "    weights = sci_dict[prov]\n",
    "    \n",
    "    # Then, we weigh the local copy of the dataframe\n",
    "    # by the weights in the sci_dict\n",
    "    for neigh_prov in weights.keys():  \n",
    "        #FOR EACH PROVINCE APART FROM prov\n",
    "        \n",
    "        # Weighting the observations by SCI weights\n",
    "        df[neigh_prov] = df[neigh_prov]*weights[neigh_prov]\n",
    "    \n",
    "    # Collecting the names of the neighbors\n",
    "    neighs = list(weights.keys())\n",
    "    \n",
    "    # Summing the weighted observations across all provinces \n",
    "    # not equal to the province we are processing the data for.\n",
    "    df_spc[prov] = df[neighs].sum(axis=1)\n",
    "    \n",
    "    # Setting the data to the correct format (i.e ln(x+1)) so\n",
    "    # that we won't have minus-valued predicted cases.\n",
    "    df_spc[prov] = np.log(df_spc[prov]+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = plt.figure()\n",
    "f.set_figheight(9)\n",
    "f.set_figwidth(15)\n",
    "\n",
    "ax1 = f.add_subplot(111)\n",
    "\n",
    "\n",
    "\n",
    "sns.lineplot(x='Y-W',\n",
    "             y='value',\n",
    "             hue = 'variable',\n",
    "             data=pd.melt(df_spc, ['Y-W']),\n",
    "             ax=ax1)\n",
    "\n",
    "fontdict = {'fontsize': 20,\n",
    "#  'fontweight' : rcParams['axes.titleweight'],\n",
    "#  'verticalalignment': 'baseline',\n",
    "#  'horizontalalignment': loc}\n",
    "           }\n",
    "plt.xlim(0,115)\n",
    "ax1.xaxis.set_major_locator(ticker.MultipleLocator(5))\n",
    "ax1.xaxis.set_major_formatter(ticker.ScalarFormatter())\n",
    "ax1.set_title(r\"Social Proximity to Cases (SPC))\",fontdict = fontdict)\n",
    "ax1.set_xlabel(r\"Week number (starting from 2020-01)\",fontsize=14)\n",
    "ax1.set_ylabel(r\"$SPC_{i,t}$\",rotation=0,labelpad=30,fontsize=14)\n",
    "\n",
    "\n",
    "\n",
    "leg = plt.legend(title='Province', loc='upper left', labels=list(df_spc.columns[0:-1]))\n",
    "# leg._legend_box.align = \"top\"\n",
    "plt.setp(leg.get_title(),fontsize='large')\n",
    "plt.savefig('/home/dante/SpatialData/spatial_project/data/processed/SPC_timeseries.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- I have a feeling that these are always driven by Uusimaa.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This now holds the SCI weighted cases!    \n",
    "df_spc.head(15)\n",
    "\n",
    "# df_spc.to_csv('/home/dante/SpatialData/spatial_project/data/processed/spc_by_week_province.csv',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Weather data\n",
    "- The weather data was gathered from the Finnish Meteorological Institute (FMI)\n",
    "https://en.ilmatieteenlaitos.fi/download-observations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filepath = '/home/dante/SpatialData/spatial_project/data/weather/'\n",
    "filelist = [file for file in os.listdir(filepath) if '.csv' in file]\n",
    "provinces = pd.read_csv('/home/dante/SpatialData/spatial_project/scripts/mob_map_nuts3.txt',sep=',').iloc[:,3].values.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initializing an empty list to fill with weather data\n",
    "df_list = []\n",
    "\n",
    "for idx,prov in enumerate(provinces):\n",
    "    # FOR EACH PROVINCE NAME\n",
    "    \n",
    "    # Create filename\n",
    "    path2data = filepath + str(prov) + '.csv'\n",
    "    \n",
    "    # Fetch data, parse it\n",
    "    df = pd.read_csv(path2data,\n",
    "                     parse_dates = {\"date\" : [\"Year\",\"m\",\"d\"]},\n",
    "                     dtype = {'Time zone' : 'object',\n",
    "                              'Precipitation amount (mm)' : 'float64',\n",
    "                              'Maximum temperature (degC)' : 'float64',\n",
    "                              'Minimum temperature (degC)' : 'float64'},\n",
    "#                      nrows=1,\n",
    "                     header=0)\n",
    "    \n",
    "    print(df.shape)\n",
    "    # Assigning columns of interest\n",
    "    cols = list(df.columns)[2:]\n",
    "        \n",
    "    # The below code bins the data according to week commencing MONDAY\n",
    "    # This code looks complicated, but achieves something quite understanable.\n",
    "    df['date'] = pd.to_datetime(df['date']) - pd.to_timedelta(7, unit='d')\n",
    "    df = df.groupby([pd.Grouper(key='date', freq='W-MON')])[cols].mean().reset_index().sort_values('date')\n",
    "\n",
    "    # Creating the column with which we can then merge to the cases data..\n",
    "    df['Y-W'] = df.date.dt.year.astype(str)+'-'+df.date.dt.isocalendar().week.astype(str)\n",
    "    \n",
    "    if idx == 0:\n",
    "        y_w = df['Y-W']\n",
    "    \n",
    "    # Adding name of province\n",
    "    df['province'] = prov\n",
    "    df.reset_index(drop=True) \n",
    "    \n",
    "    # Setting first week of January to start on 2020-01-01\n",
    "    # as per data from THL (covid source)\n",
    "    df.iloc[0,-2] = '2020-1'\n",
    "\n",
    "    # Finally, appending to the list of dfs...\n",
    "    df_list.append(df)\n",
    "    \n",
    "\n",
    "\n",
    "# concatenating it all into one big df...\n",
    "weather_data = pd.concat(df_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# weather_data.pivot('date','province','Maximum temperature (degC)').head(150).reset_index().isna().sum()\n",
    "# weather_data.pivot('date','province','Minimum temperature (degC)').head(150).reset_index().isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting the max temperature reading per weep for all provinces\n",
    "max_temp_data = weather_data.pivot('date','province','Maximum temperature (degC)')\n",
    "max_temp_data['Y-W'] = y_w.values\n",
    "max_temp_data.reset_index(drop=False)\n",
    "\n",
    "# Getting the min temperature reading per weep for all provinces\n",
    "min_temp_data = weather_data.pivot('date','province','Minimum temperature (degC)')\n",
    "min_temp_data['Y-W'] = y_w.values\n",
    "min_temp_data.reset_index(drop=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# min_temp_data.to_csv('/home/dante/SpatialData/spatial_project/data/processed/min_temp_by_week_province.csv',index=False)\n",
    "# max_temp_data.to_csv('/home/dante/SpatialData/spatial_project/data/processed/max_temp_by_week_province.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = gpd.read_file( input_directory + 'Contiguous_US.geojson')\n",
    "\n",
    "# Or alternatively:\n",
    "# url='https://drive.google.com/file/d/1MVyLzzHl3hzno4o1rLZtI0peqIi23zsr/view?usp=sharing'\n",
    "# url_counties='https://drive.google.com/uc?id=' + url.split('/')[-2]\n",
    "# data = gpd.read_file(url_counties)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['STATEFP'] = data.apply(lambda L: L.GEOID[:2], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "global number_counties \n",
    "number_counties = data.shape[0] #3103"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data.sort_values(by='GEOID', inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load COVID data and apply smoothing \n",
    "\n",
    "To alleviate inconsistencies in reporting COVID-19 cases, we apply a 7-day moving average to the case data published by JHU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_JH_covid_data(target, smooth):\n",
    "    \n",
    "    '''\n",
    "    Parameters:\n",
    "    --------------\n",
    "        target: str\n",
    "            the target variable: either 'case' or 'death'\n",
    "            \n",
    "        smooth: bool\n",
    "            wether to smooth the data frame or not.\n",
    "            The smoothing is done by using a 7-day rolling average   \n",
    "    '''\n",
    "    \n",
    "    assert isinstance(smooth, bool), \"Smooth must be a boolean variable!\"\n",
    "    \n",
    "    base_url = 'https://raw.githubusercontent.com/CSSEGISandData/COVID-19/master/csse_covid_19_data/'\n",
    "    \n",
    "    \n",
    "    if target == 'case':\n",
    "        jh_data_url = base_url + 'csse_covid_19_time_series/time_series_covid19_confirmed_US.csv'\n",
    "        cols_to_drp = ['UID', 'iso2', 'iso3', 'code3','Country_Region', 'Lat', 'Long_']\n",
    "\n",
    "    elif target=='death':\n",
    "        jh_data_url = base_url + 'csse_covid_19_time_series/time_series_covid19_deaths_US.csv'\n",
    "        cols_to_drp = ['UID', 'iso2', 'iso3', 'code3','Country_Region', 'Lat', 'Long_','Population']\n",
    "    else:\n",
    "        print(\"invalid argument for target. Acceptable values are: 'case' or 'death'\")\n",
    "        return None\n",
    "\n",
    "    jh_covid_df = pd.read_csv(jh_data_url)\n",
    "\n",
    "    # preprocessing JH COVID data\n",
    "    jh_covid_df.dropna(axis=0, how='any', inplace=True)\n",
    "\n",
    "    jh_covid_df['FIPS'] = jh_covid_df['FIPS'].astype('int64')\n",
    "\n",
    "    jh_covid_df.drop(columns=cols_to_drp, inplace=True)\n",
    "\n",
    "    #Important: check to see the column index is adherent to the imported df\n",
    "\n",
    "    first_date = datetime.strptime(jh_covid_df.columns[4], '%m/%d/%y').date()\n",
    "\n",
    "    last_date = datetime.strptime(jh_covid_df.columns[-1], '%m/%d/%y').date()\n",
    "\n",
    "\n",
    "    current_date = last_date\n",
    "\n",
    "    previous_date = last_date - timedelta (days=1)\n",
    "\n",
    "\n",
    "    while current_date > first_date:\n",
    "\n",
    "        #For unix, replace # with - in the time format\n",
    "\n",
    "        current_col = current_date.strftime(conversion_format) #replace # with - in Mac or Linux\n",
    "\n",
    "        previous_col = previous_date.strftime(conversion_format)\n",
    "\n",
    "        jh_covid_df[previous_col] = np.where(jh_covid_df[previous_col] > jh_covid_df[current_col], \n",
    "                                             jh_covid_df[current_col], jh_covid_df[previous_col])\n",
    "\n",
    "        current_date = current_date - timedelta(days=1)\n",
    "\n",
    "        previous_date = previous_date - timedelta(days=1)\n",
    "        \n",
    "    \n",
    "    if smooth:\n",
    "        jh_covid_df.iloc[:,4:] = jh_covid_df.iloc[:,4:].rolling(7,min_periods=1,axis=1).mean()\n",
    "\n",
    "\n",
    "    return jh_covid_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "covid_df = get_JH_covid_data('case', True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Facebook Movement Range Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fb_mobility = pd.read_csv(input_directory + 'movement-range-2021-03-02.txt', sep=\"\\t\", dtype={'polygon_id':str})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fb_mobility_us = fb_mobility[fb_mobility['country']=='USA']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of unique counties for which we have at least one day of data\n",
    "len(fb_mobility_us['polygon_id'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# counting Counties in the contiguous US for which there is no data in FB mobility\n",
    "contiguous_fips = set(data['GEOID']) # number of unique fips: 3103\n",
    "mobility_fips = set(fb_mobility_us['polygon_id']) # number of unique fips: 2694\n",
    "i = 0\n",
    "missing_fips = []\n",
    "for fips in contiguous_fips:\n",
    "    if (fips in mobility_fips):\n",
    "        i+=1\n",
    "    else:\n",
    "        missing_fips.append(fips)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# number of Counties in the contiguous US for which there is no data in FB mobility\n",
    "len(missing_fips)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new dataframe as transpose of the above, with days as columns and counties as rows\n",
    "\n",
    "relative_df = pd.DataFrame(columns=fb_mobility_us['ds'].unique(), index=data['GEOID'].unique(), dtype=float)\n",
    "ratio_df = pd.DataFrame(columns=fb_mobility_us['ds'].unique(), index=data['GEOID'].unique(), dtype=float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fb_mobility_us"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx_contiguous = fb_mobility_us.index[fb_mobility_us['polygon_id'].isin(contiguous_fips)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fb_mobility_contiguous = fb_mobility_us.loc[idx_contiguous]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "start = time.time()\n",
    "\n",
    "for index, row in fb_mobility_contiguous.iterrows():\n",
    "    relative_df.loc[row['polygon_id']][row['ds']] = row['all_day_bing_tiles_visited_relative_change']\n",
    "    ratio_df.loc[row['polygon_id']][row['ds']] = row['all_day_ratio_single_tile_users']\n",
    "    \n",
    "\n",
    "end = time.time()\n",
    "print(end - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "relative_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(relative_df.shape , ratio_df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Impute FB mobility dataframes\n",
    "The two dataframes above have a lot of Nan values which should be imputed by state average"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_ratio_df = data[['GEOID', 'STATEFP']].merge(ratio_df, left_on='GEOID', right_index=True, how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in temp_ratio_df.iloc[:,2:].columns:\n",
    "    temp_ratio_df[col] = temp_ratio_df.groupby('STATEFP')[col].transform(lambda x: x.fillna(x.mean()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_ratio_df.isna().sum().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_relative_df = data[['GEOID', 'STATEFP']].merge(relative_df, left_on='GEOID', right_index=True, how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in temp_relative_df.iloc[:,2:].columns:\n",
    "    temp_relative_df[col] = temp_relative_df.groupby('STATEFP')[col].transform(lambda x: x.fillna(x.mean()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_relative_df.isna().sum().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "relative_df_smooth = temp_relative_df.copy()\n",
    "ratio_df_smooth = temp_ratio_df.copy()\n",
    "\n",
    "relative_df_smooth.iloc[:,2:] = relative_df_smooth.iloc[:,2:].rolling(7,min_periods=1, axis=1).mean()\n",
    "ratio_df_smooth.iloc[:,2:] = ratio_df_smooth.iloc[:,2:].rolling(7,min_periods=1, axis=1).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "relative_df_smooth.iloc[:,2:].columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "relative_df_smooth.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add Social Proximity to Cases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SCI_df = pd.read_csv(input_directory + 'SCI_matrix.csv', dtype={'Unnamed: 0':str})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SCI_df.set_index('Unnamed: 0', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create normalized SCI. It is calculated by dividing all the columns of the sci_matrix by the sum of the rpw\n",
    "# This would give us the second term in social proximity formula above\n",
    "\n",
    "sci_matrix_normal = SCI_df.div(SCI_df.sum(axis=1), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set diagonal to zero\n",
    "sci_matrix_normal.values[[np.arange(sci_matrix_normal.shape[0])]*2] = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The matrix above is created for the entire US, but we are using contiguous US data here, therefore some rows and columns should be removed**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_drop=[]\n",
    "\n",
    "for index in sci_matrix_normal.index:\n",
    "    if not index in contiguous_fips:\n",
    "        to_drop.append(index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sci_matrix_normal.drop(to_drop, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sci_matrix_normal.drop(to_drop, axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sci_matrix_normal.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add SafeGraph mobility features\n",
    "\n",
    "Updated based on forecast hub dates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "safegraph_mobility = pd.read_csv(input_directory + 'safegraph_mobility.csv', dtype={'county_fips':str})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "safegraph_contiguous = safegraph_mobility[safegraph_mobility['county_fips'].isin(contiguous_fips)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(safegraph_contiguous['county_fips'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_cols = safegraph_contiguous.drop(['start_date', 'end_date', 'base_start', 'base_end'], axis=1)\n",
    "safegraph_metrics = temp_cols.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(safegraph_metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Temperature Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_temp = pd.read_csv(input_directory + 'max_temp_df_2021.csv', dtype={'GEOID':str})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_temp = pd.read_csv(input_directory + 'min_temp_df_2021.csv', dtype={'GEOID':str})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# return FCI-normal table for the input date\n",
    "# set path_to_fci to where FCI matrices are stored\n",
    "def get_normal_fci(date):\n",
    "    path_to_fci = './output/' + str(date.year) + '/'+ date.strftime('%m') + \n",
    "                '/FCI_normal/' + date.strftime('%Y-%m-%d') + '-FCI-normal.csv'\n",
    "    fci_norm = pd.read_csv(path_to_fci, dtype={'Unnamed: 0':str})\n",
    "    fci_norm.set_index('Unnamed: 0', inplace=True)\n",
    "    \n",
    "    to_drop=[]\n",
    "\n",
    "    for index in fci_norm.index:\n",
    "        if not index in contiguous_fips:\n",
    "            to_drop.append(index)\n",
    "            \n",
    "    fci_norm.drop(to_drop, inplace=True)\n",
    "    fci_norm.drop(to_drop, axis=1, inplace=True)\n",
    "    return fci_norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculates weekly average FPC using the end date and the start date of the week\n",
    "# the input to this fuction should be of type datetime\n",
    "def weekly_mean_FPC(end_date, start_date, logged=False):\n",
    "    \n",
    "    dates = [end_date]\n",
    "    while end_date> start_date:\n",
    "        end_date -= timedelta(days=1)\n",
    "        dates.append(end_date)\n",
    "    \n",
    "    \n",
    "    temp = data[['GEOID','FIPS', 'POPULATION']]\n",
    "    \n",
    "    \n",
    "    for date in dates:\n",
    "        # convert date to String\n",
    "        date_str = date.strftime('%Y-%m-%d')\n",
    "        \n",
    "        temp = temp.merge(covid_df_contiguous[['FIPS', date_str]], on='FIPS', how='left')\n",
    "        \n",
    "        if logged:\n",
    "            temp['inc_rate_' + date_str] = np.log(temp[date_str] / temp['POPULATION'] * 10000 + 1)\n",
    "            \n",
    "        else:\n",
    "            temp['inc_rate_' + date_str] = temp[date_str] / temp['POPULATION'] * 10000\n",
    "        \n",
    "        normal_fci = get_normal_fci(date)\n",
    "        normal_fci = normal_fci.merge(temp, left_index= True, right_on='GEOID')\n",
    "        \n",
    "        normal_fci['fpc_'+ date_str] = np.dot(normal_fci.iloc[:,:number_counties], normal_fci['inc_rate_' + date_str])\n",
    "        \n",
    "    \n",
    "    normal_fci['mean_fpc'] = normal_fci.iloc[:,-len(dates):].mean(axis=1)\n",
    "        \n",
    "    return normal_fci[['GEOID','mean_fpc']]    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculates weekly average SPC\n",
    "# the input to this fuction should be of type datetime\n",
    "def weekly_mean_SPC(end_date, start_date, logged=False):\n",
    "    \n",
    "    dates = [end_date]\n",
    "    while end_date> start_date:\n",
    "        end_date -= timedelta(days=1)\n",
    "        dates.append(end_date)\n",
    "    \n",
    "    \n",
    "    temp = data[['GEOID','FIPS', 'POPULATION']]\n",
    "    \n",
    "    for date in dates:\n",
    "        # convert date to String\n",
    "        date_str = date.strftime('%Y-%m-%d')\n",
    "        \n",
    "        temp = temp.merge(covid_df_contiguous[['FIPS', date_str]], on='FIPS', how='left')\n",
    "        \n",
    "        if logged:\n",
    "            temp['inc_rate_' + date_str] = np.log(temp[date_str] / temp['POPULATION'] * 10000 + 1)\n",
    "            \n",
    "        else:\n",
    "            temp['inc_rate_' + date_str] = temp[date_str] / temp['POPULATION'] * 10000\n",
    "        \n",
    "        \n",
    "        normal_sci = sci_matrix_normal.merge(temp, left_index= True, right_on='GEOID')\n",
    "        \n",
    "        normal_sci['spc_'+ date_str] = np.dot(normal_sci.iloc[:,:number_counties], normal_sci['inc_rate_' + date_str])\n",
    "        \n",
    "    \n",
    "    normal_sci['mean_spc'] = normal_sci.iloc[:,-len(dates):].mean(axis=1)\n",
    "        \n",
    "    return normal_sci[['GEOID','mean_spc']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the input to this fuction should be of type datetime.\n",
    "# returns a subset of FB movement range dfs based on the given week\n",
    "def weekly_fb_mobility(end_date, start_date, df):\n",
    "    \n",
    "    dates = [end_date]\n",
    "    while end_date> start_date:\n",
    "        end_date -= timedelta(days=1)\n",
    "        dates.append(end_date)\n",
    "    \n",
    "    dates_str=[]\n",
    "    for date in dates:\n",
    "        # convert date to String\n",
    "        dates_str.append(date.strftime('%Y-%m-%d'))\n",
    "    \n",
    "    return df[['GEOID', *dates_str]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate slope features\n",
    "\n",
    "from scipy.stats import linregress\n",
    "\n",
    "def linear_reg(week_df):\n",
    "    \n",
    "    x = np.arange(1,(week_df.shape[1]),1)\n",
    "    x = (x - np.mean(x))/ np.std(x)\n",
    "    \n",
    "    slopes=[]\n",
    "    \n",
    "    for index, row in week_df.iloc[:,1:].iterrows():\n",
    "        y = row\n",
    "        y = (y - np.mean(y))/ np.std(y)\n",
    "        slopes.append(linregress(x, y)[0])\n",
    "        \n",
    "    week_df.loc[:,'slope'] = slopes\n",
    "    return week_df[['GEOID','slope']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final function to combine all features generated above\n",
    "\n",
    "This function generates a dataframe and for a given date, will add the following features to the dataframe\n",
    "\n",
    "- incidence rate data\n",
    "- FB mobility data (ratio, relative)\n",
    "- SPC (facebook SCI and incidence rates)\n",
    "- SafeGraph mobility \n",
    "- FPC (FCI and incidence rate)\n",
    "\n",
    "For each period, there is a 5 week difference between the actual date (t) and the start of the second lag. For example if `T: Oct 1 (Sep 24 to Oct 1)`, then `T-1: Sep 10 to Sep 24`, and `T-2: August 27 to Sep 10`.\n",
    "\n",
    "Since the earliest day for which we have FB mobility data is March 1, the rearliest  (end) date for T will be April 5th."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "covid_df_contiguous = data[['FIPS','STATEFP','COUNTYFP','GEOID']].merge(covid_df, on='FIPS', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main function\n",
    "# the input to the funtion is the end date for which we want to add data\n",
    "\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "\n",
    "def add_lagged_data(date):\n",
    "    global data\n",
    "    \n",
    "    T_end = date\n",
    "    T_start = T_end - timedelta(days=6)\n",
    "    \n",
    "    T_1_end = T_start - timedelta(days=1)\n",
    "    T_1_start = T_1_end - timedelta(days=6)\n",
    "    \n",
    "    T_2_end = T_1_start - timedelta(days=1)\n",
    "    T_2_start = T_2_end - timedelta(days=6)\n",
    "    \n",
    "    T_3_end = T_2_start - timedelta(days=1)\n",
    "    T_3_start = T_3_end - timedelta(days=6)\n",
    "    \n",
    "    T_4_end = T_3_start - timedelta(days=1)\n",
    "    T_4_start = T_4_end - timedelta(days=6)\n",
    "    \n",
    "    # These dates are used for cumulative cases (Saturday to Saturday)\n",
    "    T_start_case = T_end - timedelta(days=7)\n",
    "    T_1_start_case = T_1_end - timedelta(days=7)\n",
    "    T_2_start_case = T_2_end - timedelta(days=7)\n",
    "    T_3_start_case = T_3_end - timedelta(days=7)\n",
    "    T_4_start_case = T_4_end - timedelta(days=7)\n",
    "    \n",
    "\n",
    "    dates = [T_end.strftime('%Y-%m-%d'), T_start.strftime('%Y-%m-%d'),\n",
    "             T_1_end.strftime('%Y-%m-%d'), T_1_start.strftime('%Y-%m-%d'), \n",
    "             T_2_end.strftime('%Y-%m-%d'), T_2_start.strftime('%Y-%m-%d'),\n",
    "             T_3_end.strftime('%Y-%m-%d'), T_3_start.strftime('%Y-%m-%d'), \n",
    "             T_4_end.strftime('%Y-%m-%d'), T_4_start.strftime('%Y-%m-%d')]\n",
    "    \n",
    "    \n",
    "    dates_non_str = [T_end, T_start,\n",
    "             T_1_end, T_1_start, \n",
    "             T_2_end, T_2_start,\n",
    "             T_3_end, T_3_start, \n",
    "             T_4_end, T_4_start]\n",
    "    \n",
    "    dates_case = [T_end.strftime('%Y-%m-%d'), T_start_case.strftime('%Y-%m-%d'),\n",
    "             T_1_end.strftime('%Y-%m-%d'), T_1_start_case.strftime('%Y-%m-%d'), \n",
    "             T_2_end.strftime('%Y-%m-%d'), T_2_start_case.strftime('%Y-%m-%d'),\n",
    "             T_3_end.strftime('%Y-%m-%d'), T_3_start_case.strftime('%Y-%m-%d'), \n",
    "             T_4_end.strftime('%Y-%m-%d'), T_4_start_case.strftime('%Y-%m-%d')]\n",
    "    \n",
    "    \n",
    "    dates_non_str_case = [T_end, T_start_case,\n",
    "             T_1_end, T_1_start_case, \n",
    "             T_2_end, T_2_start_case,\n",
    "             T_3_end, T_3_start_case, \n",
    "             T_4_end, T_4_start_case]\n",
    "\n",
    "    \n",
    "    temp = data.copy()\n",
    "    \n",
    "    temp['date_end_period'] = T_end.strftime('%Y-%m-%d')\n",
    "    temp['date_start_period'] = T_start.strftime('%Y-%m-%d')\n",
    "    temp['date_end_lag'] = T_1_end.strftime('%Y-%m-%d')\n",
    "    temp['date_start_lag'] = T_4_start.strftime('%Y-%m-%d')\n",
    "    \n",
    "    time_periods = ['T_end', 'T_start', 'T_1_end', 'T_1_start','T_2_end','T_2_start',\n",
    "                    'T_3_end','T_3_start','T_4_end','T_4_start']\n",
    "    i = 0\n",
    "    for period in time_periods:\n",
    "        \n",
    "        \n",
    "        temp = temp.merge(covid_df_contiguous[['GEOID',dates_case[i]]], on='GEOID', how='left')\n",
    "        temp.rename(columns={dates_case[i]:'case_'+ period}, inplace=True)\n",
    "\n",
    "        \n",
    "        temp['inc_rate_' + period] = temp['case_'+ period] / temp['POPULATION'] * 10000\n",
    "        \n",
    "        temp = temp.merge(relative_df_smooth[['GEOID',dates[i]]], on='GEOID', how='left')\n",
    "        temp.rename(columns={dates[i]:'relative_'+ period}, inplace=True) \n",
    "\n",
    "\n",
    "        temp = temp.merge(ratio_df_smooth[['GEOID',dates[i]]], on='GEOID', how='left')\n",
    "        temp.rename(columns={dates[i]:'ratio_'+ period}, inplace=True)\n",
    "        \n",
    "        # The same date is used as the input to weekly_mean_SPC function to calculate\n",
    "        # SPC for that given date (instead of an average over a period)\n",
    "        temp = temp.merge(weekly_mean_SPC(dates_non_str_case[i],dates_non_str_case[i], logged=False), on='GEOID', how='left')\n",
    "        temp.rename(columns={'mean_spc':'SPC_'+ period}, inplace=True)\n",
    "\n",
    "        # simiar to SPC, add FPC values\n",
    "        temp = temp.merge(weekly_mean_FPC(dates_non_str_case[i],dates_non_str_case[i], logged=False), on='GEOID', how='left')\n",
    "        temp.rename(columns={'mean_fpc':'FPC_'+ period}, inplace=True)\n",
    "        \n",
    "        \n",
    "        # add logged SPC (defined as log(delta incidence rate)*sci/sum(sci))\n",
    "        temp = temp.merge(weekly_mean_SPC(dates_non_str_case[i],dates_non_str_case[i], logged=True), on='GEOID', how='left')\n",
    "        temp.rename(columns={'mean_spc':'SPC_logged_'+ period}, inplace=True)\n",
    "        \n",
    "        \n",
    "        # add logged FPC (defined as log(delta incidence rate)*fci/sum(fci))\n",
    "        temp = temp.merge(weekly_mean_FPC(dates_non_str_case[i],dates_non_str_case[i], logged=True), on='GEOID', how='left')\n",
    "        temp.rename(columns={'mean_fpc':'FPC_logged_'+ period}, inplace=True)\n",
    "\n",
    "        # add raw John Hopkins case data\n",
    "        temp = temp.merge(jh_covid_df[['FIPS',dates_non_str_case[i].strftime('%#m/%#d/%y')]], on='FIPS', how='left')\n",
    "        temp.rename(columns={dates_non_str_case[i].strftime('%#m/%#d/%y'):'case_JH_'+ period}, inplace=True)\n",
    "        \n",
    "        # add smoothed John Hopkins case data\n",
    "        temp = temp.merge(covid_df_contiguous[['FIPS',dates_case[i]]], on='FIPS', how='left')\n",
    "        temp.rename(columns={dates_case[i]:'case_JH_smoothed_'+ period}, inplace=True)\n",
    "       \n",
    "        \n",
    "        \n",
    "        i+=1\n",
    "    \n",
    "    times = ['T', 'T_1', 'T_2', 'T_3', 'T_4']\n",
    "    \n",
    "    j = 0\n",
    "    for period in times:\n",
    "        temp['DELTA_INC_RATE_' + period] = temp['inc_rate_' + time_periods[j]] - temp['inc_rate_' + time_periods[j+1]]\n",
    "        \n",
    "        temp['DELTA_REL_MOB_' + period] = temp['relative_' + time_periods[j]] - temp['relative_' + time_periods[j+1]]\n",
    "        temp['DELTA_RATIO_MOB_' + period] = temp['ratio_' + time_periods[j]] - temp['ratio_' + time_periods[j+1]]\n",
    "        \n",
    "        temp['DELTA_SPC_' + period] = temp['SPC_' + time_periods[j]] - temp['SPC_' + time_periods[j+1]]\n",
    "        temp['DELTA_FPC_' + period] = temp['FPC_' + time_periods[j]] - temp['FPC_' + time_periods[j+1]]\n",
    "        temp['DELTA_SPC_LOGGED_' + period] = temp['SPC_logged_' + time_periods[j]] - temp['SPC_logged_' + time_periods[j+1]]\n",
    "        temp['DELTA_FPC_LOGGED_' + period] = temp['FPC_logged_' + time_periods[j]] - temp['FPC_logged_' + time_periods[j+1]]\n",
    "        \n",
    "        temp['DELTA_CASE_JH_' + period] = temp['case_JH_'+ time_periods[j]] - temp['case_JH_'+ time_periods[j+1]]\n",
    "        temp['DELTA_CASE_JH_SMOOTH_' + period] = temp['case_JH_smoothed_'+ time_periods[j]] - \n",
    "                                                 temp['case_JH_smoothed_'+ time_periods[j+1]]\n",
    "        \n",
    "        # mean incidence rate is calculated between Sunday and Saturday\n",
    "        temp['MEAN_INC_RATE_' + period] = covid_df_contiguous[pd.date_range(start=dates[j+1], end=dates[j])\n",
    "                                            .strftime('%Y-%m-%d')].mean(axis=1) / temp['POPULATION'] * 10000\n",
    "        temp['MEAN_REL_MOB_' + period] = relative_df_smooth[pd.date_range(start=dates[j+1], end=dates[j])\n",
    "                                            .strftime('%Y-%m-%d')].mean(axis=1)\n",
    "        temp['MEAN_RATIO_MOB_' + period] = ratio_df_smooth[pd.date_range(start=dates[j+1], end=dates[j])\n",
    "                                            .strftime('%Y-%m-%d')].mean(axis=1)\n",
    "\n",
    "        \n",
    "        # add Safegraph mobility features\n",
    "        safegraph_data = safegraph_contiguous[safegraph_contiguous['end_date']==dates[j]][safegraph_metrics]\n",
    "        temp = temp.merge(safegraph_data, left_on='GEOID', right_on='county_fips')\n",
    "        \n",
    "        rename_dict = dict()\n",
    "        for col in safegraph_metrics[1:]:\n",
    "            rename_dict[col] = col + '_' + period\n",
    "            \n",
    "        temp.rename(columns=rename_dict, inplace=True)\n",
    "        \n",
    "        \n",
    "        # add MEAN_FPC\n",
    "        temp = temp.merge(weekly_mean_FPC(dates_non_str_case[j], dates_non_str_case[j+1]), on='GEOID', how='left')\n",
    "        temp.rename(columns={'mean_fpc':'MEAN_FPC_'+ period}, inplace=True)\n",
    "        \n",
    "        # add logged MEAN_FPC \n",
    "        temp = temp.merge(weekly_mean_FPC(dates_non_str_case[j], dates_non_str_case[j+1], logged=True), on='GEOID', how='left')\n",
    "        temp.rename(columns={'mean_fpc':'MEAN_FPC_LOGGED_'+ period}, inplace=True)\n",
    "        \n",
    "        # add MEAN_SPC\n",
    "        temp = temp.merge(weekly_mean_SPC(dates_non_str_case[j], dates_non_str_case[j+1]), on='GEOID', how='left')\n",
    "        temp.rename(columns={'mean_spc':'MEAN_SPC_'+ period}, inplace=True)\n",
    "        \n",
    "        # add logged MEAN_SPC\n",
    "        temp = temp.merge(weekly_mean_SPC(dates_non_str_case[j], dates_non_str_case[j+1], logged=True), on='GEOID', how='left')\n",
    "        temp.rename(columns={'mean_spc':'MEAN_SPC_LOGGED_'+ period}, inplace=True)\n",
    "        \n",
    "        # add FB mobility slopes\n",
    "        ratio_slope = linear_reg(weekly_fb_mobility(dates_non_str[j], dates_non_str[j+1], ratio_df_smooth))\n",
    "        temp = temp.merge(ratio_slope, on='GEOID', how='left')\n",
    "        temp.rename(columns={'slope':'SLOPE_RATIO_MOB_'+ period}, inplace=True)\n",
    "        \n",
    "        relative_slope = linear_reg(weekly_fb_mobility(dates_non_str[j], dates_non_str[j+1], relative_df_smooth))\n",
    "        temp = temp.merge(relative_slope, on='GEOID', how='left')\n",
    "        temp.rename(columns={'slope':'SLOPE_REL_MOB_'+ period}, inplace=True)\n",
    "\n",
    "        \n",
    "        \n",
    "        # add temperature features\n",
    "        # to update for the new dates, min and max temperature are used with one day offset\n",
    "        adj_temp_date = (dates_non_str[j] + timedelta(days=1)).strftime('%Y-%m-%d')\n",
    "        \n",
    "        temp = temp.merge(max_temp[['GEOID', adj_temp_date]], on='GEOID', how='left')\n",
    "        temp.rename(columns={adj_temp_date:'MAX_TEMP_'+ period}, inplace=True)\n",
    "        \n",
    "        temp = temp.merge(min_temp[['GEOID', adj_temp_date]], on='GEOID', how='left')\n",
    "        temp.rename(columns={adj_temp_date:'MIN_TEMP_'+ period}, inplace=True)\n",
    "\n",
    "        j += 2\n",
    "\n",
    "    output_df = temp.copy()\n",
    "    \n",
    "    return output_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "end_date = datetime(2021, 2, 27)\n",
    "week_counter = 0\n",
    "df_list = []\n",
    "\n",
    "while end_date > datetime(2020, 3, 29):\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    df_list.append(add_lagged_data(end_date))\n",
    "    \n",
    "    end_time = time.time()\n",
    "    print('Feature generation for the week ending in {} finished in {} seconds'.format(end_date.strftime('%Y-%m-%d'),\n",
    "                                                                                       round(end_time-start_time,1)))\n",
    "    \n",
    "    end_date -= timedelta(weeks=1)\n",
    "    week_counter+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df = pd.concat(df_list, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check number of weeks for which we have features\n",
    "final_df.shape[0]/3103"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imputing NAs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pd.set_option('max_rows', 200)\n",
    "final_df.isna().sum().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('max_rows', 400)\n",
    "final_df.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_to_save = final_df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "na_columns = data_to_save.columns[data_to_save.isnull().any()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# impute missing values by state average\n",
    "for col in data_to_save[na_columns].columns:\n",
    "    data_to_save[col] = data_to_save.groupby('STATEFP')[col].transform(lambda x: x.fillna(x.mean()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_to_save.isna().sum().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "times = ['T', 'T_1', 'T_2', 'T_3', 'T_4']\n",
    "\n",
    "for period in times:\n",
    "    data_to_save['LOG_DELTA_INC_RATE_' + period] = np.log(data_to_save['DELTA_INC_RATE_' + period] + 1)\n",
    "    data_to_save['LOG_MEAN_INC_RATE_' + period] = np.log(data_to_save['MEAN_INC_RATE_' + period] + 1)\n",
    "    data_to_save['LOG_DELTA_SPC_' + period] = np.log(data_to_save['DELTA_SPC_' + period] + 1)\n",
    "    data_to_save['LOG_DELTA_FPC_' + period] = np.log(data_to_save['DELTA_FPC_' + period] + 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_cols = [\n",
    "'GEOID',\n",
    "'NAME',\n",
    "'State_Name',\n",
    "'STATEFP', \n",
    "'COUNTYFP', \n",
    "'date_end_period',\n",
    "'date_start_period',\n",
    "'date_end_lag',\n",
    "'date_start_lag',\n",
    "'LOG_DELTA_INC_RATE_T',\n",
    "'PCT_MALE',\n",
    "'PCT_BLACK',\n",
    "'PCT_HISPAN', \n",
    "'PCT_AMIND',\n",
    "'PCT_RURAL',\n",
    "'PCT_COL_DE' ,\n",
    "'PCT_TRUMP_',\n",
    "'MED_HOS_IN',\n",
    "'POPULATION',\n",
    "'DELTA_CASE_JH_T',\n",
    "'DELTA_CASE_JH_SMOOTH_T'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "additional_cols=[\n",
    "'LOG_DELTA_INC_RATE_T_',\n",
    "'DELTA_REL_MOB_T_',\n",
    "'DELTA_RATIO_MOB_T_',\n",
    "'DELTA_SPC_T_',\n",
    "'DELTA_SPC_LOGGED_T_',\n",
    "'DELTA_FPC_T_',\n",
    "'DELTA_FPC_LOGGED_T_',\n",
    "'LOG_MEAN_INC_RATE_T_',\n",
    "'MEAN_REL_MOB_T_',\n",
    "'MEAN_RATIO_MOB_T_',\n",
    "'MEAN_FPC_T_',\n",
    "'MEAN_SPC_T_',\n",
    "'SLOPE_RATIO_MOB_T_',\n",
    "'SLOPE_REL_MOB_T_',\n",
    "'MAX_TEMP_T_',\n",
    "'MIN_TEMP_T_',\n",
    "'pct_completely_home_device_count_current_T_',\n",
    "'pct_full_time_work_behavior_devices_current_T_',\n",
    "'pct_part_time_work_behavior_devices_current_T_',\n",
    "'pct_delivery_behavior_devices_current_T_',\n",
    "'distance_traveled_from_home_current_T_',\n",
    "'median_home_dwell_time_current_T_',\n",
    "'pct_completely_home_device_count_baselined_T_',\n",
    "'pct_full_time_work_behavior_devices_baselined_T_',\n",
    "'pct_part_time_work_behavior_devices_baselined_T_',\n",
    "'pct_delivery_behavior_devices_baselined_T_',\n",
    "'distance_traveled_from_home_baselined_T_',\n",
    "'median_home_dwell_time_baselined_T_',\n",
    "'pct_completely_home_device_count_slope_T_',\n",
    "'pct_full_time_work_behavior_devices_slope_T_',\n",
    "'pct_part_time_work_behavior_devices_slope_T_',\n",
    "'pct_delivery_behavior_devices_slope_T_',\n",
    "'distance_traveled_from_home_slope_T_',\n",
    "'median_home_dwell_time_slope_T_',\n",
    "'DELTA_CASE_JH_T_',\n",
    "'MEAN_SPC_LOGGED_T_',\n",
    "'MEAN_FPC_LOGGED_T_'\n",
    "]\n",
    "\n",
    "for i in range(1,5):\n",
    "    for col in additional_cols:\n",
    "        final_cols.append(col+str(i))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = data_to_save[final_cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output.to_csv('./output/all_features_updated_incidence.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating dataframes for 2, 3, and 4-week predictions\n",
    "\n",
    "in this dataframe, the target variables is the the number of cumulative cases in 2, 3, and 4 weeks ahead, denoted by `LOG_DELTA_INC_RATE_T_14`, `LOG_DELTA_INC_RATE_T_21`, and `LOG_DELTA_INC_RATE_T_28` respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main function\n",
    "# the input to the funtion is the end date for which we want to add data\n",
    "\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "\n",
    "def add_lagged_y(date):\n",
    "    global output, jh_covid_df\n",
    "    \n",
    "    T_end = date\n",
    "    T_start = T_end - timedelta(days=7)\n",
    "    \n",
    "    T_start_period = (T_end - timedelta(days=6)).strftime('%Y-%m-%d')\n",
    "    \n",
    "    T_14 =  T_end + timedelta(days=7)\n",
    "    T_21 =  T_end + timedelta(days=14)\n",
    "    T_28 =  T_end + timedelta(days=21)\n",
    "    \n",
    "\n",
    "    dates_non_str = [T_end, T_start, T_14, T_21, T_28]\n",
    "    \n",
    "    dates = [item.strftime('%Y-%m-%d') for item in dates_non_str]\n",
    "    \n",
    "    dates_jh = [item.strftime('%#m/%#d/%y') for item in dates_non_str]\n",
    "    \n",
    "    \n",
    "    periods = ['T_end', 'T_start', 'T_14', 'T_21', 'T_28']\n",
    "    \n",
    "    temp = output.loc[(output.date_end_period==dates[0]) & (output.date_start_period==T_start_period)].copy()\n",
    "    \n",
    "    temp['FIPS'] = temp['GEOID'].astype(int)\n",
    "    \n",
    "    #print('check 1 {}'.format(temp.shape))\n",
    "    temp['target_date_2wk'] = T_14.strftime('%Y-%m-%d')\n",
    "    temp['target_date_3wk'] = T_21.strftime('%Y-%m-%d')\n",
    "    temp['target_date_4wk'] = T_28.strftime('%Y-%m-%d')\n",
    "        \n",
    "    temp = temp.merge(covid_df_contiguous[['GEOID',*dates]], on='GEOID', how='left')\n",
    "    \n",
    "    temp = temp.merge(jh_covid_df[['FIPS',*dates_jh]], on='FIPS', how='left')\n",
    "    #print('check 2 {}'.format(temp.shape))\n",
    "\n",
    "    for period, date in zip(periods, dates):\n",
    "        temp['inc_rate_' + period] = temp[date] / temp['POPULATION'] * 10000\n",
    "\n",
    "\n",
    "    for period, date in zip(periods[-3:], dates[-3:]):\n",
    "        temp['DELTA_CASE_SMOOTHED_' + period] = temp[date] - temp[dates[1]]\n",
    "        temp['DELTA_INC_RATE_' + period] = temp['inc_rate_' + period] - temp['inc_rate_T_start']\n",
    "        temp['LOG_DELTA_INC_RATE_' + period] = np.log(temp['DELTA_INC_RATE_' + period] + 1)\n",
    "    \n",
    "    for period, date in zip(periods[-3:], dates_jh[-3:]):\n",
    "        temp['DELTA_CASE_JH_' + period] = temp[date] - temp[dates_jh[1]]\n",
    "    \n",
    "    temp['DELTA_CASE_JH_T'] = temp[dates_jh[0]] - temp[dates_jh[1]]\n",
    "        \n",
    "    \n",
    "    \n",
    "    cols = ['target_date_2wk','LOG_DELTA_INC_RATE_T_14', \n",
    "            'target_date_3wk','LOG_DELTA_INC_RATE_T_21',\n",
    "            'target_date_4wk','LOG_DELTA_INC_RATE_T_28' ]\n",
    "    #print('check 3 {}'.format(temp.shape))\n",
    "    \n",
    "    return temp[[*output.columns,'DELTA_CASE_JH_T',\n",
    "            'target_date_2wk','LOG_DELTA_INC_RATE_T_14', 'DELTA_CASE_SMOOTHED_T_14', 'DELTA_CASE_JH_T_14',\n",
    "            'target_date_3wk','LOG_DELTA_INC_RATE_T_21', 'DELTA_CASE_SMOOTHED_T_21', 'DELTA_CASE_JH_T_21',\n",
    "            'target_date_4wk','LOG_DELTA_INC_RATE_T_28', 'DELTA_CASE_SMOOTHED_T_28', 'DELTA_CASE_JH_T_28']]\n",
    "    \n",
    "    \n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "end_date = datetime(2021, 2, 27)\n",
    "df_lagged_list = []\n",
    "\n",
    "while end_date > datetime(2020, 3, 29):\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    df_lagged_list.append(add_lagged_y(end_date))\n",
    "    \n",
    "    end_time = time.time()\n",
    "    print('Feature generation for the week ending in {} finished in {} seconds'.format(end_date.strftime('%Y-%m-%d'),\n",
    "                                                                                       round(end_time-start_time,1)))\n",
    "    end_date -= timedelta(weeks=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_lagged = pd.concat(df_lagged_list, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_lagged.isna().sum().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_lagged.shape, df_lagged.shape[0]/3103"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_lagged.to_csv('./output/all_features.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sp",
   "language": "python",
   "name": "sp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
