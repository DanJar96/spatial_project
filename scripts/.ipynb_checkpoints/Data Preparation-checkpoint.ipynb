{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preparation\n",
    "\n",
    "This notebook is used to clean up, preprocess, and aggregate data necessary for STXGB models.\n",
    "\n",
    "Please refer to the `Method` section of the article and Supplementary Information document for more information.\n",
    "\n",
    "\n",
    "In order to run this notebook, you have to download the required datasets from this [address](https://drive.google.com/drive/u/1/folders/1laAZFCvsPLLaKDvg0isTMMr20kMe0x_r). Once downloaded, set the directory in which you have save the files as `input_directory` in the cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_directory = '/home/dante/SpatialData/spatial_project/data/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import geopandas as gpd\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import platform\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "import requests\n",
    "from urllib.request import urlopen\n",
    "import json\n",
    "\n",
    "\n",
    "pd.set_option('display.max_rows', 500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def datetimer(cellstring):\n",
    "    if cellstring == \"Total\":\n",
    "        return cellstring\n",
    "    strlist = cellstring.split(\" \")\n",
    "    return f\"{strlist[1]}-{strlist[3]}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2020-01'"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datetimer(\"Year 2020 Week 01\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Set date string formatting based on operating system\n",
    "\n",
    "if platform.system() == 'Windows':\n",
    "    conversion_format = '%#m/%#d/%y'\n",
    "else:\n",
    "    conversion_format = '%-m/%-d/%y'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing COVID data (HOSPITAL DISTRICT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://sampo.thl.fi/pivot/prod/en/epirapo/covid19case/fact_epirapo_covid19case.json\"\n",
    "response = urlopen(url)\n",
    "\n",
    "data_json = json.loads(response.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dict = dict(data_json)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# print(data_dict['dataset'].keys())\n",
    "# print(data_dict['dataset']['class'])\n",
    "# print(data_dict['dataset']['label'])\n",
    "# print(data_dict['dataset']['dimension'].keys())\n",
    "# print(data_dict['dataset']['value'].keys())\n",
    "# print(len(list(data_dict['dataset']['value'].values())))\n",
    "# print(data_dict['dataset']['dimension']['hcdmunicipality2020']['category'].keys())\n",
    "# print(data_dict['dataset']['dimension']['hcdmunicipality2020']['category']['index'].items())\n",
    "# print(data_dict['dataset']['dimension']['hcdmunicipality2020']['category']['label'].items())\n",
    "# print(data_dict['dataset']['dimension']['dateweek20200101']['category']['label'].values())\n",
    "\n",
    "\n",
    "# defining handy quantities\n",
    "hcd_to_number = {v:k for k,v in data_dict['dataset']['dimension']['hcdmunicipality2020']['category']['label'].items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# len(data_dict['dataset']['value'])\n",
    "hospital_districts = dict(data_dict['dataset']['dimension']['hcdmunicipality2020']['category']['label'])\n",
    "hospital_districts_no = len(hospital_districts)\n",
    "\n",
    "casebyweek = dict(data_dict['dataset']['value'])\n",
    "weeks_no = int(len(casebyweek.values()) / len(hospital_districts.values()))\n",
    "\n",
    "weeks_no\n",
    "\n",
    "weeks_range = data_dict['dataset']['dimension']['dateweek20200101']['category']['label']\n",
    "\n",
    "week_list = list(weeks_range.values()) * len(hospital_districts.values())\n",
    "week_list = (week_list[0:weeks_no-1]+['Total']) * len(hospital_districts.values())\n",
    "\n",
    "hospital_districts_rep = [entry for entry in list(hospital_districts.values()) for _ in range(weeks_no)][0:len(casebyweek.values())]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2552\n",
      "2552\n",
      "2552\n"
     ]
    }
   ],
   "source": [
    "datas = {\n",
    "    'week' : week_list,\n",
    "    'hospital_district' : hospital_districts_rep,\n",
    "    'new_cases' : list(casebyweek.values())\n",
    "}\n",
    "\n",
    "for key in datas.keys():\n",
    "    print(len(datas[key]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating dataframe object from our dictionary\n",
    "df = pd.DataFrame(datas)\n",
    "\n",
    "# Casting new cases as int\n",
    "df['new_cases'] = df.new_cases.astype(int)\n",
    "\n",
    "# Creating cumulative sum column, with 0 if the 'week' is equal to total\n",
    "df['cumsum'] = df.groupby('hospital_district')['new_cases'].cumsum()\n",
    "df['cumsum'] = np.where(df['week'] == 'Total',0,df['cumsum'])\n",
    "\n",
    "# Applying the datetimer function, then transforming into datetime\n",
    "df['week_n'] = df.iloc[:,0].apply(datetimer)\n",
    "df.drop(columns=['week'],axis=1,inplace=True)\n",
    "df.rename(columns={'week_n':'Y_W'},inplace=True)\n",
    "\n",
    "# Adding all data we have about each hospital district\n",
    "df['hospital_district_id'] = df.iloc[:,0].map(hcd_to_number)\n",
    "\n",
    "\n",
    "# Splitting into weekly all-areas, totals, and hcd by week\n",
    "df_aa = df[df.hospital_district == 'All areas'] \\\n",
    "            .reset_index(drop=True) \\\n",
    "            .iloc[:-1,:]\n",
    "\n",
    "df_tot = df[df.Y_W == 'Total'] \\\n",
    "            .reset_index(drop=True)\\\n",
    "            .drop(columns=['Y_W','cumsum']) \\\n",
    "            .rename(columns={'new_cases':'total'})\n",
    "\n",
    "df_hcd_week = df[(df.Y_W != 'Total') & (df.hospital_district != 'All areas')] \\\n",
    "                .reset_index(drop=True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_aa.head(115)\n",
    "df_aa.to_csv('/home/dante/SpatialData/spatial_project/data/processed/all_area_ts.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tot.head(hospital_districts_no)\n",
    "df_tot.to_csv('/home/dante/SpatialData/spatial_project/data/processed/total_by_hcd.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_hcd_week.head(weeks_no-1)\n",
    "df_hcd_week.to_csv('/home/dante/SpatialData/spatial_project/data/processed/hcd_ts.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing FB movement range data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "floc = '/home/dante/SpatialData/spatial_project/data/fb/movement-range-data-2020-03-01-2020-12-31/movement-range-data-2020-03-01--2020-12-31.txt'\n",
    "dtypes = {\n",
    "    'ds' : 'object',\n",
    "    'country' : 'object',\n",
    "    'polygon_source' : 'object',\n",
    "    'polygon_id' : 'object',\n",
    "    'polygon_name' : 'object',\n",
    "    'all_day_bing_tiles_visited_relative_change' : 'float64',\n",
    "    'all_day_ratio_single_tile_users' : 'float64',\n",
    "    'baseline_name' : 'object',\n",
    "    'baseline_type' : 'object',\n",
    "}\n",
    "headers = list(dtypes.keys())\n",
    "parse_dates = ['ds']\n",
    "df_mv = pd.read_csv(floc, sep='\\t', header=0, dtype=dtypes, names=headers, parse_dates=parse_dates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Eastern Finland     306\n",
       "Lapland             306\n",
       "Oulu                306\n",
       "Southern Finland    306\n",
       "Western Finland     306\n",
       "Name: polygon_name, dtype: int64"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_mv = df_mv[df_mv.country == 'FIN']\n",
    "df_mv.polygon_name.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## => movement range data for Finland is TRASH!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Google intra-region mobility data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# File location\n",
    "floc = '/home/dante/SpatialData/spatial_project/data/google/2020_FI_Region_Mobility_Report.csv'\n",
    "\n",
    "# Assigning dtypes to the to_csv function, as apparently it is very computationally hard\n",
    "# for Pandas to try and guess dtypes automatically.\n",
    "dtypes = {\n",
    "    'country_region_code' : 'object',\n",
    "    'country_region' : 'object',\n",
    "    'sub_region_1' : 'object',\n",
    "    'sub_region_2' : 'object',\n",
    "    'metro_area' : 'object',\n",
    "    'iso_3166_2_code' : 'object',\n",
    "    'census_fips_code' : 'object',\n",
    "    'place_id' : 'object',\n",
    "    'date' : 'object',\n",
    "    'retail_and_recreation_percent_change_from_baseline' : 'float64',\n",
    "    'grocery_and_pharmacy_percent_change_from_baseline' : 'float64',\n",
    "    'parks_percent_change_from_baseline' : 'float64',\n",
    "    'transit_stations_percent_change_from_baseline' : 'float64',\n",
    "    'workplaces_percent_change_from_baseline' : 'float64',\n",
    "    'residential_percent_change_from_baseline' : 'float64',\n",
    "}\n",
    "\n",
    "# Assigning the correct column names\n",
    "headers = list(dtypes.keys())\n",
    "\n",
    "# Telling Pandas what column is a datecolumn (so that it can be parsed)\n",
    "parse_dates = ['date']\n",
    "\n",
    "# Reading in file\n",
    "df_mr = pd.read_csv(floc,header=0,dtype=dtypes,names=headers,parse_dates=parse_dates)\n",
    "\n",
    "# Sorting values by date and sub_region_2,sub_region_1 as we want to be able to\n",
    "# look at the data in an intuitive way.\n",
    "df_mr.sort_values(by=['date','sub_region_2','sub_region_1'],inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a map between the ISO-codes of the different provinces and their ENG/FI/SWE \n",
    "# names, for legibility later on.\n",
    "df_codemap = pd.read_csv('/home/dante/SpatialData/spatial_project/data/google/mob_map.txt',sep=',').iloc[:,0:4]\n",
    "iso_name_map = dict(zip(df_codemap.iloc[:,0],df_codemap.iloc[:,3]))\n",
    "df_mr['province'] = df_mr.iso_3166_2_code.map(iso_name_map)\n",
    "\n",
    "\n",
    "# Dropping unneccesary columns\n",
    "df_mr.drop(columns=['country_region_code','country_region','census_fips_code','iso_3166_2_code','place_id','metro_area','sub_region_2','sub_region_1'],inplace=True)\n",
    "\n",
    "# Specifying down our data to the province level\n",
    "df_mr.dropna(subset=['province'],inplace=True)\n",
    "df_mr.reset_index(drop=True,inplace=True)\n",
    "\n",
    "# Casting date in correct format (apparently Pandas doesn't actually read the dtypes correctly)\n",
    "df_mr['date'] = pd.to_datetime(df_mr.date)\n",
    "\n",
    "# Adding day of year for gap checking if needed\n",
    "df_mr['dayofyear'] = df_mr['date'].dt.dayofyear\n",
    "\n",
    "# Defining the movement data columns of interest\n",
    "# and filling in NaNs with 0 (i.e no change/information).\n",
    "# I do not know how sustainable this assumption is...\n",
    "mvcols = [col for col in df_mr.columns if 'baseline' in col]\n",
    "values = {col : 0 for col in mvcols}\n",
    "df_mr.fillna(value=values, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>retail_and_recreation_percent_change_from_baseline</th>\n",
       "      <th>grocery_and_pharmacy_percent_change_from_baseline</th>\n",
       "      <th>parks_percent_change_from_baseline</th>\n",
       "      <th>transit_stations_percent_change_from_baseline</th>\n",
       "      <th>workplaces_percent_change_from_baseline</th>\n",
       "      <th>residential_percent_change_from_baseline</th>\n",
       "      <th>province</th>\n",
       "      <th>dayofyear</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2020-02-15</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>-5.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Central Finland</td>\n",
       "      <td>46</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2020-02-15</td>\n",
       "      <td>9.0</td>\n",
       "      <td>22.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Central Ostrobothnia</td>\n",
       "      <td>46</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2020-02-15</td>\n",
       "      <td>6.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>17.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Kainuu</td>\n",
       "      <td>46</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2020-02-15</td>\n",
       "      <td>5.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>-17.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Kymenlaakso</td>\n",
       "      <td>46</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2020-02-15</td>\n",
       "      <td>17.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>39.0</td>\n",
       "      <td>56.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>Lapland</td>\n",
       "      <td>46</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5755</th>\n",
       "      <td>2020-12-31</td>\n",
       "      <td>-10.0</td>\n",
       "      <td>28.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-15.0</td>\n",
       "      <td>-59.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>Southern Ostrobothnia</td>\n",
       "      <td>366</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5756</th>\n",
       "      <td>2020-12-31</td>\n",
       "      <td>-16.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>119.0</td>\n",
       "      <td>-46.0</td>\n",
       "      <td>-51.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>Southern Savonia</td>\n",
       "      <td>366</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5757</th>\n",
       "      <td>2020-12-31</td>\n",
       "      <td>-17.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>90.0</td>\n",
       "      <td>-38.0</td>\n",
       "      <td>-60.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>Southwest Finland</td>\n",
       "      <td>366</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5758</th>\n",
       "      <td>2020-12-31</td>\n",
       "      <td>-5.0</td>\n",
       "      <td>32.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-35.0</td>\n",
       "      <td>-60.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>Tavastia Proper</td>\n",
       "      <td>366</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5759</th>\n",
       "      <td>2020-12-31</td>\n",
       "      <td>-25.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>33.0</td>\n",
       "      <td>-53.0</td>\n",
       "      <td>-66.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>Uusimaa</td>\n",
       "      <td>366</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5760 rows × 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           date  retail_and_recreation_percent_change_from_baseline  \\\n",
       "0    2020-02-15                                                1.0    \n",
       "1    2020-02-15                                                9.0    \n",
       "2    2020-02-15                                                6.0    \n",
       "3    2020-02-15                                                5.0    \n",
       "4    2020-02-15                                               17.0    \n",
       "...         ...                                                ...    \n",
       "5755 2020-12-31                                              -10.0    \n",
       "5756 2020-12-31                                              -16.0    \n",
       "5757 2020-12-31                                              -17.0    \n",
       "5758 2020-12-31                                               -5.0    \n",
       "5759 2020-12-31                                              -25.0    \n",
       "\n",
       "      grocery_and_pharmacy_percent_change_from_baseline  \\\n",
       "0                                                   2.0   \n",
       "1                                                  22.0   \n",
       "2                                                  14.0   \n",
       "3                                                   2.0   \n",
       "4                                                  21.0   \n",
       "...                                                 ...   \n",
       "5755                                               28.0   \n",
       "5756                                               25.0   \n",
       "5757                                               15.0   \n",
       "5758                                               32.0   \n",
       "5759                                                7.0   \n",
       "\n",
       "      parks_percent_change_from_baseline  \\\n",
       "0                                    4.0   \n",
       "1                                    0.0   \n",
       "2                                   17.0   \n",
       "3                                  -17.0   \n",
       "4                                   39.0   \n",
       "...                                  ...   \n",
       "5755                                 0.0   \n",
       "5756                               119.0   \n",
       "5757                                90.0   \n",
       "5758                                 0.0   \n",
       "5759                                33.0   \n",
       "\n",
       "      transit_stations_percent_change_from_baseline  \\\n",
       "0                                               9.0   \n",
       "1                                               0.0   \n",
       "2                                               0.0   \n",
       "3                                               6.0   \n",
       "4                                              56.0   \n",
       "...                                             ...   \n",
       "5755                                          -15.0   \n",
       "5756                                          -46.0   \n",
       "5757                                          -38.0   \n",
       "5758                                          -35.0   \n",
       "5759                                          -53.0   \n",
       "\n",
       "      workplaces_percent_change_from_baseline  \\\n",
       "0                                        -5.0   \n",
       "1                                        -2.0   \n",
       "2                                         3.0   \n",
       "3                                         1.0   \n",
       "4                                         2.0   \n",
       "...                                       ...   \n",
       "5755                                    -59.0   \n",
       "5756                                    -51.0   \n",
       "5757                                    -60.0   \n",
       "5758                                    -60.0   \n",
       "5759                                    -66.0   \n",
       "\n",
       "      residential_percent_change_from_baseline               province  \\\n",
       "0                                          0.0        Central Finland   \n",
       "1                                          0.0   Central Ostrobothnia   \n",
       "2                                          0.0                 Kainuu   \n",
       "3                                          0.0            Kymenlaakso   \n",
       "4                                         -1.0                Lapland   \n",
       "...                                        ...                    ...   \n",
       "5755                                      12.0  Southern Ostrobothnia   \n",
       "5756                                      11.0       Southern Savonia   \n",
       "5757                                      13.0      Southwest Finland   \n",
       "5758                                      13.0        Tavastia Proper   \n",
       "5759                                      15.0                Uusimaa   \n",
       "\n",
       "      dayofyear  \n",
       "0            46  \n",
       "1            46  \n",
       "2            46  \n",
       "3            46  \n",
       "4            46  \n",
       "...         ...  \n",
       "5755        366  \n",
       "5756        366  \n",
       "5757        366  \n",
       "5758        366  \n",
       "5759        366  \n",
       "\n",
       "[5760 rows x 9 columns]"
      ]
     },
     "execution_count": 177,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Checking out the data\n",
    "df_mr.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'FI-01': 'Åland',\n",
       " 'FI-02': 'South Karelia',\n",
       " 'FI-03': 'Southern Ostrobothnia',\n",
       " 'FI-04': 'Southern Savonia',\n",
       " 'FI-05': 'Kainuu',\n",
       " 'FI-06': 'Tavastia Proper',\n",
       " 'FI-07': 'Central Ostrobothnia',\n",
       " 'FI-08': 'Central Finland',\n",
       " 'FI-09': 'Kymenlaakso',\n",
       " 'FI-10': 'Lapland',\n",
       " 'FI-11': 'Pirkanmaa',\n",
       " 'FI-12': 'Ostrobothnia',\n",
       " 'FI-13': 'North Karelia',\n",
       " 'FI-14': 'Northern Ostrobothnia',\n",
       " 'FI-15': 'Northern Savonia',\n",
       " 'FI-16': 'Päijänne Tavastia',\n",
       " 'FI-17': 'Satakunta',\n",
       " 'FI-18': 'Uusimaa',\n",
       " 'FI-19': 'Southwest Finland'}"
      ]
     },
     "execution_count": 178,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Printing the names of the provinces\n",
    "iso_name_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Province 0 Central Finland has shape (321, 9)\n",
      "Province 1 Ostrobothnia has shape (321, 9)\n",
      "Province 2 Tavastia Proper has shape (321, 9)\n",
      "Province 3 Southwest Finland has shape (321, 9)\n",
      "Province 4 Southern Savonia has shape (321, 9)\n",
      "Province 5 Southern Ostrobothnia has shape (321, 9)\n",
      "Province 6 Satakunta has shape (321, 9)\n",
      "Province 7 Päijänne Tavastia has shape (321, 9)\n",
      "Province 8 Pirkanmaa has shape (321, 9)\n",
      "Province 9 Northern Savonia has shape (321, 9)\n",
      "Province 10 Northern Ostrobothnia has shape (321, 9)\n",
      "Province 11 Lapland has shape (321, 9)\n",
      "Province 12 Kymenlaakso has shape (321, 9)\n",
      "Province 13 Uusimaa has shape (321, 9)\n",
      "Province 14 South Karelia has shape (318, 9)\n",
      "Province 15 North Karelia has shape (318, 9)\n",
      "Province 16 Central Ostrobothnia has shape (315, 9)\n",
      "Province 17 Kainuu has shape (315, 9)\n"
     ]
    }
   ],
   "source": [
    "# Printing the province data shapes to make sure our data is comparable...\n",
    "for idx, prov in enumerate(list(df_mr.province.value_counts().index)):\n",
    "    print(f\"Province {idx} {prov} has shape {df_mr[df_mr.province == prov].shape}\")\n",
    "\n",
    "# based on the above, we will get rid of Åland, because it is not in this dataset,\n",
    "# nor is it connected geographically to the mainland. Would be interesting to try\n",
    "# and estimate the connectedness by numbers of ferry crossings between Åland and mainland Finland\n",
    "# as well as mainland Sweden!!! Just to prove that it matters... Here, the weighting would be by\n",
    "# day by how many ferry rides across!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating loop that will make the dataframe just like we want to i.e. in right format:\n",
    "1) Year-Week Province level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating empty list, so that we can then collect all separate \n",
    "# province dataframes into this list when processed by the below\n",
    "# for - loop...\n",
    "df_list = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_20002/2885164148.py:9: FutureWarning: Series.dt.weekofyear and Series.dt.week have been deprecated.  Please use Series.dt.isocalendar().week instead.\n",
      "  df['Y-W'] = df.date.dt.year.astype(str)+'-'+df.date.dt.week.astype(str)\n",
      "/tmp/ipykernel_20002/2885164148.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['diff']=df.dayofyear.diff()\n",
      "/tmp/ipykernel_20002/2885164148.py:5: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['date'] = pd.to_datetime(df['date']) - pd.to_timedelta(7, unit='d')\n",
      "/tmp/ipykernel_20002/2885164148.py:9: FutureWarning: Series.dt.weekofyear and Series.dt.week have been deprecated.  Please use Series.dt.isocalendar().week instead.\n",
      "  df['Y-W'] = df.date.dt.year.astype(str)+'-'+df.date.dt.week.astype(str)\n",
      "/tmp/ipykernel_20002/2885164148.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['diff']=df.dayofyear.diff()\n",
      "/tmp/ipykernel_20002/2885164148.py:5: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['date'] = pd.to_datetime(df['date']) - pd.to_timedelta(7, unit='d')\n",
      "/tmp/ipykernel_20002/2885164148.py:9: FutureWarning: Series.dt.weekofyear and Series.dt.week have been deprecated.  Please use Series.dt.isocalendar().week instead.\n",
      "  df['Y-W'] = df.date.dt.year.astype(str)+'-'+df.date.dt.week.astype(str)\n",
      "/tmp/ipykernel_20002/2885164148.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['diff']=df.dayofyear.diff()\n",
      "/tmp/ipykernel_20002/2885164148.py:5: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['date'] = pd.to_datetime(df['date']) - pd.to_timedelta(7, unit='d')\n",
      "/tmp/ipykernel_20002/2885164148.py:9: FutureWarning: Series.dt.weekofyear and Series.dt.week have been deprecated.  Please use Series.dt.isocalendar().week instead.\n",
      "  df['Y-W'] = df.date.dt.year.astype(str)+'-'+df.date.dt.week.astype(str)\n",
      "/tmp/ipykernel_20002/2885164148.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['diff']=df.dayofyear.diff()\n",
      "/tmp/ipykernel_20002/2885164148.py:5: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['date'] = pd.to_datetime(df['date']) - pd.to_timedelta(7, unit='d')\n",
      "/tmp/ipykernel_20002/2885164148.py:9: FutureWarning: Series.dt.weekofyear and Series.dt.week have been deprecated.  Please use Series.dt.isocalendar().week instead.\n",
      "  df['Y-W'] = df.date.dt.year.astype(str)+'-'+df.date.dt.week.astype(str)\n",
      "/tmp/ipykernel_20002/2885164148.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['diff']=df.dayofyear.diff()\n",
      "/tmp/ipykernel_20002/2885164148.py:5: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['date'] = pd.to_datetime(df['date']) - pd.to_timedelta(7, unit='d')\n",
      "/tmp/ipykernel_20002/2885164148.py:9: FutureWarning: Series.dt.weekofyear and Series.dt.week have been deprecated.  Please use Series.dt.isocalendar().week instead.\n",
      "  df['Y-W'] = df.date.dt.year.astype(str)+'-'+df.date.dt.week.astype(str)\n",
      "/tmp/ipykernel_20002/2885164148.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['diff']=df.dayofyear.diff()\n",
      "/tmp/ipykernel_20002/2885164148.py:5: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['date'] = pd.to_datetime(df['date']) - pd.to_timedelta(7, unit='d')\n",
      "/tmp/ipykernel_20002/2885164148.py:9: FutureWarning: Series.dt.weekofyear and Series.dt.week have been deprecated.  Please use Series.dt.isocalendar().week instead.\n",
      "  df['Y-W'] = df.date.dt.year.astype(str)+'-'+df.date.dt.week.astype(str)\n",
      "/tmp/ipykernel_20002/2885164148.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['diff']=df.dayofyear.diff()\n",
      "/tmp/ipykernel_20002/2885164148.py:5: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['date'] = pd.to_datetime(df['date']) - pd.to_timedelta(7, unit='d')\n",
      "/tmp/ipykernel_20002/2885164148.py:9: FutureWarning: Series.dt.weekofyear and Series.dt.week have been deprecated.  Please use Series.dt.isocalendar().week instead.\n",
      "  df['Y-W'] = df.date.dt.year.astype(str)+'-'+df.date.dt.week.astype(str)\n",
      "/tmp/ipykernel_20002/2885164148.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['diff']=df.dayofyear.diff()\n",
      "/tmp/ipykernel_20002/2885164148.py:5: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['date'] = pd.to_datetime(df['date']) - pd.to_timedelta(7, unit='d')\n",
      "/tmp/ipykernel_20002/2885164148.py:9: FutureWarning: Series.dt.weekofyear and Series.dt.week have been deprecated.  Please use Series.dt.isocalendar().week instead.\n",
      "  df['Y-W'] = df.date.dt.year.astype(str)+'-'+df.date.dt.week.astype(str)\n",
      "/tmp/ipykernel_20002/2885164148.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['diff']=df.dayofyear.diff()\n",
      "/tmp/ipykernel_20002/2885164148.py:5: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['date'] = pd.to_datetime(df['date']) - pd.to_timedelta(7, unit='d')\n",
      "/tmp/ipykernel_20002/2885164148.py:9: FutureWarning: Series.dt.weekofyear and Series.dt.week have been deprecated.  Please use Series.dt.isocalendar().week instead.\n",
      "  df['Y-W'] = df.date.dt.year.astype(str)+'-'+df.date.dt.week.astype(str)\n",
      "/tmp/ipykernel_20002/2885164148.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['diff']=df.dayofyear.diff()\n",
      "/tmp/ipykernel_20002/2885164148.py:5: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['date'] = pd.to_datetime(df['date']) - pd.to_timedelta(7, unit='d')\n",
      "/tmp/ipykernel_20002/2885164148.py:9: FutureWarning: Series.dt.weekofyear and Series.dt.week have been deprecated.  Please use Series.dt.isocalendar().week instead.\n",
      "  df['Y-W'] = df.date.dt.year.astype(str)+'-'+df.date.dt.week.astype(str)\n",
      "/tmp/ipykernel_20002/2885164148.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['diff']=df.dayofyear.diff()\n",
      "/tmp/ipykernel_20002/2885164148.py:5: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['date'] = pd.to_datetime(df['date']) - pd.to_timedelta(7, unit='d')\n",
      "/tmp/ipykernel_20002/2885164148.py:9: FutureWarning: Series.dt.weekofyear and Series.dt.week have been deprecated.  Please use Series.dt.isocalendar().week instead.\n",
      "  df['Y-W'] = df.date.dt.year.astype(str)+'-'+df.date.dt.week.astype(str)\n",
      "/tmp/ipykernel_20002/2885164148.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['diff']=df.dayofyear.diff()\n",
      "/tmp/ipykernel_20002/2885164148.py:5: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['date'] = pd.to_datetime(df['date']) - pd.to_timedelta(7, unit='d')\n",
      "/tmp/ipykernel_20002/2885164148.py:9: FutureWarning: Series.dt.weekofyear and Series.dt.week have been deprecated.  Please use Series.dt.isocalendar().week instead.\n",
      "  df['Y-W'] = df.date.dt.year.astype(str)+'-'+df.date.dt.week.astype(str)\n",
      "/tmp/ipykernel_20002/2885164148.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['diff']=df.dayofyear.diff()\n",
      "/tmp/ipykernel_20002/2885164148.py:5: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['date'] = pd.to_datetime(df['date']) - pd.to_timedelta(7, unit='d')\n",
      "/tmp/ipykernel_20002/2885164148.py:9: FutureWarning: Series.dt.weekofyear and Series.dt.week have been deprecated.  Please use Series.dt.isocalendar().week instead.\n",
      "  df['Y-W'] = df.date.dt.year.astype(str)+'-'+df.date.dt.week.astype(str)\n",
      "/tmp/ipykernel_20002/2885164148.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['diff']=df.dayofyear.diff()\n",
      "/tmp/ipykernel_20002/2885164148.py:5: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['date'] = pd.to_datetime(df['date']) - pd.to_timedelta(7, unit='d')\n",
      "/tmp/ipykernel_20002/2885164148.py:9: FutureWarning: Series.dt.weekofyear and Series.dt.week have been deprecated.  Please use Series.dt.isocalendar().week instead.\n",
      "  df['Y-W'] = df.date.dt.year.astype(str)+'-'+df.date.dt.week.astype(str)\n",
      "/tmp/ipykernel_20002/2885164148.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['diff']=df.dayofyear.diff()\n",
      "/tmp/ipykernel_20002/2885164148.py:5: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['date'] = pd.to_datetime(df['date']) - pd.to_timedelta(7, unit='d')\n",
      "/tmp/ipykernel_20002/2885164148.py:9: FutureWarning: Series.dt.weekofyear and Series.dt.week have been deprecated.  Please use Series.dt.isocalendar().week instead.\n",
      "  df['Y-W'] = df.date.dt.year.astype(str)+'-'+df.date.dt.week.astype(str)\n",
      "/tmp/ipykernel_20002/2885164148.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['diff']=df.dayofyear.diff()\n",
      "/tmp/ipykernel_20002/2885164148.py:5: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['date'] = pd.to_datetime(df['date']) - pd.to_timedelta(7, unit='d')\n",
      "/tmp/ipykernel_20002/2885164148.py:9: FutureWarning: Series.dt.weekofyear and Series.dt.week have been deprecated.  Please use Series.dt.isocalendar().week instead.\n",
      "  df['Y-W'] = df.date.dt.year.astype(str)+'-'+df.date.dt.week.astype(str)\n",
      "/tmp/ipykernel_20002/2885164148.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['diff']=df.dayofyear.diff()\n",
      "/tmp/ipykernel_20002/2885164148.py:5: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['date'] = pd.to_datetime(df['date']) - pd.to_timedelta(7, unit='d')\n",
      "/tmp/ipykernel_20002/2885164148.py:9: FutureWarning: Series.dt.weekofyear and Series.dt.week have been deprecated.  Please use Series.dt.isocalendar().week instead.\n",
      "  df['Y-W'] = df.date.dt.year.astype(str)+'-'+df.date.dt.week.astype(str)\n",
      "/tmp/ipykernel_20002/2885164148.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['diff']=df.dayofyear.diff()\n",
      "/tmp/ipykernel_20002/2885164148.py:5: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['date'] = pd.to_datetime(df['date']) - pd.to_timedelta(7, unit='d')\n",
      "/tmp/ipykernel_20002/2885164148.py:9: FutureWarning: Series.dt.weekofyear and Series.dt.week have been deprecated.  Please use Series.dt.isocalendar().week instead.\n",
      "  df['Y-W'] = df.date.dt.year.astype(str)+'-'+df.date.dt.week.astype(str)\n"
     ]
    }
   ],
   "source": [
    "# This for - loop puts the data in the week - province format.\n",
    "# Notice that we are using the MEAN movement per week!\n",
    "\n",
    "# For province in provinces...\n",
    "for prov in iso_name_map.values():\n",
    "    \n",
    "    # Specify the correct province \n",
    "    df = df_mr[df_mr.province == prov]\n",
    "    \n",
    "    # The below code bins the data according to week commencing MONDAY\n",
    "    # This code looks complicated, but achieves something quite understanable.\n",
    "    df['date'] = pd.to_datetime(df['date']) - pd.to_timedelta(7, unit='d')\n",
    "    df = df.groupby([pd.Grouper(key='date', freq='W-MON')])[mvcols].mean().reset_index().sort_values('date')\n",
    "\n",
    "    # Creating the column with which we can then merge to the cases data..\n",
    "    df['Y-W'] = df.date.dt.year.astype(str)+'-'+df.date.dt.week.astype(str)\n",
    "    \n",
    "    # Adding name of province\n",
    "    df['province'] = prov\n",
    "    df.reset_index(drop=True) \n",
    "    \n",
    "    # Finally, appending to the list of dfs...\n",
    "    df_list.append(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combining all the province df's into one big dataframe.\n",
    "combined_mv_df = pd.concat(df_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>retail_and_recreation_percent_change_from_baseline</th>\n",
       "      <th>grocery_and_pharmacy_percent_change_from_baseline</th>\n",
       "      <th>parks_percent_change_from_baseline</th>\n",
       "      <th>transit_stations_percent_change_from_baseline</th>\n",
       "      <th>workplaces_percent_change_from_baseline</th>\n",
       "      <th>residential_percent_change_from_baseline</th>\n",
       "      <th>Y-W</th>\n",
       "      <th>province</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2020-02-10</td>\n",
       "      <td>4.333333</td>\n",
       "      <td>3.666667</td>\n",
       "      <td>-7.666667</td>\n",
       "      <td>2.666667</td>\n",
       "      <td>-0.666667</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>2020-7</td>\n",
       "      <td>South Karelia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2020-02-17</td>\n",
       "      <td>4.142857</td>\n",
       "      <td>3.857143</td>\n",
       "      <td>-2.714286</td>\n",
       "      <td>3.142857</td>\n",
       "      <td>-5.428571</td>\n",
       "      <td>1.142857</td>\n",
       "      <td>2020-8</td>\n",
       "      <td>South Karelia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2020-02-24</td>\n",
       "      <td>-1.857143</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>7.571429</td>\n",
       "      <td>0.428571</td>\n",
       "      <td>-20.000000</td>\n",
       "      <td>1.857143</td>\n",
       "      <td>2020-9</td>\n",
       "      <td>South Karelia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2020-03-02</td>\n",
       "      <td>4.285714</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>2.571429</td>\n",
       "      <td>-2.000000</td>\n",
       "      <td>-0.714286</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2020-10</td>\n",
       "      <td>South Karelia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2020-03-09</td>\n",
       "      <td>-9.000000</td>\n",
       "      <td>3.857143</td>\n",
       "      <td>-2.285714</td>\n",
       "      <td>-19.000000</td>\n",
       "      <td>-7.857143</td>\n",
       "      <td>2.142857</td>\n",
       "      <td>2020-11</td>\n",
       "      <td>South Karelia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>2020-11-30</td>\n",
       "      <td>-15.000000</td>\n",
       "      <td>-2.571429</td>\n",
       "      <td>17.285714</td>\n",
       "      <td>-34.857143</td>\n",
       "      <td>-16.571429</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>2020-49</td>\n",
       "      <td>Southwest Finland</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>2020-12-07</td>\n",
       "      <td>-13.285714</td>\n",
       "      <td>-1.714286</td>\n",
       "      <td>16.285714</td>\n",
       "      <td>-38.285714</td>\n",
       "      <td>-17.714286</td>\n",
       "      <td>8.857143</td>\n",
       "      <td>2020-50</td>\n",
       "      <td>Southwest Finland</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>2020-12-14</td>\n",
       "      <td>-8.142857</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>12.428571</td>\n",
       "      <td>-40.000000</td>\n",
       "      <td>-22.714286</td>\n",
       "      <td>10.285714</td>\n",
       "      <td>2020-51</td>\n",
       "      <td>Southwest Finland</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>2020-12-21</td>\n",
       "      <td>-35.285714</td>\n",
       "      <td>-15.000000</td>\n",
       "      <td>23.714286</td>\n",
       "      <td>-51.571429</td>\n",
       "      <td>-54.571429</td>\n",
       "      <td>13.857143</td>\n",
       "      <td>2020-52</td>\n",
       "      <td>Southwest Finland</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>2020-12-28</td>\n",
       "      <td>-15.000000</td>\n",
       "      <td>7.333333</td>\n",
       "      <td>46.666667</td>\n",
       "      <td>-43.666667</td>\n",
       "      <td>-57.000000</td>\n",
       "      <td>14.666667</td>\n",
       "      <td>2020-53</td>\n",
       "      <td>Southwest Finland</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>846 rows × 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         date  retail_and_recreation_percent_change_from_baseline  \\\n",
       "0  2020-02-10                                           4.333333    \n",
       "1  2020-02-17                                           4.142857    \n",
       "2  2020-02-24                                          -1.857143    \n",
       "3  2020-03-02                                           4.285714    \n",
       "4  2020-03-09                                          -9.000000    \n",
       "..        ...                                                ...    \n",
       "42 2020-11-30                                         -15.000000    \n",
       "43 2020-12-07                                         -13.285714    \n",
       "44 2020-12-14                                          -8.142857    \n",
       "45 2020-12-21                                         -35.285714    \n",
       "46 2020-12-28                                         -15.000000    \n",
       "\n",
       "    grocery_and_pharmacy_percent_change_from_baseline  \\\n",
       "0                                            3.666667   \n",
       "1                                            3.857143   \n",
       "2                                           -1.000000   \n",
       "3                                            4.000000   \n",
       "4                                            3.857143   \n",
       "..                                                ...   \n",
       "42                                          -2.571429   \n",
       "43                                          -1.714286   \n",
       "44                                           3.000000   \n",
       "45                                         -15.000000   \n",
       "46                                           7.333333   \n",
       "\n",
       "    parks_percent_change_from_baseline  \\\n",
       "0                            -7.666667   \n",
       "1                            -2.714286   \n",
       "2                             7.571429   \n",
       "3                             2.571429   \n",
       "4                            -2.285714   \n",
       "..                                 ...   \n",
       "42                           17.285714   \n",
       "43                           16.285714   \n",
       "44                           12.428571   \n",
       "45                           23.714286   \n",
       "46                           46.666667   \n",
       "\n",
       "    transit_stations_percent_change_from_baseline  \\\n",
       "0                                        2.666667   \n",
       "1                                        3.142857   \n",
       "2                                        0.428571   \n",
       "3                                       -2.000000   \n",
       "4                                      -19.000000   \n",
       "..                                            ...   \n",
       "42                                     -34.857143   \n",
       "43                                     -38.285714   \n",
       "44                                     -40.000000   \n",
       "45                                     -51.571429   \n",
       "46                                     -43.666667   \n",
       "\n",
       "    workplaces_percent_change_from_baseline  \\\n",
       "0                                 -0.666667   \n",
       "1                                 -5.428571   \n",
       "2                                -20.000000   \n",
       "3                                 -0.714286   \n",
       "4                                 -7.857143   \n",
       "..                                      ...   \n",
       "42                               -16.571429   \n",
       "43                               -17.714286   \n",
       "44                               -22.714286   \n",
       "45                               -54.571429   \n",
       "46                               -57.000000   \n",
       "\n",
       "    residential_percent_change_from_baseline      Y-W           province  \n",
       "0                                   0.333333   2020-7      South Karelia  \n",
       "1                                   1.142857   2020-8      South Karelia  \n",
       "2                                   1.857143   2020-9      South Karelia  \n",
       "3                                   1.000000  2020-10      South Karelia  \n",
       "4                                   2.142857  2020-11      South Karelia  \n",
       "..                                       ...      ...                ...  \n",
       "42                                  8.000000  2020-49  Southwest Finland  \n",
       "43                                  8.857143  2020-50  Southwest Finland  \n",
       "44                                 10.285714  2020-51  Southwest Finland  \n",
       "45                                 13.857143  2020-52  Southwest Finland  \n",
       "46                                 14.666667  2020-53  Southwest Finland  \n",
       "\n",
       "[846 rows x 9 columns]"
      ]
     },
     "execution_count": 189,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Having a look at what the end result looks like.\n",
    "combined_mv_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving the file as a CSV.\n",
    "combined_mv_df.to_csv('/home/dante/SpatialData/spatial_project/data/processed/google_mobility_by_week_province.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = gpd.read_file( input_directory + 'Contiguous_US.geojson')\n",
    "\n",
    "# Or alternatively:\n",
    "# url='https://drive.google.com/file/d/1MVyLzzHl3hzno4o1rLZtI0peqIi23zsr/view?usp=sharing'\n",
    "# url_counties='https://drive.google.com/uc?id=' + url.split('/')[-2]\n",
    "# data = gpd.read_file(url_counties)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['STATEFP'] = data.apply(lambda L: L.GEOID[:2], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "global number_counties \n",
    "number_counties = data.shape[0] #3103"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data.sort_values(by='GEOID', inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load COVID data and apply smoothing \n",
    "\n",
    "To alleviate inconsistencies in reporting COVID-19 cases, we apply a 7-day moving average to the case data published by JHU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_JH_covid_data(target, smooth):\n",
    "    \n",
    "    '''\n",
    "    Parameters:\n",
    "    --------------\n",
    "        target: str\n",
    "            the target variable: either 'case' or 'death'\n",
    "            \n",
    "        smooth: bool\n",
    "            wether to smooth the data frame or not.\n",
    "            The smoothing is done by using a 7-day rolling average   \n",
    "    '''\n",
    "    \n",
    "    assert isinstance(smooth, bool), \"Smooth must be a boolean variable!\"\n",
    "    \n",
    "    base_url = 'https://raw.githubusercontent.com/CSSEGISandData/COVID-19/master/csse_covid_19_data/'\n",
    "    \n",
    "    \n",
    "    if target == 'case':\n",
    "        jh_data_url = base_url + 'csse_covid_19_time_series/time_series_covid19_confirmed_US.csv'\n",
    "        cols_to_drp = ['UID', 'iso2', 'iso3', 'code3','Country_Region', 'Lat', 'Long_']\n",
    "\n",
    "    elif target=='death':\n",
    "        jh_data_url = base_url + 'csse_covid_19_time_series/time_series_covid19_deaths_US.csv'\n",
    "        cols_to_drp = ['UID', 'iso2', 'iso3', 'code3','Country_Region', 'Lat', 'Long_','Population']\n",
    "    else:\n",
    "        print(\"invalid argument for target. Acceptable values are: 'case' or 'death'\")\n",
    "        return None\n",
    "\n",
    "    jh_covid_df = pd.read_csv(jh_data_url)\n",
    "\n",
    "    # preprocessing JH COVID data\n",
    "    jh_covid_df.dropna(axis=0, how='any', inplace=True)\n",
    "\n",
    "    jh_covid_df['FIPS'] = jh_covid_df['FIPS'].astype('int64')\n",
    "\n",
    "    jh_covid_df.drop(columns=cols_to_drp, inplace=True)\n",
    "\n",
    "    #Important: check to see the column index is adherent to the imported df\n",
    "\n",
    "    first_date = datetime.strptime(jh_covid_df.columns[4], '%m/%d/%y').date()\n",
    "\n",
    "    last_date = datetime.strptime(jh_covid_df.columns[-1], '%m/%d/%y').date()\n",
    "\n",
    "\n",
    "    current_date = last_date\n",
    "\n",
    "    previous_date = last_date - timedelta (days=1)\n",
    "\n",
    "\n",
    "    while current_date > first_date:\n",
    "\n",
    "        #For unix, replace # with - in the time format\n",
    "\n",
    "        current_col = current_date.strftime(conversion_format) #replace # with - in Mac or Linux\n",
    "\n",
    "        previous_col = previous_date.strftime(conversion_format)\n",
    "\n",
    "        jh_covid_df[previous_col] = np.where(jh_covid_df[previous_col] > jh_covid_df[current_col], \n",
    "                                             jh_covid_df[current_col], jh_covid_df[previous_col])\n",
    "\n",
    "        current_date = current_date - timedelta(days=1)\n",
    "\n",
    "        previous_date = previous_date - timedelta(days=1)\n",
    "        \n",
    "    \n",
    "    if smooth:\n",
    "        jh_covid_df.iloc[:,4:] = jh_covid_df.iloc[:,4:].rolling(7,min_periods=1,axis=1).mean()\n",
    "\n",
    "\n",
    "    return jh_covid_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "covid_df = get_JH_covid_data('case', True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Facebook Movement Range Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fb_mobility = pd.read_csv(input_directory + 'movement-range-2021-03-02.txt', sep=\"\\t\", dtype={'polygon_id':str})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fb_mobility_us = fb_mobility[fb_mobility['country']=='USA']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of unique counties for which we have at least one day of data\n",
    "len(fb_mobility_us['polygon_id'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# counting Counties in the contiguous US for which there is no data in FB mobility\n",
    "contiguous_fips = set(data['GEOID']) # number of unique fips: 3103\n",
    "mobility_fips = set(fb_mobility_us['polygon_id']) # number of unique fips: 2694\n",
    "i = 0\n",
    "missing_fips = []\n",
    "for fips in contiguous_fips:\n",
    "    if (fips in mobility_fips):\n",
    "        i+=1\n",
    "    else:\n",
    "        missing_fips.append(fips)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# number of Counties in the contiguous US for which there is no data in FB mobility\n",
    "len(missing_fips)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new dataframe as transpose of the above, with days as columns and counties as rows\n",
    "\n",
    "relative_df = pd.DataFrame(columns=fb_mobility_us['ds'].unique(), index=data['GEOID'].unique(), dtype=float)\n",
    "ratio_df = pd.DataFrame(columns=fb_mobility_us['ds'].unique(), index=data['GEOID'].unique(), dtype=float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fb_mobility_us"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx_contiguous = fb_mobility_us.index[fb_mobility_us['polygon_id'].isin(contiguous_fips)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fb_mobility_contiguous = fb_mobility_us.loc[idx_contiguous]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "start = time.time()\n",
    "\n",
    "for index, row in fb_mobility_contiguous.iterrows():\n",
    "    relative_df.loc[row['polygon_id']][row['ds']] = row['all_day_bing_tiles_visited_relative_change']\n",
    "    ratio_df.loc[row['polygon_id']][row['ds']] = row['all_day_ratio_single_tile_users']\n",
    "    \n",
    "\n",
    "end = time.time()\n",
    "print(end - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "relative_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(relative_df.shape , ratio_df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Impute FB mobility dataframes\n",
    "The two dataframes above have a lot of Nan values which should be imputed by state average"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_ratio_df = data[['GEOID', 'STATEFP']].merge(ratio_df, left_on='GEOID', right_index=True, how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in temp_ratio_df.iloc[:,2:].columns:\n",
    "    temp_ratio_df[col] = temp_ratio_df.groupby('STATEFP')[col].transform(lambda x: x.fillna(x.mean()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_ratio_df.isna().sum().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_relative_df = data[['GEOID', 'STATEFP']].merge(relative_df, left_on='GEOID', right_index=True, how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in temp_relative_df.iloc[:,2:].columns:\n",
    "    temp_relative_df[col] = temp_relative_df.groupby('STATEFP')[col].transform(lambda x: x.fillna(x.mean()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_relative_df.isna().sum().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "relative_df_smooth = temp_relative_df.copy()\n",
    "ratio_df_smooth = temp_ratio_df.copy()\n",
    "\n",
    "relative_df_smooth.iloc[:,2:] = relative_df_smooth.iloc[:,2:].rolling(7,min_periods=1, axis=1).mean()\n",
    "ratio_df_smooth.iloc[:,2:] = ratio_df_smooth.iloc[:,2:].rolling(7,min_periods=1, axis=1).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "relative_df_smooth.iloc[:,2:].columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "relative_df_smooth.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add Social Proximity to Cases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SCI_df = pd.read_csv(input_directory + 'SCI_matrix.csv', dtype={'Unnamed: 0':str})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SCI_df.set_index('Unnamed: 0', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create normalized SCI. It is calculated by dividing all the columns of the sci_matrix by the sum of the rpw\n",
    "# This would give us the second term in social proximity formula above\n",
    "\n",
    "sci_matrix_normal = SCI_df.div(SCI_df.sum(axis=1), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set diagonal to zero\n",
    "sci_matrix_normal.values[[np.arange(sci_matrix_normal.shape[0])]*2] = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The matrix above is created for the entire US, but we are using contiguous US data here, therefore some rows and columns should be removed**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_drop=[]\n",
    "\n",
    "for index in sci_matrix_normal.index:\n",
    "    if not index in contiguous_fips:\n",
    "        to_drop.append(index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sci_matrix_normal.drop(to_drop, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sci_matrix_normal.drop(to_drop, axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sci_matrix_normal.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add SafeGraph mobility features\n",
    "\n",
    "Updated based on forecast hub dates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "safegraph_mobility = pd.read_csv(input_directory + 'safegraph_mobility.csv', dtype={'county_fips':str})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "safegraph_contiguous = safegraph_mobility[safegraph_mobility['county_fips'].isin(contiguous_fips)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(safegraph_contiguous['county_fips'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_cols = safegraph_contiguous.drop(['start_date', 'end_date', 'base_start', 'base_end'], axis=1)\n",
    "safegraph_metrics = temp_cols.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(safegraph_metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Temperature Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_temp = pd.read_csv(input_directory + 'max_temp_df_2021.csv', dtype={'GEOID':str})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_temp = pd.read_csv(input_directory + 'min_temp_df_2021.csv', dtype={'GEOID':str})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# return FCI-normal table for the input date\n",
    "# set path_to_fci to where FCI matrices are stored\n",
    "def get_normal_fci(date):\n",
    "    path_to_fci = './output/' + str(date.year) + '/'+ date.strftime('%m') + \n",
    "                '/FCI_normal/' + date.strftime('%Y-%m-%d') + '-FCI-normal.csv'\n",
    "    fci_norm = pd.read_csv(path_to_fci, dtype={'Unnamed: 0':str})\n",
    "    fci_norm.set_index('Unnamed: 0', inplace=True)\n",
    "    \n",
    "    to_drop=[]\n",
    "\n",
    "    for index in fci_norm.index:\n",
    "        if not index in contiguous_fips:\n",
    "            to_drop.append(index)\n",
    "            \n",
    "    fci_norm.drop(to_drop, inplace=True)\n",
    "    fci_norm.drop(to_drop, axis=1, inplace=True)\n",
    "    return fci_norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculates weekly average FPC using the end date and the start date of the week\n",
    "# the input to this fuction should be of type datetime\n",
    "def weekly_mean_FPC(end_date, start_date, logged=False):\n",
    "    \n",
    "    dates = [end_date]\n",
    "    while end_date> start_date:\n",
    "        end_date -= timedelta(days=1)\n",
    "        dates.append(end_date)\n",
    "    \n",
    "    \n",
    "    temp = data[['GEOID','FIPS', 'POPULATION']]\n",
    "    \n",
    "    \n",
    "    for date in dates:\n",
    "        # convert date to String\n",
    "        date_str = date.strftime('%Y-%m-%d')\n",
    "        \n",
    "        temp = temp.merge(covid_df_contiguous[['FIPS', date_str]], on='FIPS', how='left')\n",
    "        \n",
    "        if logged:\n",
    "            temp['inc_rate_' + date_str] = np.log(temp[date_str] / temp['POPULATION'] * 10000 + 1)\n",
    "            \n",
    "        else:\n",
    "            temp['inc_rate_' + date_str] = temp[date_str] / temp['POPULATION'] * 10000\n",
    "        \n",
    "        normal_fci = get_normal_fci(date)\n",
    "        normal_fci = normal_fci.merge(temp, left_index= True, right_on='GEOID')\n",
    "        \n",
    "        normal_fci['fpc_'+ date_str] = np.dot(normal_fci.iloc[:,:number_counties], normal_fci['inc_rate_' + date_str])\n",
    "        \n",
    "    \n",
    "    normal_fci['mean_fpc'] = normal_fci.iloc[:,-len(dates):].mean(axis=1)\n",
    "        \n",
    "    return normal_fci[['GEOID','mean_fpc']]    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculates weekly average SPC\n",
    "# the input to this fuction should be of type datetime\n",
    "def weekly_mean_SPC(end_date, start_date, logged=False):\n",
    "    \n",
    "    dates = [end_date]\n",
    "    while end_date> start_date:\n",
    "        end_date -= timedelta(days=1)\n",
    "        dates.append(end_date)\n",
    "    \n",
    "    \n",
    "    temp = data[['GEOID','FIPS', 'POPULATION']]\n",
    "    \n",
    "    for date in dates:\n",
    "        # convert date to String\n",
    "        date_str = date.strftime('%Y-%m-%d')\n",
    "        \n",
    "        temp = temp.merge(covid_df_contiguous[['FIPS', date_str]], on='FIPS', how='left')\n",
    "        \n",
    "        if logged:\n",
    "            temp['inc_rate_' + date_str] = np.log(temp[date_str] / temp['POPULATION'] * 10000 + 1)\n",
    "            \n",
    "        else:\n",
    "            temp['inc_rate_' + date_str] = temp[date_str] / temp['POPULATION'] * 10000\n",
    "        \n",
    "        \n",
    "        normal_sci = sci_matrix_normal.merge(temp, left_index= True, right_on='GEOID')\n",
    "        \n",
    "        normal_sci['spc_'+ date_str] = np.dot(normal_sci.iloc[:,:number_counties], normal_sci['inc_rate_' + date_str])\n",
    "        \n",
    "    \n",
    "    normal_sci['mean_spc'] = normal_sci.iloc[:,-len(dates):].mean(axis=1)\n",
    "        \n",
    "    return normal_sci[['GEOID','mean_spc']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the input to this fuction should be of type datetime.\n",
    "# returns a subset of FB movement range dfs based on the given week\n",
    "def weekly_fb_mobility(end_date, start_date, df):\n",
    "    \n",
    "    dates = [end_date]\n",
    "    while end_date> start_date:\n",
    "        end_date -= timedelta(days=1)\n",
    "        dates.append(end_date)\n",
    "    \n",
    "    dates_str=[]\n",
    "    for date in dates:\n",
    "        # convert date to String\n",
    "        dates_str.append(date.strftime('%Y-%m-%d'))\n",
    "    \n",
    "    return df[['GEOID', *dates_str]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate slope features\n",
    "\n",
    "from scipy.stats import linregress\n",
    "\n",
    "def linear_reg(week_df):\n",
    "    \n",
    "    x = np.arange(1,(week_df.shape[1]),1)\n",
    "    x = (x - np.mean(x))/ np.std(x)\n",
    "    \n",
    "    slopes=[]\n",
    "    \n",
    "    for index, row in week_df.iloc[:,1:].iterrows():\n",
    "        y = row\n",
    "        y = (y - np.mean(y))/ np.std(y)\n",
    "        slopes.append(linregress(x, y)[0])\n",
    "        \n",
    "    week_df.loc[:,'slope'] = slopes\n",
    "    return week_df[['GEOID','slope']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final function to combine all features generated above\n",
    "\n",
    "This function generates a dataframe and for a given date, will add the following features to the dataframe\n",
    "\n",
    "- incidence rate data\n",
    "- FB mobility data (ratio, relative)\n",
    "- SPC (facebook SCI and incidence rates)\n",
    "- SafeGraph mobility \n",
    "- FPC (FCI and incidence rate)\n",
    "\n",
    "For each period, there is a 5 week difference between the actual date (t) and the start of the second lag. For example if `T: Oct 1 (Sep 24 to Oct 1)`, then `T-1: Sep 10 to Sep 24`, and `T-2: August 27 to Sep 10`.\n",
    "\n",
    "Since the earliest day for which we have FB mobility data is March 1, the rearliest  (end) date for T will be April 5th."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "covid_df_contiguous = data[['FIPS','STATEFP','COUNTYFP','GEOID']].merge(covid_df, on='FIPS', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main function\n",
    "# the input to the funtion is the end date for which we want to add data\n",
    "\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "\n",
    "def add_lagged_data(date):\n",
    "    global data\n",
    "    \n",
    "    T_end = date\n",
    "    T_start = T_end - timedelta(days=6)\n",
    "    \n",
    "    T_1_end = T_start - timedelta(days=1)\n",
    "    T_1_start = T_1_end - timedelta(days=6)\n",
    "    \n",
    "    T_2_end = T_1_start - timedelta(days=1)\n",
    "    T_2_start = T_2_end - timedelta(days=6)\n",
    "    \n",
    "    T_3_end = T_2_start - timedelta(days=1)\n",
    "    T_3_start = T_3_end - timedelta(days=6)\n",
    "    \n",
    "    T_4_end = T_3_start - timedelta(days=1)\n",
    "    T_4_start = T_4_end - timedelta(days=6)\n",
    "    \n",
    "    # These dates are used for cumulative cases (Saturday to Saturday)\n",
    "    T_start_case = T_end - timedelta(days=7)\n",
    "    T_1_start_case = T_1_end - timedelta(days=7)\n",
    "    T_2_start_case = T_2_end - timedelta(days=7)\n",
    "    T_3_start_case = T_3_end - timedelta(days=7)\n",
    "    T_4_start_case = T_4_end - timedelta(days=7)\n",
    "    \n",
    "\n",
    "    dates = [T_end.strftime('%Y-%m-%d'), T_start.strftime('%Y-%m-%d'),\n",
    "             T_1_end.strftime('%Y-%m-%d'), T_1_start.strftime('%Y-%m-%d'), \n",
    "             T_2_end.strftime('%Y-%m-%d'), T_2_start.strftime('%Y-%m-%d'),\n",
    "             T_3_end.strftime('%Y-%m-%d'), T_3_start.strftime('%Y-%m-%d'), \n",
    "             T_4_end.strftime('%Y-%m-%d'), T_4_start.strftime('%Y-%m-%d')]\n",
    "    \n",
    "    \n",
    "    dates_non_str = [T_end, T_start,\n",
    "             T_1_end, T_1_start, \n",
    "             T_2_end, T_2_start,\n",
    "             T_3_end, T_3_start, \n",
    "             T_4_end, T_4_start]\n",
    "    \n",
    "    dates_case = [T_end.strftime('%Y-%m-%d'), T_start_case.strftime('%Y-%m-%d'),\n",
    "             T_1_end.strftime('%Y-%m-%d'), T_1_start_case.strftime('%Y-%m-%d'), \n",
    "             T_2_end.strftime('%Y-%m-%d'), T_2_start_case.strftime('%Y-%m-%d'),\n",
    "             T_3_end.strftime('%Y-%m-%d'), T_3_start_case.strftime('%Y-%m-%d'), \n",
    "             T_4_end.strftime('%Y-%m-%d'), T_4_start_case.strftime('%Y-%m-%d')]\n",
    "    \n",
    "    \n",
    "    dates_non_str_case = [T_end, T_start_case,\n",
    "             T_1_end, T_1_start_case, \n",
    "             T_2_end, T_2_start_case,\n",
    "             T_3_end, T_3_start_case, \n",
    "             T_4_end, T_4_start_case]\n",
    "\n",
    "    \n",
    "    temp = data.copy()\n",
    "    \n",
    "    temp['date_end_period'] = T_end.strftime('%Y-%m-%d')\n",
    "    temp['date_start_period'] = T_start.strftime('%Y-%m-%d')\n",
    "    temp['date_end_lag'] = T_1_end.strftime('%Y-%m-%d')\n",
    "    temp['date_start_lag'] = T_4_start.strftime('%Y-%m-%d')\n",
    "    \n",
    "    time_periods = ['T_end', 'T_start', 'T_1_end', 'T_1_start','T_2_end','T_2_start',\n",
    "                    'T_3_end','T_3_start','T_4_end','T_4_start']\n",
    "    i = 0\n",
    "    for period in time_periods:\n",
    "        \n",
    "        \n",
    "        temp = temp.merge(covid_df_contiguous[['GEOID',dates_case[i]]], on='GEOID', how='left')\n",
    "        temp.rename(columns={dates_case[i]:'case_'+ period}, inplace=True)\n",
    "\n",
    "        \n",
    "        temp['inc_rate_' + period] = temp['case_'+ period] / temp['POPULATION'] * 10000\n",
    "        \n",
    "        temp = temp.merge(relative_df_smooth[['GEOID',dates[i]]], on='GEOID', how='left')\n",
    "        temp.rename(columns={dates[i]:'relative_'+ period}, inplace=True) \n",
    "\n",
    "\n",
    "        temp = temp.merge(ratio_df_smooth[['GEOID',dates[i]]], on='GEOID', how='left')\n",
    "        temp.rename(columns={dates[i]:'ratio_'+ period}, inplace=True)\n",
    "        \n",
    "        # The same date is used as the input to weekly_mean_SPC function to calculate\n",
    "        # SPC for that given date (instead of an average over a period)\n",
    "        temp = temp.merge(weekly_mean_SPC(dates_non_str_case[i],dates_non_str_case[i], logged=False), on='GEOID', how='left')\n",
    "        temp.rename(columns={'mean_spc':'SPC_'+ period}, inplace=True)\n",
    "\n",
    "        # simiar to SPC, add FPC values\n",
    "        temp = temp.merge(weekly_mean_FPC(dates_non_str_case[i],dates_non_str_case[i], logged=False), on='GEOID', how='left')\n",
    "        temp.rename(columns={'mean_fpc':'FPC_'+ period}, inplace=True)\n",
    "        \n",
    "        \n",
    "        # add logged SPC (defined as log(delta incidence rate)*sci/sum(sci))\n",
    "        temp = temp.merge(weekly_mean_SPC(dates_non_str_case[i],dates_non_str_case[i], logged=True), on='GEOID', how='left')\n",
    "        temp.rename(columns={'mean_spc':'SPC_logged_'+ period}, inplace=True)\n",
    "        \n",
    "        \n",
    "        # add logged FPC (defined as log(delta incidence rate)*fci/sum(fci))\n",
    "        temp = temp.merge(weekly_mean_FPC(dates_non_str_case[i],dates_non_str_case[i], logged=True), on='GEOID', how='left')\n",
    "        temp.rename(columns={'mean_fpc':'FPC_logged_'+ period}, inplace=True)\n",
    "\n",
    "        # add raw John Hopkins case data\n",
    "        temp = temp.merge(jh_covid_df[['FIPS',dates_non_str_case[i].strftime('%#m/%#d/%y')]], on='FIPS', how='left')\n",
    "        temp.rename(columns={dates_non_str_case[i].strftime('%#m/%#d/%y'):'case_JH_'+ period}, inplace=True)\n",
    "        \n",
    "        # add smoothed John Hopkins case data\n",
    "        temp = temp.merge(covid_df_contiguous[['FIPS',dates_case[i]]], on='FIPS', how='left')\n",
    "        temp.rename(columns={dates_case[i]:'case_JH_smoothed_'+ period}, inplace=True)\n",
    "       \n",
    "        \n",
    "        \n",
    "        i+=1\n",
    "    \n",
    "    times = ['T', 'T_1', 'T_2', 'T_3', 'T_4']\n",
    "    \n",
    "    j = 0\n",
    "    for period in times:\n",
    "        temp['DELTA_INC_RATE_' + period] = temp['inc_rate_' + time_periods[j]] - temp['inc_rate_' + time_periods[j+1]]\n",
    "        \n",
    "        temp['DELTA_REL_MOB_' + period] = temp['relative_' + time_periods[j]] - temp['relative_' + time_periods[j+1]]\n",
    "        temp['DELTA_RATIO_MOB_' + period] = temp['ratio_' + time_periods[j]] - temp['ratio_' + time_periods[j+1]]\n",
    "        \n",
    "        temp['DELTA_SPC_' + period] = temp['SPC_' + time_periods[j]] - temp['SPC_' + time_periods[j+1]]\n",
    "        temp['DELTA_FPC_' + period] = temp['FPC_' + time_periods[j]] - temp['FPC_' + time_periods[j+1]]\n",
    "        temp['DELTA_SPC_LOGGED_' + period] = temp['SPC_logged_' + time_periods[j]] - temp['SPC_logged_' + time_periods[j+1]]\n",
    "        temp['DELTA_FPC_LOGGED_' + period] = temp['FPC_logged_' + time_periods[j]] - temp['FPC_logged_' + time_periods[j+1]]\n",
    "        \n",
    "        temp['DELTA_CASE_JH_' + period] = temp['case_JH_'+ time_periods[j]] - temp['case_JH_'+ time_periods[j+1]]\n",
    "        temp['DELTA_CASE_JH_SMOOTH_' + period] = temp['case_JH_smoothed_'+ time_periods[j]] - \n",
    "                                                 temp['case_JH_smoothed_'+ time_periods[j+1]]\n",
    "        \n",
    "        # mean incidence rate is calculated between Sunday and Saturday\n",
    "        temp['MEAN_INC_RATE_' + period] = covid_df_contiguous[pd.date_range(start=dates[j+1], end=dates[j])\n",
    "                                            .strftime('%Y-%m-%d')].mean(axis=1) / temp['POPULATION'] * 10000\n",
    "        temp['MEAN_REL_MOB_' + period] = relative_df_smooth[pd.date_range(start=dates[j+1], end=dates[j])\n",
    "                                            .strftime('%Y-%m-%d')].mean(axis=1)\n",
    "        temp['MEAN_RATIO_MOB_' + period] = ratio_df_smooth[pd.date_range(start=dates[j+1], end=dates[j])\n",
    "                                            .strftime('%Y-%m-%d')].mean(axis=1)\n",
    "\n",
    "        \n",
    "        # add Safegraph mobility features\n",
    "        safegraph_data = safegraph_contiguous[safegraph_contiguous['end_date']==dates[j]][safegraph_metrics]\n",
    "        temp = temp.merge(safegraph_data, left_on='GEOID', right_on='county_fips')\n",
    "        \n",
    "        rename_dict = dict()\n",
    "        for col in safegraph_metrics[1:]:\n",
    "            rename_dict[col] = col + '_' + period\n",
    "            \n",
    "        temp.rename(columns=rename_dict, inplace=True)\n",
    "        \n",
    "        \n",
    "        # add MEAN_FPC\n",
    "        temp = temp.merge(weekly_mean_FPC(dates_non_str_case[j], dates_non_str_case[j+1]), on='GEOID', how='left')\n",
    "        temp.rename(columns={'mean_fpc':'MEAN_FPC_'+ period}, inplace=True)\n",
    "        \n",
    "        # add logged MEAN_FPC \n",
    "        temp = temp.merge(weekly_mean_FPC(dates_non_str_case[j], dates_non_str_case[j+1], logged=True), on='GEOID', how='left')\n",
    "        temp.rename(columns={'mean_fpc':'MEAN_FPC_LOGGED_'+ period}, inplace=True)\n",
    "        \n",
    "        # add MEAN_SPC\n",
    "        temp = temp.merge(weekly_mean_SPC(dates_non_str_case[j], dates_non_str_case[j+1]), on='GEOID', how='left')\n",
    "        temp.rename(columns={'mean_spc':'MEAN_SPC_'+ period}, inplace=True)\n",
    "        \n",
    "        # add logged MEAN_SPC\n",
    "        temp = temp.merge(weekly_mean_SPC(dates_non_str_case[j], dates_non_str_case[j+1], logged=True), on='GEOID', how='left')\n",
    "        temp.rename(columns={'mean_spc':'MEAN_SPC_LOGGED_'+ period}, inplace=True)\n",
    "        \n",
    "        # add FB mobility slopes\n",
    "        ratio_slope = linear_reg(weekly_fb_mobility(dates_non_str[j], dates_non_str[j+1], ratio_df_smooth))\n",
    "        temp = temp.merge(ratio_slope, on='GEOID', how='left')\n",
    "        temp.rename(columns={'slope':'SLOPE_RATIO_MOB_'+ period}, inplace=True)\n",
    "        \n",
    "        relative_slope = linear_reg(weekly_fb_mobility(dates_non_str[j], dates_non_str[j+1], relative_df_smooth))\n",
    "        temp = temp.merge(relative_slope, on='GEOID', how='left')\n",
    "        temp.rename(columns={'slope':'SLOPE_REL_MOB_'+ period}, inplace=True)\n",
    "\n",
    "        \n",
    "        \n",
    "        # add temperature features\n",
    "        # to update for the new dates, min and max temperature are used with one day offset\n",
    "        adj_temp_date = (dates_non_str[j] + timedelta(days=1)).strftime('%Y-%m-%d')\n",
    "        \n",
    "        temp = temp.merge(max_temp[['GEOID', adj_temp_date]], on='GEOID', how='left')\n",
    "        temp.rename(columns={adj_temp_date:'MAX_TEMP_'+ period}, inplace=True)\n",
    "        \n",
    "        temp = temp.merge(min_temp[['GEOID', adj_temp_date]], on='GEOID', how='left')\n",
    "        temp.rename(columns={adj_temp_date:'MIN_TEMP_'+ period}, inplace=True)\n",
    "\n",
    "        j += 2\n",
    "\n",
    "    output_df = temp.copy()\n",
    "    \n",
    "    return output_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "end_date = datetime(2021, 2, 27)\n",
    "week_counter = 0\n",
    "df_list = []\n",
    "\n",
    "while end_date > datetime(2020, 3, 29):\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    df_list.append(add_lagged_data(end_date))\n",
    "    \n",
    "    end_time = time.time()\n",
    "    print('Feature generation for the week ending in {} finished in {} seconds'.format(end_date.strftime('%Y-%m-%d'),\n",
    "                                                                                       round(end_time-start_time,1)))\n",
    "    \n",
    "    end_date -= timedelta(weeks=1)\n",
    "    week_counter+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df = pd.concat(df_list, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check number of weeks for which we have features\n",
    "final_df.shape[0]/3103"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imputing NAs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pd.set_option('max_rows', 200)\n",
    "final_df.isna().sum().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('max_rows', 400)\n",
    "final_df.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_to_save = final_df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "na_columns = data_to_save.columns[data_to_save.isnull().any()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# impute missing values by state average\n",
    "for col in data_to_save[na_columns].columns:\n",
    "    data_to_save[col] = data_to_save.groupby('STATEFP')[col].transform(lambda x: x.fillna(x.mean()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_to_save.isna().sum().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "times = ['T', 'T_1', 'T_2', 'T_3', 'T_4']\n",
    "\n",
    "for period in times:\n",
    "    data_to_save['LOG_DELTA_INC_RATE_' + period] = np.log(data_to_save['DELTA_INC_RATE_' + period] + 1)\n",
    "    data_to_save['LOG_MEAN_INC_RATE_' + period] = np.log(data_to_save['MEAN_INC_RATE_' + period] + 1)\n",
    "    data_to_save['LOG_DELTA_SPC_' + period] = np.log(data_to_save['DELTA_SPC_' + period] + 1)\n",
    "    data_to_save['LOG_DELTA_FPC_' + period] = np.log(data_to_save['DELTA_FPC_' + period] + 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_cols = [\n",
    "'GEOID',\n",
    "'NAME',\n",
    "'State_Name',\n",
    "'STATEFP', \n",
    "'COUNTYFP', \n",
    "'date_end_period',\n",
    "'date_start_period',\n",
    "'date_end_lag',\n",
    "'date_start_lag',\n",
    "'LOG_DELTA_INC_RATE_T',\n",
    "'PCT_MALE',\n",
    "'PCT_BLACK',\n",
    "'PCT_HISPAN', \n",
    "'PCT_AMIND',\n",
    "'PCT_RURAL',\n",
    "'PCT_COL_DE' ,\n",
    "'PCT_TRUMP_',\n",
    "'MED_HOS_IN',\n",
    "'POPULATION',\n",
    "'DELTA_CASE_JH_T',\n",
    "'DELTA_CASE_JH_SMOOTH_T'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "additional_cols=[\n",
    "'LOG_DELTA_INC_RATE_T_',\n",
    "'DELTA_REL_MOB_T_',\n",
    "'DELTA_RATIO_MOB_T_',\n",
    "'DELTA_SPC_T_',\n",
    "'DELTA_SPC_LOGGED_T_',\n",
    "'DELTA_FPC_T_',\n",
    "'DELTA_FPC_LOGGED_T_',\n",
    "'LOG_MEAN_INC_RATE_T_',\n",
    "'MEAN_REL_MOB_T_',\n",
    "'MEAN_RATIO_MOB_T_',\n",
    "'MEAN_FPC_T_',\n",
    "'MEAN_SPC_T_',\n",
    "'SLOPE_RATIO_MOB_T_',\n",
    "'SLOPE_REL_MOB_T_',\n",
    "'MAX_TEMP_T_',\n",
    "'MIN_TEMP_T_',\n",
    "'pct_completely_home_device_count_current_T_',\n",
    "'pct_full_time_work_behavior_devices_current_T_',\n",
    "'pct_part_time_work_behavior_devices_current_T_',\n",
    "'pct_delivery_behavior_devices_current_T_',\n",
    "'distance_traveled_from_home_current_T_',\n",
    "'median_home_dwell_time_current_T_',\n",
    "'pct_completely_home_device_count_baselined_T_',\n",
    "'pct_full_time_work_behavior_devices_baselined_T_',\n",
    "'pct_part_time_work_behavior_devices_baselined_T_',\n",
    "'pct_delivery_behavior_devices_baselined_T_',\n",
    "'distance_traveled_from_home_baselined_T_',\n",
    "'median_home_dwell_time_baselined_T_',\n",
    "'pct_completely_home_device_count_slope_T_',\n",
    "'pct_full_time_work_behavior_devices_slope_T_',\n",
    "'pct_part_time_work_behavior_devices_slope_T_',\n",
    "'pct_delivery_behavior_devices_slope_T_',\n",
    "'distance_traveled_from_home_slope_T_',\n",
    "'median_home_dwell_time_slope_T_',\n",
    "'DELTA_CASE_JH_T_',\n",
    "'MEAN_SPC_LOGGED_T_',\n",
    "'MEAN_FPC_LOGGED_T_'\n",
    "]\n",
    "\n",
    "for i in range(1,5):\n",
    "    for col in additional_cols:\n",
    "        final_cols.append(col+str(i))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = data_to_save[final_cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output.to_csv('./output/all_features_updated_incidence.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating dataframes for 2, 3, and 4-week predictions\n",
    "\n",
    "in this dataframe, the target variables is the the number of cumulative cases in 2, 3, and 4 weeks ahead, denoted by `LOG_DELTA_INC_RATE_T_14`, `LOG_DELTA_INC_RATE_T_21`, and `LOG_DELTA_INC_RATE_T_28` respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main function\n",
    "# the input to the funtion is the end date for which we want to add data\n",
    "\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "\n",
    "def add_lagged_y(date):\n",
    "    global output, jh_covid_df\n",
    "    \n",
    "    T_end = date\n",
    "    T_start = T_end - timedelta(days=7)\n",
    "    \n",
    "    T_start_period = (T_end - timedelta(days=6)).strftime('%Y-%m-%d')\n",
    "    \n",
    "    T_14 =  T_end + timedelta(days=7)\n",
    "    T_21 =  T_end + timedelta(days=14)\n",
    "    T_28 =  T_end + timedelta(days=21)\n",
    "    \n",
    "\n",
    "    dates_non_str = [T_end, T_start, T_14, T_21, T_28]\n",
    "    \n",
    "    dates = [item.strftime('%Y-%m-%d') for item in dates_non_str]\n",
    "    \n",
    "    dates_jh = [item.strftime('%#m/%#d/%y') for item in dates_non_str]\n",
    "    \n",
    "    \n",
    "    periods = ['T_end', 'T_start', 'T_14', 'T_21', 'T_28']\n",
    "    \n",
    "    temp = output.loc[(output.date_end_period==dates[0]) & (output.date_start_period==T_start_period)].copy()\n",
    "    \n",
    "    temp['FIPS'] = temp['GEOID'].astype(int)\n",
    "    \n",
    "    #print('check 1 {}'.format(temp.shape))\n",
    "    temp['target_date_2wk'] = T_14.strftime('%Y-%m-%d')\n",
    "    temp['target_date_3wk'] = T_21.strftime('%Y-%m-%d')\n",
    "    temp['target_date_4wk'] = T_28.strftime('%Y-%m-%d')\n",
    "        \n",
    "    temp = temp.merge(covid_df_contiguous[['GEOID',*dates]], on='GEOID', how='left')\n",
    "    \n",
    "    temp = temp.merge(jh_covid_df[['FIPS',*dates_jh]], on='FIPS', how='left')\n",
    "    #print('check 2 {}'.format(temp.shape))\n",
    "\n",
    "    for period, date in zip(periods, dates):\n",
    "        temp['inc_rate_' + period] = temp[date] / temp['POPULATION'] * 10000\n",
    "\n",
    "\n",
    "    for period, date in zip(periods[-3:], dates[-3:]):\n",
    "        temp['DELTA_CASE_SMOOTHED_' + period] = temp[date] - temp[dates[1]]\n",
    "        temp['DELTA_INC_RATE_' + period] = temp['inc_rate_' + period] - temp['inc_rate_T_start']\n",
    "        temp['LOG_DELTA_INC_RATE_' + period] = np.log(temp['DELTA_INC_RATE_' + period] + 1)\n",
    "    \n",
    "    for period, date in zip(periods[-3:], dates_jh[-3:]):\n",
    "        temp['DELTA_CASE_JH_' + period] = temp[date] - temp[dates_jh[1]]\n",
    "    \n",
    "    temp['DELTA_CASE_JH_T'] = temp[dates_jh[0]] - temp[dates_jh[1]]\n",
    "        \n",
    "    \n",
    "    \n",
    "    cols = ['target_date_2wk','LOG_DELTA_INC_RATE_T_14', \n",
    "            'target_date_3wk','LOG_DELTA_INC_RATE_T_21',\n",
    "            'target_date_4wk','LOG_DELTA_INC_RATE_T_28' ]\n",
    "    #print('check 3 {}'.format(temp.shape))\n",
    "    \n",
    "    return temp[[*output.columns,'DELTA_CASE_JH_T',\n",
    "            'target_date_2wk','LOG_DELTA_INC_RATE_T_14', 'DELTA_CASE_SMOOTHED_T_14', 'DELTA_CASE_JH_T_14',\n",
    "            'target_date_3wk','LOG_DELTA_INC_RATE_T_21', 'DELTA_CASE_SMOOTHED_T_21', 'DELTA_CASE_JH_T_21',\n",
    "            'target_date_4wk','LOG_DELTA_INC_RATE_T_28', 'DELTA_CASE_SMOOTHED_T_28', 'DELTA_CASE_JH_T_28']]\n",
    "    \n",
    "    \n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "end_date = datetime(2021, 2, 27)\n",
    "df_lagged_list = []\n",
    "\n",
    "while end_date > datetime(2020, 3, 29):\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    df_lagged_list.append(add_lagged_y(end_date))\n",
    "    \n",
    "    end_time = time.time()\n",
    "    print('Feature generation for the week ending in {} finished in {} seconds'.format(end_date.strftime('%Y-%m-%d'),\n",
    "                                                                                       round(end_time-start_time,1)))\n",
    "    end_date -= timedelta(weeks=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_lagged = pd.concat(df_lagged_list, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_lagged.isna().sum().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_lagged.shape, df_lagged.shape[0]/3103"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_lagged.to_csv('./output/all_features.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sp",
   "language": "python",
   "name": "sp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
